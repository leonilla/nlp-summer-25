{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2BLz5rCRDKS"
      },
      "source": [
        "**Introduction to Natural Language Processing**<br/>\n",
        "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
        "Prof. Dr. Annemarie Friedrich<br/>\n",
        "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
        "Date: **SS 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx0Edci8Am39"
      },
      "source": [
        "# Overview of NLP Tasks\n",
        "\n",
        "**Learning Goals**\n",
        "\n",
        "* Obtain an overview of a selection of common NLP tasks.\n",
        "* Run some state-of-the-art models on different types of texts.\n",
        "* Observe what type of information the models return.\n",
        "\n",
        "**Don't Panic**\n",
        "\n",
        "* You do not need to understand the code below in detail yet. The comments and explanations should enable you to run it and to make small modifications to the input examples. (If this is not the case, let me know!)\n",
        "* Many of the links below lead to websites with more advanced information on the topics. You do not need to understand them in detail, but if you read a bit on these websites and take notes, you will probably learn a lot.\n",
        "\n",
        "**Instructions for In-Class Group Activity**\n",
        "\n",
        "1. Distribute the NLP tasks listed below among your team members. You have 15 minutes to work on one NLP task individually or in teams of two (if you are done more quickly, work on more NLP tasks). Be prepared to present your findings in a two-minute oral presentation (without slides). You may of course show your notebook to your fellow group members during this presentation.\n",
        "2. Briefly present all the NLP tasks within your group (max. 2 minutes per task). Take notes.\n",
        "\n",
        "**Technical Prerequisites**\n",
        "\n",
        "Use a GPU to process your data.\n",
        "This step is particularly helpful to speed up processing when working with large neural models. In this notebook, we use them for the following tasks:\n",
        "\n",
        "* Text Classification and Sentiment Analysis\n",
        "* Question Answering\n",
        "* Grammar Correction\n",
        "\n",
        "On Google Colaboraty, do the following steps:<br/>\n",
        "``Runtime --> Change runtime type --> Hardware accelerator --> T4 GPU or TPU``\n",
        "\n",
        "And now, let's get started.\n",
        "First, let's import some modules that we will need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7E5FG1YAjHU"
      },
      "outputs": [],
      "source": [
        "# Imports and general settings\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEjUT03LEY2t"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load the model that we will use for predictions\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY037vQPa-GH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHhS41pOu1Pu"
      },
      "source": [
        "❗Now, you can jump to the section of the NLP Tasks that you were assigned within your group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfmNgA2MEIGF"
      },
      "source": [
        "---\n",
        "\n",
        "## Text Classification and Sentiment Analysis\n",
        "We will use the [HuggingFace](https://huggingface.co/) library to load a model trained for binary sentiment analysis and apply it to a movie reviews corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kVyjSOQEHXp"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiNvKFXjxD2T"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "data = [\"I love you\", \"I hate you\"]\n",
        "sentiment_pipeline(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1qXCvkFKElt"
      },
      "source": [
        "### Working with a sentiment analysis dataset\n",
        "\n",
        "We will next load an existing dataset of movie reviews (`rotten_tomatoes`) with gold labels, i.e., labels assigned by a human, that indicate whether a review expresses a positive or a negative sentiment towards the movie.\n",
        "You can find more information on the dataset [here](https://huggingface.co/datasets/rotten_tomatoes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYfWgAsgIiG0"
      },
      "outputs": [],
      "source": [
        "# Load a dataset of movie reviews that are annotated with regard to whether they are positive or negative\n",
        "dataset = load_dataset(\"rotten_tomatoes\", split=\"validation\") # we choose the validation split of the dataset\n",
        "\n",
        "# Collect 10 positive and 10 negative instances\n",
        "positive_reviews = defaultdict(list)\n",
        "negative_reviews = defaultdict(list)\n",
        "for i, (gold_label, review_text) in enumerate(zip(dataset[\"label\"], dataset[\"text\"])):\n",
        "  if gold_label == 1 and len(positive_reviews[\"text\"]) < 10:\n",
        "    positive_reviews[\"text\"].append(review_text)\n",
        "    positive_reviews[\"label\"].append(gold_label)\n",
        "  if gold_label == 0 and len(negative_reviews[\"text\"]) < 10:\n",
        "    # skipping a few cases (honestly, they are just odd cases in the dataset, this happens :))\n",
        "    if i not in (539, 541, 542):\n",
        "      negative_reviews[\"text\"].append(review_text)\n",
        "      negative_reviews[\"label\"].append(gold_label)\n",
        "\n",
        "print(\"POSITIVE REVIEWS:\")\n",
        "for i, text in enumerate(positive_reviews[\"text\"]):\n",
        "  print(positive_reviews[\"label\"][i], \"\\t\", text)\n",
        "\n",
        "print(\"\\nNEGATIVE REVIEWS:\")\n",
        "for i, text in enumerate(negative_reviews[\"text\"]):\n",
        "  print(negative_reviews[\"label\"][i], \"\\t\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZNy2dHSKjVd"
      },
      "outputs": [],
      "source": [
        "# Let's process this dataset with our model\n",
        "sentiment_pipeline(positive_reviews[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo5M6seRMKI-"
      },
      "outputs": [],
      "source": [
        "# Wow, that worked well, what about the negative cases?\n",
        "sentiment_pipeline(negative_reviews[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogOolGBFMSMX"
      },
      "source": [
        "❓[1.1] Why do you think the model got one case wrong here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITYB44IHxOX7"
      },
      "source": [
        "❓[1.2] Read about **aspect-based sentiment analysis** on [PapersWithCode](https://paperswithcode.com/task/aspect-based-sentiment-analysis). What would be possible aspects for hotel reviews / restaurant reviews?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1O44IYDOtK6"
      },
      "source": [
        "Sentiment analysis is one type of **text classification** task: we provide the model with a text and obtain a label. In this case, we had a **binary** classification task that only decides between two labels. In principle, **multi-class** classification tasks decide between more than two labels, and **multi-label** classification tasks even allow a model (or a human annotator) to assign more than one label from the label set.\n",
        "\n",
        "❓[1.3] Can you find further examples for binary, multi-class, and multi-label text classification tasks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0qnSFHlEVs3"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "## Named Entity Recognition and Typing\n",
        "\n",
        "We will use the [spaCy](https://spacy.io/) library to find out which **named entities** occur in a given text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZcOMKDgR-7_"
      },
      "source": [
        "❓ [1.4] Briefly skim [the Wikipedia page on Named Entities](https://en.wikipedia.org/wiki/Named_entity) and note down a definition that will help you to remember what this NLP task is about.\n",
        "\n",
        "**Definition:** In Information Extraction, the NLP task of Named Entity Recognition and Typing means ... *(add your answer here`)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrdG__ygHHad"
      },
      "outputs": [],
      "source": [
        "# Source: https://en.wikipedia.org/wiki/Augsburg\n",
        "input_text = \"\"\"Augsburg (UK: /ˈaʊɡzbɜːrɡ/ OWGZ-burg,[3] US: /ˈɔːɡz-/ AWGZ-,[4] German: [ˈaʊksbʊʁk] (listen);\n",
        "Swabian German: Ougschburg) is a city in Swabia, Swabia, Germany,\n",
        "around 50 kilometres (31 mi) west of Bavarian capital Munich.\n",
        "It is a university town and regional seat of the Regierungsbezirk Swabia\n",
        "with an impressive Altstadt (historical city centre).\n",
        "Augsburg is an urban district and home to the institutions of the Landkreis Augsburg.\n",
        "It is the third-largest city in Bavaria (after Munich and Nuremberg), with a population of 300,000\n",
        "and 885,000 in its metropolitan area.[5]\n",
        "\"\"\"\n",
        "\n",
        "# Process the text and annotate the named entities.\n",
        "doc = nlp(input_text)\n",
        "\n",
        "# Visualize results.\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'distance': 90})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiUp2TEHTJCG"
      },
      "source": [
        "Among others, spaCy recognizes the following built-in _entity types_ (see [Kaggle](https://www.kaggle.com/code/curiousprogrammer/entity-extraction-and-classification-using-spacy)):\n",
        "\n",
        "**PERSON** - People, including fictional.\n",
        "\n",
        "**NORP** - Nationalities or religious or political groups.\n",
        "\n",
        "**ORG** - Companies, agencies, institutions, etc.\n",
        "\n",
        "**GPE** - Countries, cities, states.\n",
        "\n",
        "**LOC** - Non-GPE locations, mountain ranges, bodies of water.\n",
        "\n",
        "**LANGUAGE** - Any named language.\n",
        "\n",
        "**DATE** - Absolute or relative dates or periods.\n",
        "\n",
        "**TIME** - Times smaller than a day.\n",
        "\n",
        "**QUANTITY** - Measurements, as of weight or distance.\n",
        "\n",
        "**ORDINAL** - \"first\", \"second\", etc.\n",
        "\n",
        "**CARDINAL** - Numerals that do not fall under another type.\n",
        "\n",
        "❓[1.5] Which types of built-in _entity types_ does our spaCy model recognize in the input text? How are the types we have encountered in our text defined? Do all assigned types make sense to you in this case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-a2ZydXNQH1"
      },
      "outputs": [],
      "source": [
        "# Access text and spans using code\n",
        "s = \"{:<30} {:<4} {:<4} {:<8}\"\n",
        "for ent in doc.ents: # Iterate over the list of NEs that the model has found.\n",
        "  print(s.format(ent.text, ent.start_char, ent.end_char, ent.label_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAVGwtopPcDC"
      },
      "source": [
        "You have just encountered the concept of **stand-off annotation**: Each NE annotation is defined by two character offsets referring to the number of characters counting from the beginning of the document. For example, *Augsburg* in the document above has the `start_char` (starting character) 0 and the `end_char` 8. (As almost always in computer science, we start counting at 0.)\n",
        "\n",
        "You can most easily remember `star_char` as the number of characters that come before a word (or some annotation) in a document, and `end_char` as the number of characters that occur before the word is completed.\n",
        "\n",
        "\n",
        "```\n",
        "0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |  ...\n",
        "  A   u   g   s   b   u   r   g       (    U    K  ...\n",
        "```\n",
        "\n",
        "The Pythonian way to remember this is to just access the original text at these slices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QPgl8fkRKce"
      },
      "outputs": [],
      "source": [
        "print(input_text[0:8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnCsTmdXSoyq"
      },
      "source": [
        "❓[1.6] The model that we have loaded here was trained on English data. Feed in some input text of a different language of your choice. What do you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNxCFmqEba6"
      },
      "source": [
        "---\n",
        "## Question Answering\n",
        "\n",
        "In this section, we will introduce different types of __question answering__ (QA) systems and run an extractive QA model based on the [HuggingFace](https://huggingface.co/) library on parts of the [Squad](https://huggingface.co/datasets/squad) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjDEgystSSXH"
      },
      "source": [
        "You have probably played with models like ChatGPT that are based on huge pre-trained language models (_large language models_ (LLMs)). These models are **generative** models, i.e., they _generate_ answers token by token without a direct reference to a source text. Their answers are thus based on the latent knowledge they have learned from large volumes of text, and the questions that is used as a _prompt_ to the model. Essentially, such **Generative QA** models output what their internal statistics think to be the most likely continuation after the question has been asked. This is also why it is extremely hard to tell whether they are saying the truth or whether they are \"hallucinating.\" (If you're interested, you can read more about this problem [here](https://arxiv.org/abs/2202.03629).)\n",
        "\n",
        "Another QA task is that of **Extractive QA**, where the task is to retrieve an answer given a source document (_context_). The task is:\n",
        "* Determine whether the source document provides an answer.\n",
        "* If so, return the **span** within the source document that corresponds to the answer. The span is returned as two character offsets counting the number of characters from the beginning of the document until the answer starts (`start`), and the number of characters from the beginning of the document until the answer ends (`end`).\n",
        "\n",
        "Let's take a look at one example from the [Squad](https://huggingface.co/datasets/squad) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBzY-VaiULbu"
      },
      "outputs": [],
      "source": [
        "from textwrap import wrap\n",
        "\n",
        "# The document text is provided as the context for answering the question.\n",
        "doc_text = \"CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game. The Super Bowl 50 halftime show was headlined by the British rock group Coldplay with special guest performers Beyoncé and Bruno Mars, who headlined the Super Bowl XLVII and Super Bowl XLVIII halftime shows, respectively. It was the third-most watched U.S. broadcast ever.\"\n",
        "# Let's print this in a more human-friendly way.\n",
        "print(\"\\n\".join(wrap(doc_text, width=100)))\n",
        "\n",
        "question1 = \"Who were special guests for the Super Bowl halftime show?\" # Answer: \"Beyoncé and Bruno Mars\"\n",
        "question2 = \"What was the cost for a half minute ad?\" # Answer: \"$5 million\"\n",
        "question3 = \"Who watched the game on TV?\" # Is there an answer in the text?\n",
        "question4 = \"Which teams played against each other in the Super Bowl 50?\" # Not answerable based on the text above.\n",
        "\n",
        "questions = [question1, question2, question3, question4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXtp2pM8V-MJ"
      },
      "outputs": [],
      "source": [
        "# Load a QA model trained on the training part of the Squad dataset\n",
        "from transformers import pipeline\n",
        "qa_model = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxpY1Ag-WYqU"
      },
      "outputs": [],
      "source": [
        "for question in questions:\n",
        "  print(\"\\n\"+question)\n",
        "  result = qa_model(question=question, context=doc_text)\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEroXlVkYr0j"
      },
      "source": [
        "❓[1.7] Which answers did the model get right/wrong?\n",
        "\n",
        "❓[1.8] Does the _confidence score_ (`score`) correspond to the correctness of the answer?\n",
        "\n",
        "❓[1.9] What do you think went wrong in the cases that the model answered incorrectly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roJq0bWHEq9a"
      },
      "source": [
        "---\n",
        "\n",
        "## Part-of-Speech Tagging and Syntactic Parsing\n",
        "\n",
        "We will use the [spaCy](https://spacy.io/) library to find out which **part-of-speech** tag a word should carry in a sentence, and which **grammatical relations** hold between words in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfo-oNwqmrv1"
      },
      "source": [
        "### Part-of-speech Tagging\n",
        "\n",
        "**Part-of-speech** (POS) tagging refers to the task of assigning each word in a sentence or text its part-of-speech (German: _Wortart_) according to a fixed inventory. An influential inventory of POS tags is the [Penn Treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
        "The spaCy model below uses the [UPOS tagset](https://universaldependencies.org/u/pos/) developed by the [Universal Dependencies project](https://universaldependencies.org/).\n",
        "\n",
        "Before neural models became the prevalent way of solving NLP tasks, POS tags were an important source of information for models based on \"traditional\" features. Today, analysing them can be an _explainable_ way of analysing a dataset or the results produced by a model. For example, we can answer questions such as \"Are there many long noun phrases in the data? How does the model perform on them?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf52F49PmvEj"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load the model that we will use for predictions\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
        "\n",
        "input_sentences = [\"Susan can run 1km in under 3 minutes.\", \"This could open a can of worms.\"]\n",
        "\n",
        "# Process the input sentences and print the POS tag of each word\n",
        "for input_sentence in input_sentences:\n",
        "  print(\"\\n\" + input_sentence)\n",
        "  doc = nlp(input_sentence)\n",
        "  for word in doc:\n",
        "    print(\"{:<12} {:<4}\".format(word.text, word.pos_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0Zws2ppAYa"
      },
      "source": [
        "❓[1.10] Which of the above POS tags can you define easily? Look up the UPOS tags that you cannot (yet) make sense of on the [Universal Dependencies page](https://universaldependencies.org/u/pos/).\n",
        "\n",
        "❓[1.11] The word \"can\" occurs in each sentences. Which POS tag does it carry in each case? Hopefully, this case convinces you that POS tagging is not completely straightforward as there is no one-to-one correspondence between words and POS tags. Check out the currently reported best scores on POS tagging benchmarks on [this website](http://nlpprogress.com/english/part-of-speech_tagging.html) - is POS tagging a solved task? In which cases are still improvements possible?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkn40e_PmOne"
      },
      "source": [
        "### Syntactic Parsing\n",
        "\n",
        "**Syntactic parsing** aims to identify grammatical relations between the words of a sentence. This is often very useful to describe grammatical rules, e.g., when teaching a language or when linguists analyse or compare languages.\n",
        "The [Universal Dependencies project](https://universaldependencies.org/) collects **treebanks**, i.e., text corpora that are annotated with syntactic structures, for many languages of the world. Before neural models became prevalent in state-of-the-art NLP, syntactic features were a main source of information for automatic classifiers in many NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr9rBQzrmT31"
      },
      "outputs": [],
      "source": [
        "# Load the model including the parser\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "input_sentence = \"Joe gave Mary the book.\"\n",
        "# Try these as well\n",
        "#input_sentence = \"The cake has been baked by Mary.\"\n",
        "#input_sentence = \"He saw the man with the telescope.\"\n",
        "\n",
        "# Process sentence\n",
        "doc = nlp(input_sentence)\n",
        "# Visualize results.\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kkPjiHhP9CL"
      },
      "source": [
        "Above, you should now be able to see the **dependency tree** representing the syntactic structure of the sentence.\n",
        "* \"gave\" is the _main verb_ of the sentence, and the dependency tree's _root node_.\n",
        "* \"Joe\" is the _nominal subject_ (nsubj) of \"gave\": a _subject_ answers the question \"**who** did sth.\", nominal means that the subject is a noun.*\n",
        "* \"Mary\" is the _dative object_, usually called _indirect object_ (iobj).\n",
        "* \"book\" is the _direct object_ (dobj) of \"gave\", and \"the\" is its _determiner_ (det).\n",
        "\n",
        "*If you are wondering what else a subject could be:\n",
        "* \"_That she won_ upset him.\" - The italic part is a _clausal subject_ (csubj).\n",
        "\n",
        "❓[1.12] Try to parse another sentence using the code above. Try to understand the syntactic structure output by the parser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phlQ8vX1EtXV"
      },
      "source": [
        "## Grammar Correction\n",
        "\n",
        "We will use a [grammar correction model](https://huggingface.co/vennify/t5-base-grammar-correction) based on a language model  called T5 which has been trained on a grammar correction dataset that consists of mistakes and their corrections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li6R7wmZEu-7"
      },
      "outputs": [],
      "source": [
        "# Install the dependencies\n",
        "!pip install happytransformer\n",
        "from happytransformer import HappyTextToText, TTSettings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y8aPsADR1ys"
      },
      "outputs": [],
      "source": [
        "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
        "\n",
        "args = TTSettings(num_beams=5, min_length=1)\n",
        "\n",
        "# Add the prefix \"grammar: \" before each input\n",
        "result = happy_tt.generate_text(\"grammar: This sentences has has bads grammar.\", args=args)\n",
        "\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x24laklzSEcZ"
      },
      "outputs": [],
      "source": [
        "inputs = [\n",
        "    \"The apple was in table.\",\n",
        "    \"The apple was in tabel.\",\n",
        "    \"Dear Sir or Madam, I would to request a dedline extinsion for submit thesis.\",\n",
        "    \"I couldnt finish it yesterday because dog ate homework.\"\n",
        "]\n",
        "for input in inputs:\n",
        "  result = happy_tt.generate_text(\"grammar: \" + input, args=args)\n",
        "\n",
        "  print(result.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgS2fk_LTCGw"
      },
      "source": [
        "❓[1.13] Try out a few sentences of your choice. Does the model work for languages other than English?\n",
        "\n",
        "❓[1.14] Which types of errors were made in the sentences above? Which of them did the model fix?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YQz2-DVW5o-"
      },
      "source": [
        "The model we used above has been trained on the `validation` split of the [JFLEG dataset](https://huggingface.co/datasets/jfleg) as described in [this blogpost](https://www.vennify.ai/fine-tune-grammar-correction/).\n",
        "Let's load the `test`split and run the model on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu0Li_auWL2d"
      },
      "outputs": [],
      "source": [
        "# Let's load the dataset on which the model has been trained\n",
        "!pip install datasets\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "jfleg_dataset = load_dataset(\"jfleg\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml9ezuKAXZ0u"
      },
      "outputs": [],
      "source": [
        "# Let's print some instances of the dataset, which consist of an input\n",
        "# sentence with potentially wrong grammar, and 4 possible corrections.\n",
        "for i in [0,4,8]:\n",
        "  sentence = jfleg_dataset[\"sentence\"][i]\n",
        "  print(\"\\n\", sentence)\n",
        "  print(\"\\t\" + \"-\"*90)\n",
        "  result = happy_tt.generate_text(\"grammar: \" + sentence, args=args)\n",
        "  for correction in jfleg_dataset[\"corrections\"][i]:\n",
        "    print(\"\\tGold correction:     \", correction)\n",
        "  print(\"\\t\" + \"-\"*90)\n",
        "  print(\"\\tAutomatic correction:\", result.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rzI-7X3UuEm"
      },
      "source": [
        "❓[1.15] Is a valid correction produced for each input sentence?\n",
        "\n",
        "For seeing commercial grammar correction software in action, check out [Grammarly](grammarly.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOEXNNx0-UDy"
      },
      "source": [
        "---\n",
        "\n",
        "## Other NLP Tasks\n",
        "\n",
        "This section serves as a reference for several other NLP tasks (not part of in-class activity)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Hx5Rd5_2hJ"
      },
      "source": [
        "### Automatic Summarization\n",
        "\n",
        "**Single-document summarization** refers to the process of condensing a longer piece of text into a shorter version while retaining its main ideas and key points. In **multi-document summarization**, the task is to summarize a collection of documents.\n",
        "\n",
        "There are two main approaches to summarization: extractive and abstractive.\n",
        "\n",
        "**Extractive Summarization** involves selecting and combining the most important sentences or phrases from the original text to form a summary. It relies on identifying relevant sentences that already exist in the source text. Some problems associated with extractive summarization include:\n",
        "\n",
        "1. Redundancy: Extractive methods may include multiple similar or redundant sentences in the summary, leading to unnecessary repetition.\n",
        "2. Incoherence: Extracted sentences may not always fit together coherently, resulting in a summary that lacks overall coherence and cohesion.\n",
        "3. Lack of Paraphrasing: Extractive techniques do not involve paraphrasing or rewording the sentences. This can limit the expressiveness and novelty of the summary, making it a verbatim reproduction of the source text.\n",
        "\n",
        "**Abstractive summarization** involves generating new sentences that capture the essence of the original text, rather than directly selecting sentences. It aims to create summaries that may not be present in the source text but convey the same meaning. Some challenges with abstractive summarization are:\n",
        "\n",
        "1. Unfaithful Output: Abstractive summarization models may occasionally generate summaries that include incorrect or misleading information, deviating from the facts or intent of the original text.\n",
        "2. Coherence and Consistency: Abstractive methods often struggle with maintaining coherent and consistent summaries, as they need to generate novel sentences while ensuring they align with the overall context and tone of the original text.\n",
        "3. Overgeneralization or Undergeneralization: Abstractive methods may sometimes produce summaries that overgeneralize or undergeneralize the information, leading to misleading or incomplete representations of the source text.\n",
        "\n",
        "Both extractive and abstractive summarization methods have their own strengths and limitations, and ongoing research aims to address these challenges to improve the quality and reliability of automatic summarization systems, e.g., by combining the approaches above.\n",
        "\n",
        "Research also addresses **user-focused summarization**, where the aim is to generate summaries that cater specifically to the needs and preferences of individual users. The process involves analyzing the user's interests, preferences, and context to create personalized summaries that are highly relevant and useful to them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LST_GUle_35Y"
      },
      "source": [
        "### Intent Classification\n",
        "\n",
        "**Intent classification** is a text classification task within _conversational AI_ that involves determining the underlying intent or purpose behind a given text or user query. It plays a crucial role in various applications, such as chatbots, virtual assistants, and customer support systems, by enabling accurate understanding and appropriate response generation.\n",
        "\n",
        "Intent classification typically involves training a machine learning or deep learning model on labeled data, where each input is associated with a specific intent category. For example, a question like \"How can I return a product I purchased?\" is mapped to the _intent_ label `Return_Request`, which then triggers a specific response such as \"You can find information on returning a product on this website: [URL]\".\n",
        "The model learns to recognize patterns and features within the text to predict the intent of unseen inputs. The predicted intent can then be used to trigger relevant actions or responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdh4RHS1Dg8V"
      },
      "source": [
        "### Natural Language Inference (NLI)\n",
        "\n",
        "**Natural Language Inference** (NLI), also known as **Recognizing Textual Entailment** (RTE), is a task in NLP that involves determining the relationship between two given sentences: a premise and a hypothesis.\n",
        "\n",
        "For example, consider the following premise and hypothesis:\n",
        "\n",
        "Premise: \"The cat is sitting on the mat.\"<br/>\n",
        "Hypothesis: \"The mat is empty.\"\n",
        "\n",
        "In this example, the task of NLI is to determine the relationship between the premise and hypothesis, which is that the hypothesis contradicts the premise. The NLI system would classify this relationship as `contradiction`.\n",
        "\n",
        "The possible relationship categories in NLI are typically `entailment` (the hypothesis can be inferred from the premise), \"contradiction\" (the hypothesis contradicts the premise), or \"neutral\" (there is no clear relationship between the premise and hypothesis).\n",
        "\n",
        "Some people question whether this is a \"real\" NLP task, but there has been a lot of research on this topic in the past years, as it provides an interesting testbed for the reasoning capabilities of large language models. (Though some datasets have also been critized to include simply biases that the models simply pick up, but this is just a side note.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDifziGTA7qH"
      },
      "source": [
        "### Information Extraction\n",
        "\n",
        "Above, we have already learned about one **information extraction** task: named entity recognition. Once we have identified the entities that occur in a sentence and assigned them a coarse-grained entity type, we can next perform **entity linking** (also called **named entity disambiguation**), which finds, for each entity mention (i.e., an entity mentioned in a sentence), the corresponding real-world entity in a given inventory. For example, such an inventory can be a _knowledge graph_ such as [WikiData](https://www.wikidata.org/).\n",
        "\n",
        "**Relation extraction** is another information extraction task that involves identifying and extracting semantic relationships between entities mentioned in a text. It aims to determine the nature of the relationship between two entities, such as \"works-for,\" \"married-to,\" or \"located-in.\" Consider the following sentence: \"Barack Obama was born in Honolulu, Hawaii.\"\n",
        "In this example, the entities mentioned are \"Barack Obama\" and \"Honolulu, Hawaii.\" The relationship between these entities is that Barack Obama was born in Honolulu, Hawaii. Relation extraction aims to automatically identify and extract this relationship from the sentence.\n",
        "The extracted relation could be represented as follows `(Barack Obama, born-in, Honolulu, Hawaii)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wvQzyu5FKDb"
      },
      "source": [
        "### Semantic Parsing\n",
        "\n",
        "**Semantic Parsing** involves mapping natural language expressions to formal representations of their meaning. It aims to extract the underlying semantics or structured information from text and represent it in a machine-readable format.\n",
        "\n",
        "For example, consider the following natural language sentence and its corresponding semantic representation:\n",
        "\n",
        "Sentence: \"John likes to play the guitar.\"\n",
        "\n",
        "Semantic Representation: `likes_to_play(John, guitar)`\n",
        "\n",
        "In this example, the semantic parsing system analyzes the sentence and maps it to a structured representation using subject-predicate-object format. The semantic representation captures the subject (John), the predicate (likes to play), and the object (guitar) of the sentence.\n",
        "\n",
        "Semantic parsing enables machines to understand the relationships between entities in a sentence and extract the intended meaning. It has applications in various domains such as question answering, information retrieval, and knowledge graph construction.\n",
        "For example, we could use this method to fill a big knowledge graph recording who plays which instrument. We can then find all people who play a particular instrument using a query like `likes_to_play(*, guitar)`.\n",
        "\n",
        "A prominent implementation of an open information extraction system that performs semantic parsing is [OpenIE](https://nlp.stanford.edu/software/openie.html). Other semantic parsing approaches can, for example, parse directly to semantic representations such as SQL, [AMR](https://amr.isi.edu/), or [FrameNet](https://framenet.icsi.berkeley.edu/fndrupal/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRSQoByeqVt4"
      },
      "source": [
        "## Further Notes, Links & References\n",
        "\n",
        "Congratulations, you made it through a very long list of NLP tasks! Hopefully, you're very motivated now about learning the methods, datasets, and tools, the NLP community has come up with to solve them. You will also learn how to apply the methods and ideas to new problems and datasets, and you will learn about the challenges and limitations we face.\n",
        "\n",
        "The [NLPProgress](http://nlpprogress.com/) website maintains a list of NLP tasks together with datasets and the state-of-the-art scores achieved on datasets for each task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUiJ_y9SLVW6"
      },
      "source": [
        "## Homework\n",
        "\n",
        "The questions in this section are intended to get you started for some further self-study on the topic of this session. Search the web (but do not trust ChatGPT, please), read as much as you want, take notes, discuss with your fellow students!\n",
        "\n",
        "❗[1.16] Work through code of NLP tasks that you have not yet studied in detail and answer the related questions in Digicampus/Vips (graded self-test).\n",
        "\n",
        "❓ [1.17] Which definitions of **Natural Language Processing** can you find on the web / in the literature? Collect at least 3 definitions. Which sounds best to you?\n",
        "\n",
        "❓[1.18] What is the difference between **Natural Language Processing** vs. **Computational Linguistics** vs. **Speech Processing**?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Baku3hq3Gp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "u0qnSFHlEVs3",
        "hLNxCFmqEba6",
        "sfo-oNwqmrv1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
