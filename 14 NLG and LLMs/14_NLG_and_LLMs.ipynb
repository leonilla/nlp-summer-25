{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480THlo8zSZu"
      },
      "source": [
        "## Introduction to Natural Language Processing\n",
        "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
        "Prof. Dr. Annemarie Friedrich<br/>\n",
        "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
        "Date: **SS 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJyQlMcizVLU"
      },
      "source": [
        "## 14. Natural Language Generation and Large Language Models\n",
        "\n",
        "Generating text with HuggingFace and open-weights LLMs is actually quite easy.\n",
        "We will work with a small model (Qwen2-1.5B-Instruct) that we will load in a quantized format. You can find information on the model here: https://ai.azure.com/catalog/models/qwen-qwen2-1-5b-instruct\n",
        "\n",
        "### Quantization\n",
        "In full-precision, each parameter (weight) of the model takes 32bits in the memory. Quantization is a technique that compresses the model weights such that every parameter only takes 16-bit or 8-bit (or 4-bit if we really want to downsize). The performance is often not compromised a lot, though occasionally LLMs exhibit \"weird\" behavior when quantized.\n",
        "\n",
        "Want to learn more about this? --> You are welcome to join our \"Search Engines and Neural Information Retrieval\" course in your Master studies! ðŸ˜€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfa4hMdVzTeS",
        "outputId": "9972a304-61c4-4cd5-a469-aa26552a8dab"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "from transformers import pipeline, BitsAndBytesConfig\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device) # This must print True and \"cuda:0\"\n",
        "import timeit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qea5SJF0yfi",
        "outputId": "caa57216-d47b-4086-be6e-49c850dac270"
      },
      "outputs": [],
      "source": [
        "# This will take a while! You may have to restart the environment after installing.\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUeIIPzd0Ayd",
        "outputId": "376f5caf-6226-430f-8400-c2970725aafe"
      },
      "outputs": [],
      "source": [
        "# For our project, models will be provided in Licca scratch - talk to Fabio about which models you need!\n",
        "model_path = \"Qwen/Qwen2-1.5B-Instruct\"\n",
        "\n",
        "# Define how we want to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(load_in_16bit=True)\n",
        "\n",
        "# Define the pipeline that we use to access the model\n",
        "pipe = pipeline(\"text-generation\", model=model_path, model_kwargs={\"quantization_config\": quantization_config}, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBRNIN0N23RS"
      },
      "source": [
        "Inspect the data structure returned by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXtMdK2u2jz7",
        "outputId": "4bf0124a-074e-4890-9418-7de15a9e10bb"
      },
      "outputs": [],
      "source": [
        "# Define the messages that will be added to the prompt\n",
        "messages = [\n",
        "{\"role\" : \"system\", \"content\" : \"Hello\"},\n",
        "{\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "# The pipe function passes the messages to the model.\n",
        "generated_text = pipe(messages, max_new_tokens=100, do_sample=False, temperature=0.0)\n",
        "\n",
        "print(generated_text)\n",
        "\n",
        "# Task: print only the generated response text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EY2XZ0D5ev7"
      },
      "source": [
        "The pipeline can also process batches of instances as illustrated in the following code segment.\n",
        "\n",
        "You can ignore the following warning (it is due to an [inconsistency in the HuggingFace library](https://github.com/Vision-CAIR/MiniGPT-4/issues/129)):\n",
        "\n",
        "\n",
        "> A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
        "\n",
        "Try out various batch sizes and observe how long it takes.\n",
        "\n",
        "ðŸ’¡Make use of batch processing in your software project!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m6Ga8L33LLY",
        "outputId": "ad5cd5be-644e-4160-de35-00f6c8a122bb"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 2\n",
        "\n",
        "# Prepare a batch of messages\n",
        "batch_messages = [\n",
        "    [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me a short story about a dog.\"}],\n",
        "    [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me a short story about a cat.\"}],\n",
        "    [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Write a poem about the ocean.\"}],\n",
        "    [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Explain quantum physics in simple terms.\"}],\n",
        "]\n",
        "\n",
        "# Use the pipeline to process the batch of messages\n",
        "start = timeit.default_timer()\n",
        "batch_results = pipe(batch_messages, max_new_tokens=256, do_sample=False, batch_size=BATCH_SIZE)\n",
        "end = timeit.default_timer()\n",
        "\n",
        "print(\"Batch processing took\", \"{:.1f}\".format(end - start), \"seconds\")\n",
        "\n",
        "# Print the generated text for each item in the batch\n",
        "for i, result in enumerate(batch_results):\n",
        "    print(f\"Result for item {i+1}:\")\n",
        "    print(result)\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mALw2bfG7crY"
      },
      "source": [
        "This coding approach should work similarly with other open-weight models when you access them via HuggingFace.\n",
        "\n",
        "Hints: For using LLama/Gemma, first create and set HuggingFace access token following this tutorial:\n",
        "https://pyimagesearch.com/2025/04/04/configure-your-hugging-face-access-token-in-colab-environment/\n",
        "\n",
        "It may take several hours to 1â€“2 days for your access to be approved"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
