{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWTaayb9q5ns"
   },
   "source": [
    "## Introduction to Natural Language Processing\n",
    "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
    "Prof. Dr. Annemarie Friedrich<br/>\n",
    "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
    "Date: **SS 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5w2Jlq6q7Ri"
   },
   "source": [
    "# 9. Attention + 10. Transformers (Homework)\n",
    "\n",
    "__Recommendation:__ Use a GPU for the second part of the homework, e.g., in Google Colab Runtime --> Change Runtime --> GPU --> T4.\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "* Explain why we need contextualized word embeddings\n",
    "* Compute the forward pass of attention\n",
    "* Explain the encoder-decoder architecture\n",
    "* Explain the encoder block of the transformer\n",
    "* Use BERT-specific implementation details\n",
    "* Obtain an overview of influential pre-trained transformers\n",
    "* Scientific reading + presentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiPVhD0Aq4aH"
   },
   "outputs": [],
   "source": [
    "# Installations\n",
    "!pip install -U datasets\n",
    "!pip install transformers\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# This is true if you are working on an GPU.\n",
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNoOtY7src-_"
   },
   "source": [
    "## Implementing Attention in PyTorch\n",
    "\n",
    "❓ Your task is to implement the matrix-based calculation of self-attention using PyTorch. You will make use of the following functions and classes:\n",
    "\n",
    "* `torch.Tensor`: keep in mind that even scalar values must be implemented as a Tensor object.\n",
    "* `torch.sqrt`: computes the square root.\n",
    "* `torch.mm`: matrix multiplication.\n",
    "\n",
    "Pytorch provides a function for computing the softmax over a tensor. The function must first be created as an object of the `torch.nn.Softmax` class. (In Python, even functions are objects.)\n",
    "\n",
    "Note over which dimension the softmax is computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Izk8quDW4cES"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 2., 3.]])\n",
      "\n",
      "softmax with dim=0\n",
      " tensor([[0.5000, 0.2689, 0.1192],\n",
      "        [0.5000, 0.7311, 0.8808]])\n",
      "-> softmax is computed for each column\n",
      "\n",
      "softmax with dim=1\n",
      " tensor([[0.3333, 0.3333, 0.3333],\n",
      "        [0.0900, 0.2447, 0.6652]])\n",
      "-> softmax is computed for each row\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1, 1, 1], [1, 2, 3]])\n",
    "print(a)\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "print(\"\\nsoftmax with dim=0\\n\", softmax(a))\n",
    "print(\"-> softmax is computed for each column\\n\")\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "print(\"softmax with dim=1\\n\", softmax(a))\n",
    "print(\"-> softmax is computed for each row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzs3ZTru5GIj"
   },
   "source": [
    "For computing the matrix-based calulation of self-attention for one input sentence (we do not have a batch dimension here!), follow the explanations of [Jay Alammar - The Illustrated Transfromer](http://jalammar.github.io/illustrated-transformer/).\n",
    "\n",
    "First compute all query, key, and value vectors:\n",
    "\n",
    "![Image illustrating how to compute Q, K and V](http://jalammar.github.io/images/t/self-attention-matrix-calculation.png)\n",
    "\n",
    "Then, compute the attention weights via softmax and use them to create sums of the value vectors to get Z:\n",
    "\n",
    "![Image illustrating how to compute Z using the attention weights computed using softmax](http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n",
    "\n",
    "\n",
    "\n",
    "This is your input:\n",
    "\n",
    "$x_1 = [0.2, 0.04, 0.8, 0.09]$\n",
    "\n",
    "$x_2 = [0.1, 0.31, 0.13, 0.06]$\n",
    "\n",
    "$x_3 = [0.1, 0.4, 0.07, 0.1]$\n",
    "\n",
    "$W_q = \\begin{pmatrix}\n",
    "4 & 2\\\\\n",
    "1 & 6\\\\\n",
    "1 & 1\\\\\n",
    "2 & 1\\\\\n",
    "\\end{pmatrix}\n",
    "W_k = \\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "2 & 5\\\\\n",
    "0 & 3\\\\\n",
    "8 & 2\\\\\n",
    "\\end{pmatrix}\n",
    "W_v = \\begin{pmatrix}\n",
    "3 & 1\\\\\n",
    "3 & 5\\\\\n",
    "1 & 0\\\\\n",
    "2 & 4\\\\\n",
    "\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdQRqx5HKFWg"
   },
   "outputs": [],
   "source": [
    "x1 = torch.Tensor([[0.2, 0.04, 0.8, 0.09]])\n",
    "print(x1)\n",
    "x2 = torch.Tensor([[0.1, 0.31, 0.13, 0.06]])\n",
    "print(x2)\n",
    "x3 = torch.Tensor([[0.1, 0.4, 0.07, 0.1]])\n",
    "print(x3)\n",
    "\n",
    "w_q = torch.tensor([[4, 2], [1, 6], [1, 1], [2, 1]])\n",
    "print(w_q)\n",
    "w_k = torch.tensor([[1, 1], [2, 5], [0, 3], [8, 2]])\n",
    "print(w_k)\n",
    "w_v = torch.tensor([[3, 1], [3, 5], [1, 0], [2, 4]])\n",
    "print(w_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyrYnIXjLCYM"
   },
   "source": [
    "## Finetuning BERT\n",
    "\n",
    "In this homework, we'll use a \"vanilla BERT\" as provided by the HuggingFace transformers library that we just modify for binary classification to predict whether a sentence (in our dataset) is objective or subjective. We focus on the English part of the dataset described in this paper:\n",
    "[Antici et al.: A Corpus for Sentence-level Subjectivity Detection on English News Articles. 2023.](https://arxiv.org/abs/2305.18034)\n",
    "\n",
    "Take a brief look at the paper and at the HuggingFace [dataset card](https://huggingface.co/datasets/tasksource/subjectivity/viewer/tasksource--subjectivity/train?p=7) to get an idea of what the task is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuYy33miX-Vo"
   },
   "outputs": [],
   "source": [
    "# Define the device we'll use for tensor computations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Computing on:\", device)\n",
    "\n",
    "# Imports\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "train_data = load_dataset(\"tasksource/subjectivity\", split=\"train\")\n",
    "val_data = load_dataset(\"tasksource/subjectivity\", split=\"validation\")\n",
    "test_data = load_dataset(\"tasksource/subjectivity\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqLGY_VynEMC"
   },
   "source": [
    "❓ It's ALWAYS a good idea to first LOOK AT THE DATA. Compute the label distributions for the train, val, and test splits. (Recommendation: Write a function `print_label_dist(data_set)` that takes in a datasplit and prints out its label distribution. Print out a few instances to familiarize yourself with the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqaJCAJSnhHS"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSmRwid1pOSk"
   },
   "source": [
    "\n",
    "### Tokenization\n",
    "\n",
    "We can call the tokenizer object directly (see __call__: https://huggingface.co/docs/transformers/main_classes/tokenizer).\n",
    "It splits the text into word-piece tokens and returns a list containing tensors for the input_ids, the token_type_ids (which we will not need today), and the attention_mask (more on this below).\n",
    "The `encoding` data structure looks like this:\n",
    "\n",
    "```\n",
    "{'input_ids': tensor([[  101,  6620, 22933,  2869,  2018,  2815, 27836,\n",
    "      1010,  3038,  1996,  6514,  2231, 10858,  1996,  3423,  3691,  2000,\n",
    "      17542,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0,\n",
    "      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1,\n",
    "      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQ-zbA_mOGre"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # instantiate the tokenizer that corresponds to the model that we will use\n",
    "\n",
    "def tokenize(data_set):\n",
    "  input_data = []\n",
    "  max_len = 0\n",
    "  for i in range(len(data_set)):\n",
    "    # use the tokenizer to tokenize the data\n",
    "    encoding = tokenizer(data_set[\"Sentence\"][i], return_tensors='pt', add_special_tokens=True, \\\n",
    "                         return_attention_mask=True)\n",
    "    max_len = max(max_len, len(encoding['input_ids'].squeeze()))\n",
    "    encoding[\"input_ids\"] = encoding[\"input_ids\"].squeeze()\n",
    "    encoding[\"attention_mask\"] = encoding[\"attention_mask\"].squeeze()\n",
    "    encoding[\"token_type_ids\"] = encoding[\"token_type_ids\"].squeeze()\n",
    "    encoding = encoding.to(device)\n",
    "    input_data.append(encoding)\n",
    "  return input_data, max_len\n",
    "\n",
    "train_input, max_len = tokenize(train_data)\n",
    "val_input, _ = tokenize(val_data)\n",
    "test_input, _ = tokenize(test_data)\n",
    "\n",
    "print(train_input[0])\n",
    "print(tokenizer.convert_ids_to_tokens(train_input[0][\"input_ids\"].squeeze()))\n",
    "print()\n",
    "print(\"Maximum number of tokens in training set:\", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s214QhsEp0NK"
   },
   "source": [
    "### Padding and Truncation\n",
    "For the sake of understanding, we will perform __padding__ and __truncation__ manually as a preprocessing step today. (Later, you can use a `collate_fn` function that you pass to the dataloader, which only pads/truncates per batch to optimize speed, or you can use the tokenizer (see link above) to perform these steps for you.)\n",
    "\n",
    "The inputs to the neural network, if computed using tensors on a GPU, must have the exact same dimensions. Remember that our input tensors are a list of token IDs. But not every sentence has the same number of tokens! Also, even large language models have a limited size of tensors that they accept as input, e.g., for the bert-base model, this number is 512 word-piece tokens. Hence, we need to do two things to make our inputs compatible such that they can be passed to the model as one batch:\n",
    "\n",
    "* Define a maximum length of the vectors that represent each input (sentence, short text, ...). Today, we'll use the maximum length of the training set inputs. They fit into BERT.\n",
    "* But wait, what if the validation/test set have a longer sentence? We also need to make sure to cut off (truncate) the input sequences as the predefined maximum length to ensure that they will fit into a tensor (in our case: a tensor that looks like a matrix).\n",
    "\n",
    "The following function performs these steps. Read it carefully and try to understand each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs0b6C0zx5wX"
   },
   "outputs": [],
   "source": [
    "# Padding / Truncation\n",
    "def pad_truncate(data_set, max_len):\n",
    "  # Make sure all input tensors are of length max_len\n",
    "  for i in range(len(data_set)):\n",
    "    instance = data_set[i]\n",
    "    # Pad the input_ids\n",
    "    zeros = torch.zeros(max_len-len(instance[\"input_ids\"].squeeze()), dtype=torch.long, device=device)\n",
    "    instance[\"input_ids\"] = torch.cat((instance[\"input_ids\"], zeros))\n",
    "    instance[\"input_ids\"] = instance[\"input_ids\"].squeeze()[:max_len].unsqueeze(0) # slicing as in Python lists, the squeeze() removes the batch dimension here, unsqueeze(0) adds it back\n",
    "    # Pad the masks\n",
    "    instance[\"token_type_ids\"] = torch.zeros(max_len, dtype=torch.long, device=device) # these are only needed if we perform sentence pairs tasks\n",
    "    instance[\"attention_mask\"] = torch.cat((instance[\"attention_mask\"], zeros)) # these are needed such that the models knows where to perform attention\n",
    "    # remove batch dimensions\n",
    "    instance[\"input_ids\"] = instance[\"input_ids\"].squeeze()\n",
    "    #instance[\"token_type_ids\"] = instance[\"token_type_ids\"].squeeze()\n",
    "    instance[\"attention_mask\"] = instance[\"attention_mask\"].squeeze()\n",
    "  return data_set\n",
    "\n",
    "\n",
    "train_input = pad_truncate(train_input, max_len)\n",
    "val_input = pad_truncate(val_input, max_len)\n",
    "test_input = pad_truncate(test_input, max_len)\n",
    "print(train_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQ4vFKu3rNi6"
   },
   "source": [
    "### Labels to Tensors\n",
    "\n",
    "❓ We have now represented our input data as tensors. Next, we need to map the labels to a single list of labels, respecting the order in our dataset. Write a function `get_labels(data_set)` that returns such a list for each datasplit. Replace the None values below with the return values of the calls to this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRv9ohiHrgPX"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "train_labels = None\n",
    "dev_labels = None\n",
    "test_labels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHOXx-Hjr0i8"
   },
   "source": [
    "### PyTorch Dataset: BERT Input\n",
    "\n",
    "The pre-trained BERT model expects not only the input_ids as input, but also the token_type_ids and the attention_masks that we have computed earlier.\n",
    "Each input instance will consist of one dictionary (as above). The DataLoader will combine the values of these dictionaries into tensors that contain all the instances that will be used in one batch. The `__get_item__` method only needs to return this data structure for one instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRL8eORWUWNe"
   },
   "outputs": [],
   "source": [
    "# Preparing a custom dataset for BERT\n",
    "class SubjectivityDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32, device=device)\n",
    "        item = {\"input_ids\" : self.encodings[idx][\"input_ids\"],\n",
    "                \"token_type_ids\" : self.encodings[idx][\"token_type_ids\"],\n",
    "                \"attention_mask\" : self.encodings[idx][\"attention_mask\"]}\n",
    "        return item, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SubjectivityDataset(train_input, train_labels)\n",
    "print(train_dataset[0])\n",
    "val_dataset = SubjectivityDataset(val_input, val_labels)\n",
    "test_dataset = SubjectivityDataset(test_input, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_27O_fnsV_o"
   },
   "source": [
    "### Adapting BERT\n",
    "The code below shows an example of using a pre-trained BERT model as an \"embedding layer\" in a neural network. The original model was pre-trained with a classification layer which we will ignore. Instead, we retrieve the embedding for the CLS token (the first embedding of the last hidden layer of BERT) and feed this into a linear layer predicting a z-score, and then pass this z through the sigmoid function to compute a probability score. When fine-tuning the model, the embeddings will also slightly change. However, if we would do this with a huge randomly initialized model, we would likely not achieve anything meaningful with just a small training dataset. Adapting the model to our task and domain, however, is highly effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqXmCbaygntW"
   },
   "outputs": [],
   "source": [
    "class MyFinetunedModel(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    # max_len is the number of input_ids per token\n",
    "    super(MyFinetunedModel, self).__init__()\n",
    "    self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased') #, return_dict=True)\n",
    "    self.linear = torch.nn.Linear(768, 1) # map from one BERT token embedding to a single scalar\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    outputs = self.bert(**inputs, output_hidden_states=True) # obtain embeddings for inputs\n",
    "    last_hidden_state = outputs.hidden_states[-1] # hidden state values of last BERT layer\n",
    "    cls_embedding = last_hidden_state[:,0,:] # selects the 768-dimensional embedding output for CLS\n",
    "    # see: https://huggingface.co/docs/transformers/main_classes/output\n",
    "    # (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size))\n",
    "    logit = self.linear(cls_embedding) # Classifier layer mapping embedding of CLS token to logit\n",
    "    score = self.sigmoid(logit)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icRQm_TRv9Wg"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "  # Compute accuracy of model on data provided by data_loader\n",
    "  correct = 0\n",
    "  num_instances = len(data_loader.dataset)\n",
    "  with torch.no_grad(): # This tells the model that we're not training\n",
    "                        # Will not remember gradients for this block\n",
    "    model.eval()\n",
    "    for X, y in iter(data_loader):\n",
    "      y_probs = model(X)\n",
    "      y_probs = y_probs.squeeze(1)\n",
    "      y_pred = torch.where(y_probs >= 0.5, 1, 0.)\n",
    "      correct += (y_pred == y).float().sum()\n",
    "\n",
    "  accuracy = 100 * correct / num_instances\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEh65nauTZjk"
   },
   "outputs": [],
   "source": [
    "# Always fun with the random seeds ...\n",
    "# We need to set them such that our results will be replicable.\n",
    "# (Hint: for an experiment later, you can change the random seed here and check what happens.\n",
    "# But for now, let's keep the answer to all questions of the universe, 42.)\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "if cuda_available:\n",
    "  # This is needed on Colab as we are working in a distributed environment\n",
    "  # If you are working in a different GPU environment, you can probably omit this line if it results in errors.\n",
    "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "\n",
    "# Should we still have some source for non-determinism in our code, this will complain:\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Instantiate the model             #\n",
    "#####################################\n",
    "\n",
    "model = MyFinetunedModel()\n",
    "model = model.to(device)\n",
    "\n",
    "#####################################\n",
    "# Training / Fine-tuning the model  #\n",
    "#####################################\n",
    "\n",
    "num_epochs = 12\n",
    "batch_size = 64\n",
    "learning_rate = 1e-5\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate) # Always required for BERT!\n",
    "loss_fn = torch.nn.BCELoss() # This loss function does not include sigmoid.\n",
    "# Side note: if you exclude the sigmoid above, you have to use:\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "\n",
    "# ... and the rest of the code: is just as before!\n",
    "\n",
    "# Data Loaders\n",
    "data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "data_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "data_loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training\n",
    "for n in range(num_epochs):\n",
    "  model.train()\n",
    "  it = iter(data_loader_train)  # Create the iterator from the training dataset\n",
    "  epoch_loss, steps = 0, 0      # To keep track of the current epoch's loss\n",
    "\n",
    "  for  X, y in it:              # Obtain a tensor X = batch of X-values, y accordingly\n",
    "    y_pred = model(X)           # Have our model with current weights make a prediction\n",
    "    y_pred = y_pred.squeeze(1)  # Removes the extra batch dimension (technical trick)\n",
    "    loss = loss_fn(y_pred, y)   # Have the loss function compute the loss value\n",
    "    optimizer.zero_grad()       # Reset the optimizer (otherwise it accumulates results - would be wrong here)\n",
    "    loss.backward()             # Compute the gradients (partial derivatives)\n",
    "    optimizer.step()            # Update the network's weights\n",
    "    epoch_loss += loss          # For tracking the epoch's loss\n",
    "    steps += 1\n",
    "\n",
    "  print(\"\\nEpoch:\", n+1, \"    Loss: {:0.4f}\".format(epoch_loss/steps))\n",
    "  # evaluate model at end of epoch\n",
    "  print(\"Training accuracy:   {:2.1f}\".format(evaluate(model, data_loader_train)))\n",
    "  print(\"Validation accuracy: {:2.1f}\".format(evaluate(model, data_loader_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f5VGi5U3hFQ"
   },
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "print(\"     Test accuracy: {:2.1f}\".format(evaluate(model, data_loader_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAXbBwhatelv"
   },
   "source": [
    "❓ When you look at the training logs above: does the model overfit? After which epoch could we have stopped the training?\n",
    "\n",
    "Congratulations, you have just fine-tuned your first BERT-based model. Make sure to work through the code above carefully such that you understand each line. Take notes. In future tasks, you will have to modify the code further."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMMx2+EfnGQyoL7AJxvzlEj",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1q80BpWBlrLLJ4WOzZ0lYoaebofmVqxce",
     "timestamp": 1688572229822
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
