{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWTaayb9q5ns"
      },
      "source": [
        "## Introduction to Natural Language Processing\n",
        "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
        "Prof. Dr. Annemarie Friedrich<br/>\n",
        "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
        "Date: **SS 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5w2Jlq6q7Ri"
      },
      "source": [
        "# 9. Attention + 10. Transformers (Homework)\n",
        "\n",
        "__Recommendation:__ Use a GPU for the second part of the homework, e.g., in Google Colab Runtime --> Change Runtime --> GPU --> T4.\n",
        "\n",
        "**Learning Goals**\n",
        "\n",
        "* Explain why we need contextualized word embeddings\n",
        "* Compute the forward pass of attention\n",
        "* Explain the encoder-decoder architecture\n",
        "* Explain the encoder block of the transformer\n",
        "* Use BERT-specific implementation details\n",
        "* Obtain an overview of influential pre-trained transformers\n",
        "* Scientific reading + presentations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OiPVhD0Aq4aH",
        "outputId": "415cc7fe-c68a-480c-c14d-77137fd78a28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "# Installations\n",
        "!pip install -U datasets\n",
        "!pip install transformers\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# This is true if you are working on an GPU.\n",
        "cuda_available = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNoOtY7src-_"
      },
      "source": [
        "## Implementing Attention in PyTorch\n",
        "\n",
        "❓ Your task is to implement the matrix-based calculation of self-attention using PyTorch. You will make use of the following functions and classes:\n",
        "\n",
        "* `torch.Tensor`: keep in mind that even scalar values must be implemented as a Tensor object.\n",
        "* `torch.sqrt`: computes the square root.\n",
        "* `torch.mm`: matrix multiplication.\n",
        "\n",
        "Pytorch provides a function for computing the softmax over a tensor. The function must first be created as an object of the `torch.nn.Softmax` class. (In Python, even functions are objects.)\n",
        "\n",
        "Note over which dimension the softmax is computed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Izk8quDW4cES",
        "outputId": "7f30e476-b309-413e-c014-35b7f9f72d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 2., 3.]])\n",
            "\n",
            "softmax with dim=0\n",
            " tensor([[0.5000, 0.2689, 0.1192],\n",
            "        [0.5000, 0.7311, 0.8808]])\n",
            "-> softmax is computed for each column\n",
            "\n",
            "softmax with dim=1\n",
            " tensor([[0.3333, 0.3333, 0.3333],\n",
            "        [0.0900, 0.2447, 0.6652]])\n",
            "-> softmax is computed for each row\n"
          ]
        }
      ],
      "source": [
        "a = torch.Tensor([[1, 1, 1], [1, 2, 3]])\n",
        "print(a)\n",
        "\n",
        "softmax = torch.nn.Softmax(dim=0)\n",
        "print(\"\\nsoftmax with dim=0\\n\", softmax(a))\n",
        "print(\"-> softmax is computed for each column\\n\")\n",
        "\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "print(\"softmax with dim=1\\n\", softmax(a))\n",
        "print(\"-> softmax is computed for each row\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzs3ZTru5GIj"
      },
      "source": [
        "For computing the matrix-based calulation of self-attention for one input sentence (we do not have a batch dimension here!), follow the explanations of [Jay Alammar - The Illustrated Transfromer](http://jalammar.github.io/illustrated-transformer/).\n",
        "\n",
        "First compute all query, key, and value vectors:\n",
        "\n",
        "![Image illustrating how to compute Q, K and V](http://jalammar.github.io/images/t/self-attention-matrix-calculation.png)\n",
        "\n",
        "Then, compute the attention weights via softmax and use them to create sums of the value vectors to get Z:\n",
        "\n",
        "![Image illustrating how to compute Z using the attention weights computed using softmax](http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n",
        "\n",
        "\n",
        "\n",
        "This is your input:\n",
        "\n",
        "$x_1 = [0.2, 0.04, 0.8, 0.09]$\n",
        "\n",
        "$x_2 = [0.1, 0.31, 0.13, 0.06]$\n",
        "\n",
        "$x_3 = [0.1, 0.4, 0.07, 0.1]$\n",
        "\n",
        "$W_q = \\begin{pmatrix}\n",
        "4 & 2\\\\\n",
        "1 & 6\\\\\n",
        "1 & 1\\\\\n",
        "2 & 1\\\\\n",
        "\\end{pmatrix}\n",
        "W_k = \\begin{pmatrix}\n",
        "1 & 1\\\\\n",
        "2 & 5\\\\\n",
        "0 & 3\\\\\n",
        "8 & 2\\\\\n",
        "\\end{pmatrix}\n",
        "W_v = \\begin{pmatrix}\n",
        "3 & 1\\\\\n",
        "3 & 5\\\\\n",
        "1 & 0\\\\\n",
        "2 & 4\\\\\n",
        "\\end{pmatrix}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rdQRqx5HKFWg",
        "outputId": "977a9a7c-6086-49f0-fe3b-54ff0d0b3fdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor X (matrix) containing x_1, x_2, x_3:\n",
            "tensor([[0.2000, 0.0400, 0.8000, 0.0900],\n",
            "        [0.1000, 0.3100, 0.1300, 0.0600],\n",
            "        [0.1000, 0.4000, 0.0700, 0.1000]])\n",
            "Tensor w_q (matrix) for query weights:\n",
            "tensor([[4., 2.],\n",
            "        [1., 6.],\n",
            "        [1., 1.],\n",
            "        [2., 1.]])\n",
            "Tensor w_k (matrix) for key weights:\n",
            "tensor([[1., 1.],\n",
            "        [2., 5.],\n",
            "        [0., 3.],\n",
            "        [8., 2.]])\n",
            "Tensor w_v (matrix) for value weights:\n",
            "tensor([[3., 1.],\n",
            "        [3., 5.],\n",
            "        [1., 0.],\n",
            "        [2., 4.]])\n",
            "--------------------------------------------------\n",
            "MULTIPLY X WITH EACH OF THE WEIGHT MATRIXES:\n",
            "Result: Queries matrix:\n",
            "tensor([[1.8200, 1.5300],\n",
            "        [0.9600, 2.2500],\n",
            "        [1.0700, 2.7700]])\n",
            "Result: Keys matrix:\n",
            "tensor([[1.0000, 2.9800],\n",
            "        [1.2000, 2.1600],\n",
            "        [1.7000, 2.5100]])\n",
            "Result: Values matrix:\n",
            "tensor([[1.7000, 0.7600],\n",
            "        [1.4800, 1.8900],\n",
            "        [1.7700, 2.5000]])\n",
            "--------------------------------------------------\n",
            "TRANSPOSE K:\n",
            "Result: Transposed K:\n",
            "tensor([[1.0000, 1.2000, 1.7000],\n",
            "        [2.9800, 2.1600, 2.5100]])\n",
            "--------------------------------------------------\n",
            "MULTIPLY Q AND K+, DIVIDE BY SQRT OF DIM(Q):\n",
            "Result: Argument for the Softmax:\n",
            "tensor([[4.5109, 3.8812, 4.9033],\n",
            "        [5.4200, 4.2511, 5.1474],\n",
            "        [6.5935, 5.1387, 6.2025]])\n",
            "--------------------------------------------------\n",
            "SOFTMAXING PER ROW (dim = 1):\n",
            "Result: Softmaxed Q x K+:\n",
            "tensor([[0.3319, 0.1768, 0.4913],\n",
            "        [0.4826, 0.1500, 0.3675],\n",
            "        [0.5236, 0.1222, 0.3542]])\n",
            "--------------------------------------------------\n",
            "Finally, MULTIPLY SOFTMAX BY V:\n",
            "Result: z:\n",
            "tensor([[1.6955, 1.8147],\n",
            "        [1.6927, 1.5688],\n",
            "        [1.6979, 1.5144]])\n"
          ]
        }
      ],
      "source": [
        "# Tensor X (matrix) for holding our instances x_1, x_2, x_3\n",
        "print(f\"Tensor X (matrix) containing x_1, x_2, x_3:\")\n",
        "X = torch.Tensor([[0.2, 0.04, 0.8, 0.09], [0.1, 0.31, 0.13, 0.06], [0.1, 0.4, 0.07, 0.1]])\n",
        "print(X)\n",
        "\n",
        "print(f\"Tensor w_q (matrix) for query weights:\")\n",
        "w_q = torch.tensor([[4., 2.], [1., 6.], [1., 1.], [2., 1.]])\n",
        "print(w_q)\n",
        "\n",
        "print(f\"Tensor w_k (matrix) for key weights:\")\n",
        "w_k = torch.tensor([[1., 1.], [2., 5.], [0., 3.], [8., 2.]])\n",
        "print(w_k)\n",
        "\n",
        "print(f\"Tensor w_v (matrix) for value weights:\")\n",
        "w_v = torch.tensor([[3., 1.], [3., 5.], [1., 0.], [2., 4.]])\n",
        "print(w_v)\n",
        "print('-' * 50)\n",
        "\n",
        "print(f\"MULTIPLY X WITH EACH OF THE WEIGHT MATRIXES:\")\n",
        "print(f\"Result: Queries matrix:\")\n",
        "Q = torch.mm(X, w_q)\n",
        "print(Q)\n",
        "\n",
        "print(f\"Result: Keys matrix:\")\n",
        "K = torch.mm(X, w_k)\n",
        "print(K)\n",
        "\n",
        "print(f\"Result: Values matrix:\")\n",
        "V = torch.mm(X, w_v)\n",
        "print(V)\n",
        "print('-' * 50)\n",
        "\n",
        "print(f\"TRANSPOSE K:\")\n",
        "print(f\"Result: Transposed K:\")\n",
        "K = torch.transpose(K, 0, 1)\n",
        "print(K)\n",
        "print('-' * 50)\n",
        "\n",
        "print(f\"MULTIPLY Q AND K+, DIVIDE BY SQRT OF DIM(Q):\")\n",
        "print(f\"Result: Argument for the Softmax:\")\n",
        "softmax_arg = torch.mm(Q, K)/torch.sqrt(torch.Tensor([2]))\n",
        "print(softmax_arg)\n",
        "print('-' * 50)\n",
        "\n",
        "print(f\"SOFTMAXING PER ROW (dim = 1):\")\n",
        "print(f\"Result: Softmaxed Q x K+:\")\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "softmax_res = softmax(softmax_arg)\n",
        "print(softmax_res)\n",
        "print('-' * 50)\n",
        "\n",
        "print(f\"Finally, MULTIPLY SOFTMAX BY V:\")\n",
        "print(f\"Result: z:\")\n",
        "z = torch.mm(softmax_res, V)\n",
        "print(z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrYnIXjLCYM"
      },
      "source": [
        "## Finetuning BERT\n",
        "\n",
        "In this homework, we'll use a \"vanilla BERT\" as provided by the HuggingFace transformers library that we just modify for binary classification to predict whether a sentence (in our dataset) is objective or subjective. We focus on the English part of the dataset described in this paper:\n",
        "[Antici et al.: A Corpus for Sentence-level Subjectivity Detection on English News Articles. 2023.](https://arxiv.org/abs/2305.18034)\n",
        "\n",
        "Take a brief look at the paper and at the HuggingFace [dataset card](https://huggingface.co/datasets/tasksource/subjectivity/viewer/tasksource--subjectivity/train?p=7) to get an idea of what the task is about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuYy33miX-Vo"
      },
      "outputs": [],
      "source": [
        "# Define the device we'll use for tensor computations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Computing on:\", device)\n",
        "\n",
        "# Imports\n",
        "from transformers import BertForSequenceClassification\n",
        "import torch.optim as optim\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load the dataset\n",
        "train_data = load_dataset(\"tasksource/subjectivity\", split=\"train\")\n",
        "val_data = load_dataset(\"tasksource/subjectivity\", split=\"validation\")\n",
        "test_data = load_dataset(\"tasksource/subjectivity\", split=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLGY_VynEMC"
      },
      "source": [
        "❓ It's ALWAYS a good idea to first LOOK AT THE DATA. Compute the label distributions for the train, val, and test splits. (Recommendation: Write a function `print_label_dist(data_set)` that takes in a datasplit and prints out its label distribution. Print out a few instances to familiarize yourself with the data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqaJCAJSnhHS"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSmRwid1pOSk"
      },
      "source": [
        "\n",
        "### Tokenization\n",
        "\n",
        "We can call the tokenizer object directly (see __call__: https://huggingface.co/docs/transformers/main_classes/tokenizer).\n",
        "It splits the text into word-piece tokens and returns a list containing tensors for the input_ids, the token_type_ids (which we will not need today), and the attention_mask (more on this below).\n",
        "The `encoding` data structure looks like this:\n",
        "\n",
        "```\n",
        "{'input_ids': tensor([[  101,  6620, 22933,  2869,  2018,  2815, 27836,\n",
        "      1010,  3038,  1996,  6514,  2231, 10858,  1996,  3423,  3691,  2000,\n",
        "      17542,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0,\n",
        "      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1,\n",
        "      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ-zbA_mOGre"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # instantiate the tokenizer that corresponds to the model that we will use\n",
        "\n",
        "def tokenize(data_set):\n",
        "  input_data = []\n",
        "  max_len = 0\n",
        "  for i in range(len(data_set)):\n",
        "    # use the tokenizer to tokenize the data\n",
        "    encoding = tokenizer(data_set[\"Sentence\"][i], return_tensors='pt', add_special_tokens=True, \\\n",
        "                         return_attention_mask=True)\n",
        "    max_len = max(max_len, len(encoding['input_ids'].squeeze()))\n",
        "    encoding[\"input_ids\"] = encoding[\"input_ids\"].squeeze()\n",
        "    encoding[\"attention_mask\"] = encoding[\"attention_mask\"].squeeze()\n",
        "    encoding[\"token_type_ids\"] = encoding[\"token_type_ids\"].squeeze()\n",
        "    encoding = encoding.to(device)\n",
        "    input_data.append(encoding)\n",
        "  return input_data, max_len\n",
        "\n",
        "train_input, max_len = tokenize(train_data)\n",
        "val_input, _ = tokenize(val_data)\n",
        "test_input, _ = tokenize(test_data)\n",
        "\n",
        "print(train_input[0])\n",
        "print(tokenizer.convert_ids_to_tokens(train_input[0][\"input_ids\"].squeeze()))\n",
        "print()\n",
        "print(\"Maximum number of tokens in training set:\", max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s214QhsEp0NK"
      },
      "source": [
        "### Padding and Truncation\n",
        "For the sake of understanding, we will perform __padding__ and __truncation__ manually as a preprocessing step today. (Later, you can use a `collate_fn` function that you pass to the dataloader, which only pads/truncates per batch to optimize speed, or you can use the tokenizer (see link above) to perform these steps for you.)\n",
        "\n",
        "The inputs to the neural network, if computed using tensors on a GPU, must have the exact same dimensions. Remember that our input tensors are a list of token IDs. But not every sentence has the same number of tokens! Also, even large language models have a limited size of tensors that they accept as input, e.g., for the bert-base model, this number is 512 word-piece tokens. Hence, we need to do two things to make our inputs compatible such that they can be passed to the model as one batch:\n",
        "\n",
        "* Define a maximum length of the vectors that represent each input (sentence, short text, ...). Today, we'll use the maximum length of the training set inputs. They fit into BERT.\n",
        "* But wait, what if the validation/test set have a longer sentence? We also need to make sure to cut off (truncate) the input sequences as the predefined maximum length to ensure that they will fit into a tensor (in our case: a tensor that looks like a matrix).\n",
        "\n",
        "The following function performs these steps. Read it carefully and try to understand each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs0b6C0zx5wX"
      },
      "outputs": [],
      "source": [
        "# Padding / Truncation\n",
        "def pad_truncate(data_set, max_len):\n",
        "  # Make sure all input tensors are of length max_len\n",
        "  for i in range(len(data_set)):\n",
        "    instance = data_set[i]\n",
        "    # Pad the input_ids\n",
        "    zeros = torch.zeros(max_len-len(instance[\"input_ids\"].squeeze()), dtype=torch.long, device=device)\n",
        "    instance[\"input_ids\"] = torch.cat((instance[\"input_ids\"], zeros))\n",
        "    instance[\"input_ids\"] = instance[\"input_ids\"].squeeze()[:max_len].unsqueeze(0) # slicing as in Python lists, the squeeze() removes the batch dimension here, unsqueeze(0) adds it back\n",
        "    # Pad the masks\n",
        "    instance[\"token_type_ids\"] = torch.zeros(max_len, dtype=torch.long, device=device) # these are only needed if we perform sentence pairs tasks\n",
        "    instance[\"attention_mask\"] = torch.cat((instance[\"attention_mask\"], zeros)) # these are needed such that the models knows where to perform attention\n",
        "    # remove batch dimensions\n",
        "    instance[\"input_ids\"] = instance[\"input_ids\"].squeeze()\n",
        "    #instance[\"token_type_ids\"] = instance[\"token_type_ids\"].squeeze()\n",
        "    instance[\"attention_mask\"] = instance[\"attention_mask\"].squeeze()\n",
        "  return data_set\n",
        "\n",
        "\n",
        "train_input = pad_truncate(train_input, max_len)\n",
        "val_input = pad_truncate(val_input, max_len)\n",
        "test_input = pad_truncate(test_input, max_len)\n",
        "print(train_input[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ4vFKu3rNi6"
      },
      "source": [
        "### Labels to Tensors\n",
        "\n",
        "❓ We have now represented our input data as tensors. Next, we need to map the labels to a single list of labels, respecting the order in our dataset. Write a function `get_labels(data_set)` that returns such a list for each datasplit. Replace the None values below with the return values of the calls to this function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRv9ohiHrgPX"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "train_labels = None\n",
        "dev_labels = None\n",
        "test_labels = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHOXx-Hjr0i8"
      },
      "source": [
        "### PyTorch Dataset: BERT Input\n",
        "\n",
        "The pre-trained BERT model expects not only the input_ids as input, but also the token_type_ids and the attention_masks that we have computed earlier.\n",
        "Each input instance will consist of one dictionary (as above). The DataLoader will combine the values of these dictionaries into tensors that contain all the instances that will be used in one batch. The `__get_item__` method only needs to return this data structure for one instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRL8eORWUWNe"
      },
      "outputs": [],
      "source": [
        "# Preparing a custom dataset for BERT\n",
        "class SubjectivityDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32, device=device)\n",
        "        item = {\"input_ids\" : self.encodings[idx][\"input_ids\"],\n",
        "                \"token_type_ids\" : self.encodings[idx][\"token_type_ids\"],\n",
        "                \"attention_mask\" : self.encodings[idx][\"attention_mask\"]}\n",
        "        return item, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SubjectivityDataset(train_input, train_labels)\n",
        "print(train_dataset[0])\n",
        "val_dataset = SubjectivityDataset(val_input, val_labels)\n",
        "test_dataset = SubjectivityDataset(test_input, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_27O_fnsV_o"
      },
      "source": [
        "### Adapting BERT\n",
        "The code below shows an example of using a pre-trained BERT model as an \"embedding layer\" in a neural network. The original model was pre-trained with a classification layer which we will ignore. Instead, we retrieve the embedding for the CLS token (the first embedding of the last hidden layer of BERT) and feed this into a linear layer predicting a z-score, and then pass this z through the sigmoid function to compute a probability score. When fine-tuning the model, the embeddings will also slightly change. However, if we would do this with a huge randomly initialized model, we would likely not achieve anything meaningful with just a small training dataset. Adapting the model to our task and domain, however, is highly effective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqXmCbaygntW"
      },
      "outputs": [],
      "source": [
        "class MyFinetunedModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    # max_len is the number of input_ids per token\n",
        "    super(MyFinetunedModel, self).__init__()\n",
        "    self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased') #, return_dict=True)\n",
        "    self.linear = torch.nn.Linear(768, 1) # map from one BERT token embedding to a single scalar\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    outputs = self.bert(**inputs, output_hidden_states=True) # obtain embeddings for inputs\n",
        "    last_hidden_state = outputs.hidden_states[-1] # hidden state values of last BERT layer\n",
        "    cls_embedding = last_hidden_state[:,0,:] # selects the 768-dimensional embedding output for CLS\n",
        "    # see: https://huggingface.co/docs/transformers/main_classes/output\n",
        "    # (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size))\n",
        "    logit = self.linear(cls_embedding) # Classifier layer mapping embedding of CLS token to logit\n",
        "    score = self.sigmoid(logit)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icRQm_TRv9Wg"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader):\n",
        "  # Compute accuracy of model on data provided by data_loader\n",
        "  correct = 0\n",
        "  num_instances = len(data_loader.dataset)\n",
        "  with torch.no_grad(): # This tells the model that we're not training\n",
        "                        # Will not remember gradients for this block\n",
        "    model.eval()\n",
        "    for X, y in iter(data_loader):\n",
        "      y_probs = model(X)\n",
        "      y_probs = y_probs.squeeze(1)\n",
        "      y_pred = torch.where(y_probs >= 0.5, 1, 0.)\n",
        "      correct += (y_pred == y).float().sum()\n",
        "\n",
        "  accuracy = 100 * correct / num_instances\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEh65nauTZjk"
      },
      "outputs": [],
      "source": [
        "# Always fun with the random seeds ...\n",
        "# We need to set them such that our results will be replicable.\n",
        "# (Hint: for an experiment later, you can change the random seed here and check what happens.\n",
        "# But for now, let's keep the answer to all questions of the universe, 42.)\n",
        "seed=42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "if cuda_available:\n",
        "  # This is needed on Colab as we are working in a distributed environment\n",
        "  # If you are working in a different GPU environment, you can probably omit this line if it results in errors.\n",
        "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
        "\n",
        "# Should we still have some source for non-determinism in our code, this will complain:\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Instantiate the model             #\n",
        "#####################################\n",
        "\n",
        "model = MyFinetunedModel()\n",
        "model = model.to(device)\n",
        "\n",
        "#####################################\n",
        "# Training / Fine-tuning the model  #\n",
        "#####################################\n",
        "\n",
        "num_epochs = 12\n",
        "batch_size = 64\n",
        "learning_rate = 1e-5\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate) # Always required for BERT!\n",
        "loss_fn = torch.nn.BCELoss() # This loss function does not include sigmoid.\n",
        "# Side note: if you exclude the sigmoid above, you have to use:\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
        "\n",
        "# ... and the rest of the code: is just as before!\n",
        "\n",
        "# Data Loaders\n",
        "data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "data_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "data_loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Training\n",
        "for n in range(num_epochs):\n",
        "  model.train()\n",
        "  it = iter(data_loader_train)  # Create the iterator from the training dataset\n",
        "  epoch_loss, steps = 0, 0      # To keep track of the current epoch's loss\n",
        "\n",
        "  for  X, y in it:              # Obtain a tensor X = batch of X-values, y accordingly\n",
        "    y_pred = model(X)           # Have our model with current weights make a prediction\n",
        "    y_pred = y_pred.squeeze(1)  # Removes the extra batch dimension (technical trick)\n",
        "    loss = loss_fn(y_pred, y)   # Have the loss function compute the loss value\n",
        "    optimizer.zero_grad()       # Reset the optimizer (otherwise it accumulates results - would be wrong here)\n",
        "    loss.backward()             # Compute the gradients (partial derivatives)\n",
        "    optimizer.step()            # Update the network's weights\n",
        "    epoch_loss += loss          # For tracking the epoch's loss\n",
        "    steps += 1\n",
        "\n",
        "  print(\"\\nEpoch:\", n+1, \"    Loss: {:0.4f}\".format(epoch_loss/steps))\n",
        "  # evaluate model at end of epoch\n",
        "  print(\"Training accuracy:   {:2.1f}\".format(evaluate(model, data_loader_train)))\n",
        "  print(\"Validation accuracy: {:2.1f}\".format(evaluate(model, data_loader_val)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f5VGi5U3hFQ"
      },
      "outputs": [],
      "source": [
        "# Test accuracy\n",
        "print(\"     Test accuracy: {:2.1f}\".format(evaluate(model, data_loader_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAXbBwhatelv"
      },
      "source": [
        "❓ When you look at the training logs above: does the model overfit? After which epoch could we have stopped the training?\n",
        "\n",
        "Congratulations, you have just fine-tuned your first BERT-based model. Make sure to work through the code above carefully such that you understand each line. Take notes. In future tasks, you will have to modify the code further."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}