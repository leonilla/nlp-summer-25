{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQsdK5_z4ng"
      },
      "source": [
        "## Introduction to Natural Language Processing\n",
        "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
        "Prof. Dr. Annemarie Friedrich<br/>\n",
        "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
        "Date: **SS 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MiU9xkBz6J0"
      },
      "source": [
        "# 4. Representing Text (Homework)\n",
        "\n",
        "In this homework, we will try out some methods to compute semantic relatedness between words.\n",
        "\n",
        "❓Read the first two pages of [this article](https://aclanthology.org/J06-1003.pdf) by Budanitsky and Hirst (2006). Answer the questions about the article in Vips.\n",
        "\n",
        "## WordSim Dataset\n",
        "\n",
        "[WordSim353](https://gabrilovich.com/resources/data/wordsim353/wordsim353.html) is a test collection for measuring word similarity or relatedness.\n",
        "Each instance consists of a pair of words that were judged by humans with regard to how similar or related they are. For example, \"midday\" and \"noon\" are rated to be more similar than \"noon\" and \"string.\"\n",
        "The task for the models is to produce similarity scores that _correlate_ well with the human ratings. We will measure that in terms of [__Pearson's r__](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).\n",
        "The full math can be found [here](https://mathworld.wolfram.com/CorrelationCoefficient.html).\n",
        "\n",
        "In this homework, we use the version by [Agirre et al., 2009](https://aclanthology.org/N09-1003/), who split the dataset into a part about relatedness and one about similarity. Your first task is to read in the dataset from a tab-separated CSV file.\n",
        "\n",
        "The dataset is in the folder `wordsim353_sim_rel`.\n",
        "\n",
        "Rename `wordsim_relatedness_goldstandard.txt`to `wordsim_relatedness_goldstandard.csv` (or `.tsv`) and upload it to Colab or place it in the same directory like the Jupyter notebook. The content of the file looks like this:\n",
        "\n",
        "```\n",
        "computer\tkeyboard\t7.62\n",
        "Jerusalem\tIsrael\t8.46\n",
        "planet\tgalaxy\t8.11\n",
        "canyon\tlandscape\t7.53\n",
        "OPEC\tcountry\t5.63\n",
        "day\tsummer\t3.94\n",
        "...\n",
        "```\n",
        "\n",
        "In fact, the file format is a [tab-separated format](https://en.wikipedia.org/wiki/Tab-separated_values). As this is just a variant of the comma-separated format, we can easily read the file in using [Python's csv package](https://docs.python.org/3/library/csv.html) by setting the `delimiter` to `\"\\t\"`.\n",
        "If you prefer, you can also use `pandas` to handle the file. If you have never read a csv file with Python using the `csv` reader package, I suggest you implement it this way, just so you know how to that in case you ever need it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce_8_vg_zusS",
        "outputId": "418595d3-58b9-4b4b-92b5-b969cbe4a025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Some imports\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "import csv # see: https://docs.python.org/3/library/csv.html\n",
        "import numpy as np\n",
        "import scipy\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMh3xIsOIF46"
      },
      "source": [
        "Read in the content of the file `\"wordsim_relatedness_goldstandard.csv` into a data structure of your choice. How many instances does the dataset contain? What is the average similarity of the rating by the humans, what is the minimum, what is the maximum? (You may use `numpy` to compute these statistics.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBy9bUL60GzD",
        "outputId": "e58c7816-6e46-4999-de33-c59c118bd8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of instances in set: 252.\n",
            "Max score: 8.81.\n",
            "Min score: 0.23.\n",
            "Mean of all scores: 5.294642857142857.\n"
          ]
        }
      ],
      "source": [
        "# A list of instances to read the /t-separated value strings into\n",
        "instances = []\n",
        "# A list of only the word pairs of the instances (useful later)\n",
        "wordPairs = []\n",
        "# A list of only the (float) scores of the instances (useful for min, max, mean)\n",
        "scores = []\n",
        "\n",
        "with open('/content/wordsim_relatedness_goldstandard.csv', newline='') as csvfile:\n",
        "    filereader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
        "\n",
        "\n",
        "    for line in filereader:\n",
        "        instances.append(line)\n",
        "        wordPairs.append((line[0], line[1]))\n",
        "        scores.append(float(line[2]))\n",
        "\n",
        "    print(f\"Number of instances in set: {len(instances)}.\")\n",
        "    print(f\"Max score: {np.max(scores)}.\")\n",
        "    print(f\"Min score: {np.min(scores)}.\")\n",
        "    print(f\"Mean of all scores: {np.mean(scores)}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5-Z-0E3H9xM"
      },
      "source": [
        "\n",
        "\n",
        "## WordNet Similarity\n",
        "\n",
        "First, we will use WordNet to compute semantic relatedness between the words.\n",
        "Re-read the Wiki page on WordNet. Recall that WordNet is a huge graph.\n",
        "\n",
        "❓ Read sections 2.3 and section 2.5.3 of the Budanitsky and Hirst (2006) paper. Make sure you understand how the Leacock-Chodorow algorithm works. It might help you to sketch an example on a piece of paper. You can also use additional sources that you find about the algorithm.\n",
        "\n",
        "The [`nltk`](https://www.nltk.org/index.html) toolkit provides a method to compute the Leacock Chodorow (LCH) similarity. Go to [this website](https://www.nltk.org/howto/wordnet.html) and figure out how it works. You can assume that all the words in the wordsim353 dataset the we are using are nouns.\n",
        "\n",
        "__Computing LCH for wordsim353__: Wait a minute, LCH works for pairs of synsets, and in wordsim353 we are just given words, completely out of context! While this may admittedly also cause some problems for human annotators, for our purpose, we will simply retrieve _all_ the synsets associated with a noun and then compute the LCH similarities between all pairs of synsets for the two words. We define the LCH similarity between the two words as the maximum similarity score we found.\n",
        "\n",
        "❓Compute the maximum LCH similarity for each pair of words in the wordsim353 dataset.\n",
        "\n",
        "Hint: If you use a list for the computed similarities, it should start like this: `[2.2512917986064953, 1.6916760106710724, 1.55814461804655,...`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k82Lwef44XXw",
        "outputId": "b97953fa-94aa-423f-fe35-892063a9c1ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed Similarity for computer and keyboard = 2.2512917986064953.\n",
            "Computed Similarity for Jerusalem and Israel = 1.6916760106710724.\n",
            "Computed Similarity for planet and galaxy = 1.55814461804655.\n",
            "Computed Similarity for canyon and landscape = 1.1526795099383855.\n",
            "Computed Similarity for OPEC and country = 1.6916760106710724.\n",
            "Computed Similarity for day and summer = 2.2512917986064953.\n",
            "Computed Similarity for day and dawn = 2.538973871058276.\n",
            "Computed Similarity for country and citizen = 1.4403615823901665.\n",
            "Computed Similarity for planet and people = 1.4403615823901665.\n",
            "Computed Similarity for environment and ecology = 2.9444389791664407.\n",
            "Computed Similarity for Maradona and football = 0.\n",
            "Computed Similarity for OPEC and oil = 0.9985288301111273.\n",
            "Computed Similarity for money and bank = 1.6916760106710724.\n",
            "Computed Similarity for computer and software = 1.072636802264849.\n",
            "Computed Similarity for law and lawyer = 1.2396908869280152.\n",
            "Computed Similarity for weather and forecast = 0.9985288301111273.\n",
            "Computed Similarity for network and hardware = 2.2512917986064953.\n",
            "Computed Similarity for nature and environment = 1.845826690498331.\n",
            "Computed Similarity for FBI and investigation = 0.9295359586241757.\n",
            "Computed Similarity for money and wealth = 2.9444389791664407.\n",
            "Computed Similarity for psychology and Freud = 0.6418538861723948.\n",
            "Computed Similarity for news and report = 2.9444389791664407.\n",
            "Computed Similarity for war and troops = 1.3350010667323402.\n",
            "Computed Similarity for physics and proton = 0.9295359586241757.\n",
            "Computed Similarity for bank and money = 1.6916760106710724.\n",
            "Computed Similarity for stock and market = 1.6916760106710724.\n",
            "Computed Similarity for planet and constellation = 2.2512917986064953.\n",
            "Computed Similarity for credit and card = 1.845826690498331.\n",
            "Computed Similarity for hotel and reservation = 1.2396908869280152.\n",
            "Computed Similarity for closet and clothes = 1.55814461804655.\n",
            "Computed Similarity for soap and opera = 0.9985288301111273.\n",
            "Computed Similarity for planet and astronomer = 1.845826690498331.\n",
            "Computed Similarity for planet and space = 1.6916760106710724.\n",
            "Computed Similarity for movie and theater = 1.6916760106710724.\n",
            "Computed Similarity for treatment and recovery = 2.2512917986064953.\n",
            "Computed Similarity for baby and mother = 2.0281482472922856.\n",
            "Computed Similarity for money and deposit = 2.538973871058276.\n",
            "Computed Similarity for television and film = 2.0281482472922856.\n",
            "Computed Similarity for psychology and mind = 1.6916760106710724.\n",
            "Computed Similarity for game and team = 1.3350010667323402.\n",
            "Computed Similarity for admission and ticket = 1.6916760106710724.\n",
            "Computed Similarity for Jerusalem and Palestinian = 0.8649974374866046.\n",
            "Computed Similarity for Arafat and terror = 1.845826690498331.\n",
            "Computed Similarity for boxing and round = 1.6916760106710724.\n",
            "Computed Similarity for computer and internet = 1.55814461804655.\n",
            "Computed Similarity for money and property = 2.538973871058276.\n",
            "Computed Similarity for tennis and racket = 1.4403615823901665.\n",
            "Computed Similarity for telephone and communication = 1.2396908869280152.\n",
            "Computed Similarity for currency and market = 1.3350010667323402.\n",
            "Computed Similarity for psychology and cognition = 1.845826690498331.\n",
            "Computed Similarity for seafood and sea = 1.55814461804655.\n",
            "Computed Similarity for book and paper = 2.538973871058276.\n",
            "Computed Similarity for book and library = 2.538973871058276.\n",
            "Computed Similarity for psychology and depression = 1.1526795099383855.\n",
            "Computed Similarity for fighting and defeating = 0.\n",
            "Computed Similarity for movie and star = 1.55814461804655.\n",
            "Computed Similarity for hundred and percent = 1.072636802264849.\n",
            "Computed Similarity for dollar and profit = 1.3350010667323402.\n",
            "Computed Similarity for money and possession = 2.2512917986064953.\n",
            "Computed Similarity for cup and drink = 2.0281482472922856.\n",
            "Computed Similarity for psychology and health = 1.1526795099383855.\n",
            "Computed Similarity for summer and drought = 2.2512917986064953.\n",
            "Computed Similarity for investor and earning = 0.\n",
            "Computed Similarity for company and stock = 1.845826690498331.\n",
            "Computed Similarity for stroke and hospital = 1.2396908869280152.\n",
            "Computed Similarity for liability and insurance = 1.6916760106710724.\n",
            "Computed Similarity for game and victory = 1.6916760106710724.\n",
            "Computed Similarity for psychology and anxiety = 1.072636802264849.\n",
            "Computed Similarity for game and defeat = 1.6916760106710724.\n",
            "Computed Similarity for FBI and fingerprint = 1.072636802264849.\n",
            "Computed Similarity for money and withdrawal = 1.2396908869280152.\n",
            "Computed Similarity for psychology and fear = 1.072636802264849.\n",
            "Computed Similarity for drug and abuse = 1.3350010667323402.\n",
            "Computed Similarity for concert and virtuoso = 1.072636802264849.\n",
            "Computed Similarity for computer and laboratory = 1.4403615823901665.\n",
            "Computed Similarity for love and sex = 2.9444389791664407.\n",
            "Computed Similarity for problem and challenge = 1.845826690498331.\n",
            "Computed Similarity for movie and critic = 1.2396908869280152.\n",
            "Computed Similarity for Arafat and peace = 1.072636802264849.\n",
            "Computed Similarity for bed and closet = 2.2512917986064953.\n",
            "Computed Similarity for lawyer and evidence = 1.2396908869280152.\n",
            "Computed Similarity for fertility and egg = 1.1526795099383855.\n",
            "Computed Similarity for precedent and law = 2.9444389791664407.\n",
            "Computed Similarity for minister and party = 1.845826690498331.\n",
            "Computed Similarity for psychology and clinic = 0.9985288301111273.\n",
            "Computed Similarity for cup and coffee = 1.845826690498331.\n",
            "Computed Similarity for water and seepage = 1.1526795099383855.\n",
            "Computed Similarity for government and crisis = 1.6916760106710724.\n",
            "Computed Similarity for space and world = 2.0281482472922856.\n",
            "Computed Similarity for dividend and calculation = 1.2396908869280152.\n",
            "Computed Similarity for victim and emergency = 1.3350010667323402.\n",
            "Computed Similarity for luxury and car = 0.9295359586241757.\n",
            "Computed Similarity for tool and implement = 2.9444389791664407.\n",
            "Computed Similarity for competition and price = 1.55814461804655.\n",
            "Computed Similarity for psychology and doctor = 1.072636802264849.\n",
            "Computed Similarity for gender and equality = 1.55814461804655.\n",
            "Computed Similarity for listing and category = 1.4403615823901665.\n",
            "Computed Similarity for video and archive = 1.4403615823901665.\n",
            "Computed Similarity for oil and stock = 1.4403615823901665.\n",
            "Computed Similarity for governor and office = 1.3350010667323402.\n",
            "Computed Similarity for discovery and space = 1.6916760106710724.\n",
            "Computed Similarity for record and number = 2.9444389791664407.\n",
            "Computed Similarity for brother and monk = 2.9444389791664407.\n",
            "Computed Similarity for production and crew = 1.55814461804655.\n",
            "Computed Similarity for nature and man = 2.2512917986064953.\n",
            "Computed Similarity for family and planning = 1.3350010667323402.\n",
            "Computed Similarity for disaster and area = 1.4403615823901665.\n",
            "Computed Similarity for food and preparation = 1.6916760106710724.\n",
            "Computed Similarity for preservation and world = 1.6916760106710724.\n",
            "Computed Similarity for movie and popcorn = 0.9985288301111273.\n",
            "Computed Similarity for lover and quarrel = 1.072636802264849.\n",
            "Computed Similarity for game and series = 2.538973871058276.\n",
            "Computed Similarity for dollar and loss = 1.4403615823901665.\n",
            "Computed Similarity for weapon and secret = 1.2396908869280152.\n",
            "Computed Similarity for shower and flood = 1.55814461804655.\n",
            "Computed Similarity for registration and arrangement = 2.0281482472922856.\n",
            "Computed Similarity for arrival and hotel = 1.3350010667323402.\n",
            "Computed Similarity for announcement and warning = 1.845826690498331.\n",
            "Computed Similarity for game and round = 2.538973871058276.\n",
            "Computed Similarity for baseball and season = 0.9295359586241757.\n",
            "Computed Similarity for drink and mouth = 1.6916760106710724.\n",
            "Computed Similarity for life and lesson = 1.55814461804655.\n",
            "Computed Similarity for grocery and money = 0.9985288301111273.\n",
            "Computed Similarity for energy and crisis = 1.6916760106710724.\n",
            "Computed Similarity for reason and criterion = 1.55814461804655.\n",
            "Computed Similarity for equipment and maker = 1.4403615823901665.\n",
            "Computed Similarity for cup and liquid = 1.845826690498331.\n",
            "Computed Similarity for deployment and withdrawal = 2.0281482472922856.\n",
            "Computed Similarity for tiger and zoo = 1.55814461804655.\n",
            "Computed Similarity for journey and car = 0.7472144018302211.\n",
            "Computed Similarity for money and laundering = 1.072636802264849.\n",
            "Computed Similarity for summer and nature = 1.4403615823901665.\n",
            "Computed Similarity for decoration and valor = 1.1526795099383855.\n",
            "Computed Similarity for Mars and scientist = 1.3350010667323402.\n",
            "Computed Similarity for alcohol and chemistry = 2.0281482472922856.\n",
            "Computed Similarity for disability and death = 1.55814461804655.\n",
            "Computed Similarity for change and attitude = 1.845826690498331.\n",
            "Computed Similarity for arrangement and accommodation = 1.6916760106710724.\n",
            "Computed Similarity for territory and surface = 1.845826690498331.\n",
            "Computed Similarity for size and prominence = 1.845826690498331.\n",
            "Computed Similarity for exhibit and memorabilia = 1.4403615823901665.\n",
            "Computed Similarity for credit and information = 2.2512917986064953.\n",
            "Computed Similarity for territory and kilometer = 1.072636802264849.\n",
            "Computed Similarity for death and row = 1.55814461804655.\n",
            "Computed Similarity for doctor and liability = 1.1526795099383855.\n",
            "Computed Similarity for impartiality and interest = 1.4403615823901665.\n",
            "Computed Similarity for energy and laboratory = 1.3350010667323402.\n",
            "Computed Similarity for secretary and senate = 0.9985288301111273.\n",
            "Computed Similarity for death and inmate = 1.2396908869280152.\n",
            "Computed Similarity for monk and oracle = 1.55814461804655.\n",
            "Computed Similarity for cup and food = 1.845826690498331.\n",
            "Computed Similarity for journal and association = 1.6916760106710724.\n",
            "Computed Similarity for street and children = 1.3350010667323402.\n",
            "Computed Similarity for car and flight = 1.55814461804655.\n",
            "Computed Similarity for space and chemistry = 1.845826690498331.\n",
            "Computed Similarity for situation and conclusion = 1.6916760106710724.\n",
            "Computed Similarity for word and similarity = 1.4403615823901665.\n",
            "Computed Similarity for peace and plan = 1.3350010667323402.\n",
            "Computed Similarity for consumer and energy = 1.3350010667323402.\n",
            "Computed Similarity for ministry and culture = 1.55814461804655.\n",
            "Computed Similarity for smart and student = 0.9985288301111273.\n",
            "Computed Similarity for investigation and effort = 2.2512917986064953.\n",
            "Computed Similarity for image and surface = 2.0281482472922856.\n",
            "Computed Similarity for life and term = 2.538973871058276.\n",
            "Computed Similarity for start and match = 1.6916760106710724.\n",
            "Computed Similarity for computer and news = 1.2396908869280152.\n",
            "Computed Similarity for board and recommendation = 1.1526795099383855.\n",
            "Computed Similarity for lad and brother = 2.0281482472922856.\n",
            "Computed Similarity for observation and architecture = 1.845826690498331.\n",
            "Computed Similarity for coast and hill = 2.0281482472922856.\n",
            "Computed Similarity for deployment and departure = 2.0281482472922856.\n",
            "Computed Similarity for benchmark and index = 2.2512917986064953.\n",
            "Computed Similarity for attempt and peace = 1.3350010667323402.\n",
            "Computed Similarity for consumer and confidence = 1.1526795099383855.\n",
            "Computed Similarity for start and year = 1.845826690498331.\n",
            "Computed Similarity for focus and life = 1.6916760106710724.\n",
            "Computed Similarity for development and issue = 2.2512917986064953.\n",
            "Computed Similarity for theater and history = 1.6916760106710724.\n",
            "Computed Similarity for situation and isolation = 2.2512917986064953.\n",
            "Computed Similarity for profit and warning = 1.2396908869280152.\n",
            "Computed Similarity for media and trading = 1.6916760106710724.\n",
            "Computed Similarity for chance and credibility = 1.845826690498331.\n",
            "Computed Similarity for precedent and information = 2.538973871058276.\n",
            "Computed Similarity for architecture and century = 1.3350010667323402.\n",
            "Computed Similarity for population and development = 1.6916760106710724.\n",
            "Computed Similarity for stock and live = 0.\n",
            "Computed Similarity for peace and atmosphere = 2.0281482472922856.\n",
            "Computed Similarity for morality and marriage = 1.845826690498331.\n",
            "Computed Similarity for minority and peace = 1.845826690498331.\n",
            "Computed Similarity for atmosphere and landscape = 1.4403615823901665.\n",
            "Computed Similarity for report and gain = 1.55814461804655.\n",
            "Computed Similarity for music and project = 2.2512917986064953.\n",
            "Computed Similarity for seven and series = 1.4403615823901665.\n",
            "Computed Similarity for experience and music = 1.845826690498331.\n",
            "Computed Similarity for school and center = 2.538973871058276.\n",
            "Computed Similarity for five and month = 1.55814461804655.\n",
            "Computed Similarity for announcement and production = 1.6916760106710724.\n",
            "Computed Similarity for morality and importance = 2.0281482472922856.\n",
            "Computed Similarity for money and operation = 1.4403615823901665.\n",
            "Computed Similarity for delay and news = 1.55814461804655.\n",
            "Computed Similarity for governor and interview = 0.9985288301111273.\n",
            "Computed Similarity for practice and institution = 2.538973871058276.\n",
            "Computed Similarity for century and nation = 1.55814461804655.\n",
            "Computed Similarity for coast and forest = 1.845826690498331.\n",
            "Computed Similarity for shore and woodland = 2.0281482472922856.\n",
            "Computed Similarity for drink and car = 1.2396908869280152.\n",
            "Computed Similarity for president and medal = 1.072636802264849.\n",
            "Computed Similarity for prejudice and recognition = 1.845826690498331.\n",
            "Computed Similarity for viewer and serial = 1.2396908869280152.\n",
            "Computed Similarity for peace and insurance = 2.2512917986064953.\n",
            "Computed Similarity for Mars and water = 1.4403615823901665.\n",
            "Computed Similarity for media and gain = 1.6916760106710724.\n",
            "Computed Similarity for precedent and cognition = 2.2512917986064953.\n",
            "Computed Similarity for announcement and effort = 1.3350010667323402.\n",
            "Computed Similarity for line and insurance = 2.0281482472922856.\n",
            "Computed Similarity for crane and implement = 2.0281482472922856.\n",
            "Computed Similarity for drink and mother = 1.845826690498331.\n",
            "Computed Similarity for opera and industry = 1.2396908869280152.\n",
            "Computed Similarity for volunteer and motto = 1.1526795099383855.\n",
            "Computed Similarity for listing and proximity = 1.072636802264849.\n",
            "Computed Similarity for precedent and collection = 2.538973871058276.\n",
            "Computed Similarity for cup and article = 2.0281482472922856.\n",
            "Computed Similarity for sign and recess = 2.0281482472922856.\n",
            "Computed Similarity for problem and airport = 0.9985288301111273.\n",
            "Computed Similarity for reason and hypertension = 1.4403615823901665.\n",
            "Computed Similarity for direction and combination = 1.6916760106710724.\n",
            "Computed Similarity for Wednesday and news = 1.1526795099383855.\n",
            "Computed Similarity for glass and magician = 1.55814461804655.\n",
            "Computed Similarity for cemetery and woodland = 1.4403615823901665.\n",
            "Computed Similarity for possibility and girl = 1.2396908869280152.\n",
            "Computed Similarity for cup and substance = 1.6916760106710724.\n",
            "Computed Similarity for forest and graveyard = 1.4403615823901665.\n",
            "Computed Similarity for stock and egg = 1.6916760106710724.\n",
            "Computed Similarity for month and hotel = 1.1526795099383855.\n",
            "Computed Similarity for energy and secretary = 1.2396908869280152.\n",
            "Computed Similarity for precedent and group = 2.2512917986064953.\n",
            "Computed Similarity for production and hike = 1.55814461804655.\n",
            "Computed Similarity for stock and phone = 1.6916760106710724.\n",
            "Computed Similarity for holy and sex = 1.3350010667323402.\n",
            "Computed Similarity for stock and CD = 1.845826690498331.\n",
            "Computed Similarity for drink and ear = 1.55814461804655.\n",
            "Computed Similarity for delay and racism = 1.6916760106710724.\n",
            "Computed Similarity for stock and life = 1.845826690498331.\n",
            "Computed Similarity for stock and jaguar = 1.845826690498331.\n",
            "Computed Similarity for monk and slave = 2.0281482472922856.\n",
            "Computed Similarity for lad and wizard = 2.0281482472922856.\n",
            "Computed Similarity for sugar and approach = 1.2396908869280152.\n",
            "Computed Similarity for rooster and voyage = 0.4595323293784402.\n",
            "Computed Similarity for noon and string = 1.1526795099383855.\n",
            "Computed Similarity for chord and smile = 1.2396908869280152.\n",
            "Computed Similarity for professor and cucumber = 1.072636802264849.\n",
            "Computed Similarity for king and cabbage = 1.3350010667323402.\n"
          ]
        }
      ],
      "source": [
        "# Compute maximum LCH similarity between the two sets of synsets that belong to the two lemmas\n",
        "# Do this for every pair of words in the wordsim353 dataset and collect the results\n",
        "# Your code here\n",
        "\n",
        "def lchmax(word1, word2):\n",
        "  # Get all synsets for a word, assume all words are nouns.\n",
        "  sets1 = wn.synsets(word1, pos=wn.NOUN)\n",
        "  sets2 = wn.synsets(word2, pos=wn.NOUN)\n",
        "\n",
        "  lchmax = 0\n",
        "\n",
        "  # Nested loop to iterate over all pairs, calculating lch for each pair,\n",
        "  # storing max in acc var 'lchmax'.\n",
        "  for ss1 in sets1:\n",
        "    for ss2 in sets2:\n",
        "      lch = ss1.lch_similarity(ss2)\n",
        "      if( lch > lchmax):\n",
        "        lchmax = lch\n",
        "\n",
        "  return lchmax\n",
        "\n",
        "\n",
        "# Compute similarity for each pair into a list of similarities\n",
        "compsims = []\n",
        "for wordPair in wordPairs:\n",
        "  sim = lchmax(wordPair[0], wordPair[1])\n",
        "  print(f\"Computed Similarity for {wordPair[0]} and {wordPair[1]} = {sim}.\")\n",
        "  compsims.append(sim)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DggTsHn-6jIx"
      },
      "source": [
        "__Evaluation:__ Next, we evaluate the system ratings by computing the correlation with the human ratings.\n",
        "\n",
        "More on computing various correlation coefficients in Python: https://realpython.com/numpy-scipy-pandas-correlation-python/#example-numpy-correlation-calculation\n",
        "\n",
        "A function for computing Pearson's r is given below. Use it to compute Pearson's correlation for the human ratings of the wordsim353 dataset and the system ratings provided by the LCH metric that you have implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m02_erD65tpt",
        "outputId": "8425f576-99c9-41d1-89ae-f5811411e996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed Pearson correlation coefficient is 0.007949148935555112.\n",
            "Computed p value is 0.9000775038545066.\n"
          ]
        }
      ],
      "source": [
        "# function is given\n",
        "def compute_correlation(human_ratings, system_ratings):\n",
        "  \"\"\" Input: two lists (of equal length) with numeric values.\n",
        "  Computes Pearson's correlation coefficient.\n",
        "  \"\"\"\n",
        "  assert len(human_ratings), len(system_ratings)\n",
        "  return scipy.stats.pearsonr(human_ratings, system_ratings)\n",
        "\n",
        "# Use the function above to compute the correlation of the human and the LCH ratings\n",
        "corr_coeff, p_val = compute_correlation(scores, compsims)\n",
        "print(f\"Computed Pearson correlation coefficient is {corr_coeff}.\")\n",
        "print(f\"Computed p value is {p_val}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqbbsP5k77Tk"
      },
      "source": [
        "## Distributional Similarity\n",
        "\n",
        "Next, we will use a distributional method to compute relatedness values.\n",
        "\n",
        "In order to compute how often a word co-occurs with another word, we need a plaintext corpus. In this homework, we will use the Brown corpus as provided by `nltk`, check out the examples [here](https://www.nltk.org/howto/corpus.html#plaintext-corpora).\n",
        "\n",
        "❓Import the `brown` corpus using nltk. For each pair of words in the wordsim353 dataset, compute the _Pointwise Mutual Information_ (see wiki!) as\n",
        "\n",
        "$ \\displaystyle PMI(w1, w2) = log_2 \\big( \\frac{p(w1, w2)}{p(w1)*p(w2)}   \\big)$\n",
        "\n",
        "* $p(w1, w2)$ denotes the probability that $w1$ and $w2$ occur together in a sentence.\n",
        "* $p(w1)$ is the probability that $w1$ occurs in a sentence; $w2$ accordingly.\n",
        "\n",
        "Use the PMI scores as the similarity ratings between two words. Compute the Pearson correlation vs. the human ratings using this method. Compare the results to that of the LCH method above.\n",
        "Hint: Write a function `compute_pmi` to structure your code in a good way. Again, use the `compute_corelation` function to compute the correlation between the human and the PMI-based system ratings.\n",
        "\n",
        "By the way, a naive iterative implementation runs about 10 minutes (hint: print some progress statements to see what is going on). My optimized solution using dictionaries (`defaultdict`) and `set` operations runs in just a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zokwdqzt79aQ",
        "outputId": "4208ba4b-a00e-4a1c-a183-79927f96490d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i0WuRX208qda",
        "outputId": "0ae6328d-3bb0-44e3-ef64-9c81f6d7614a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseball: 57.\n",
            "season: 105.\n",
            "Co-occur: 4.\n",
            "PMI: 5.260118752297021.\n",
            "\n",
            "computer: 13.\n",
            "keyboard: 4.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "Jerusalem: 7.\n",
            "Israel: 15.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "planet: 21.\n",
            "galaxy: 3.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "canyon: 12.\n",
            "landscape: 20.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "OPEC: 0.\n",
            "country: 324.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "day: 687.\n",
            "summer: 134.\n",
            "Co-occur: 7.\n",
            "PMI: 2.1243537269096175.\n",
            "\n",
            "day: 687.\n",
            "dawn: 28.\n",
            "Co-occur: 2.\n",
            "PMI: 2.5757330732521813.\n",
            "\n",
            "country: 324.\n",
            "citizen: 30.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "planet: 21.\n",
            "people: 847.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "environment: 43.\n",
            "ecology: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "Maradona: 0.\n",
            "football: 36.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "OPEC: 0.\n",
            "oil: 95.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "money: 265.\n",
            "bank: 83.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "computer: 13.\n",
            "software: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "law: 299.\n",
            "lawyer: 43.\n",
            "Co-occur: 4.\n",
            "PMI: 4.156987855227683.\n",
            "\n",
            "weather: 69.\n",
            "forecast: 10.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "network: 30.\n",
            "hardware: 11.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "nature: 191.\n",
            "environment: 43.\n",
            "Co-occur: 1.\n",
            "PMI: 2.8035607013900394.\n",
            "\n",
            "FBI: 8.\n",
            "investigation: 51.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "money: 265.\n",
            "wealth: 22.\n",
            "Co-occur: 1.\n",
            "PMI: 3.297974116040027.\n",
            "\n",
            "psychology: 14.\n",
            "Freud: 9.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "news: 102.\n",
            "report: 174.\n",
            "Co-occur: 3.\n",
            "PMI: 3.276847947028818.\n",
            "\n",
            "war: 464.\n",
            "troops: 53.\n",
            "Co-occur: 5.\n",
            "PMI: 3.543280929324477.\n",
            "\n",
            "physics: 22.\n",
            "proton: 3.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "bank: 83.\n",
            "money: 265.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "stock: 147.\n",
            "market: 155.\n",
            "Co-occur: 16.\n",
            "PMI: 5.331457534017284.\n",
            "\n",
            "planet: 21.\n",
            "constellation: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "credit: 64.\n",
            "card: 26.\n",
            "Co-occur: 1.\n",
            "PMI: 5.106814565986793.\n",
            "\n",
            "hotel: 126.\n",
            "reservation: 8.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "closet: 16.\n",
            "clothes: 89.\n",
            "Co-occur: 3.\n",
            "PMI: 6.9164833538826445.\n",
            "\n",
            "soap: 22.\n",
            "opera: 47.\n",
            "Co-occur: 1.\n",
            "PMI: 5.793233813812951.\n",
            "\n",
            "planet: 21.\n",
            "astronomer: 1.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "planet: 21.\n",
            "space: 184.\n",
            "Co-occur: 1.\n",
            "PMI: 3.8913749052921127.\n",
            "\n",
            "movie: 29.\n",
            "theater: 52.\n",
            "Co-occur: 3.\n",
            "PMI: 6.833796071580378.\n",
            "\n",
            "treatment: 127.\n",
            "recovery: 29.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "baby: 62.\n",
            "mother: 216.\n",
            "Co-occur: 1.\n",
            "PMI: 2.098170471577542.\n",
            "\n",
            "money: 265.\n",
            "deposit: 9.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "television: 50.\n",
            "film: 96.\n",
            "Co-occur: 1.\n",
            "PMI: 3.5784355936320047.\n",
            "\n",
            "psychology: 14.\n",
            "mind: 325.\n",
            "Co-occur: 1.\n",
            "PMI: 3.655603454154465.\n",
            "\n",
            "game: 123.\n",
            "team: 83.\n",
            "Co-occur: 2.\n",
            "PMI: 3.489700347441721.\n",
            "\n",
            "admission: 33.\n",
            "ticket: 16.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "Jerusalem: 7.\n",
            "Palestinian: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "Arafat: 0.\n",
            "terror: 25.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "boxing: 0.\n",
            "round: 75.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "computer: 13.\n",
            "internet: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "money: 265.\n",
            "property: 156.\n",
            "Co-occur: 5.\n",
            "PMI: 2.7939316107024386.\n",
            "\n",
            "tennis: 15.\n",
            "racket: 5.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "telephone: 76.\n",
            "communication: 67.\n",
            "Co-occur: 1.\n",
            "PMI: 3.493237580226528.\n",
            "\n",
            "currency: 12.\n",
            "market: 155.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "psychology: 14.\n",
            "cognition: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "seafood: 3.\n",
            "sea: 95.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "book: 197.\n",
            "paper: 157.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "book: 197.\n",
            "library: 62.\n",
            "Co-occur: 5.\n",
            "PMI: 4.552934249171997.\n",
            "\n",
            "psychology: 14.\n",
            "depression: 24.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "fighting: 72.\n",
            "defeating: 3.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "movie: 29.\n",
            "star: 25.\n",
            "Co-occur: 2.\n",
            "PMI: 7.305417099225589.\n",
            "\n",
            "hundred: 171.\n",
            "percent: 53.\n",
            "Co-occur: 1.\n",
            "PMI: 2.661481314678789.\n",
            "\n",
            "dollar: 46.\n",
            "profit: 28.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "money: 265.\n",
            "possession: 21.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "cup: 45.\n",
            "drink: 82.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "psychology: 14.\n",
            "health: 105.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "summer: 134.\n",
            "drought: 5.\n",
            "Co-occur: 1.\n",
            "PMI: 6.419236998782751.\n",
            "\n",
            "investor: 2.\n",
            "earning: 9.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "company: 290.\n",
            "stock: 147.\n",
            "Co-occur: 6.\n",
            "PMI: 3.012635349997743.\n",
            "\n",
            "stroke: 19.\n",
            "hospital: 110.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "liability: 7.\n",
            "insurance: 46.\n",
            "Co-occur: 3.\n",
            "PMI: 9.061299906734424.\n",
            "\n",
            "game: 123.\n",
            "victory: 61.\n",
            "Co-occur: 3.\n",
            "PMI: 4.5189649419469164.\n",
            "\n",
            "psychology: 14.\n",
            "anxiety: 42.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "game: 123.\n",
            "defeat: 31.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "FBI: 8.\n",
            "fingerprint: 6.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "money: 265.\n",
            "withdrawal: 6.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "psychology: 14.\n",
            "fear: 127.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "drug: 24.\n",
            "abuse: 18.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "concert: 39.\n",
            "virtuoso: 3.\n",
            "Co-occur: 1.\n",
            "PMI: 8.936889564544481.\n",
            "\n",
            "computer: 13.\n",
            "laboratory: 40.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "love: 231.\n",
            "sex: 84.\n",
            "Co-occur: 3.\n",
            "PMI: 3.1481503206542247.\n",
            "\n",
            "problem: 313.\n",
            "challenge: 36.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "movie: 29.\n",
            "critic: 25.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "Arafat: 0.\n",
            "peace: 198.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "bed: 127.\n",
            "closet: 16.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "lawyer: 43.\n",
            "evidence: 204.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "fertility: 10.\n",
            "egg: 12.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "precedent: 9.\n",
            "law: 299.\n",
            "Co-occur: 1.\n",
            "PMI: 4.413327608487469.\n",
            "\n",
            "minister: 61.\n",
            "party: 216.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "psychology: 14.\n",
            "clinic: 3.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "cup: 45.\n",
            "coffee: 78.\n",
            "Co-occur: 17.\n",
            "PMI: 8.117461810186303.\n",
            "\n",
            "water: 445.\n",
            "seepage: 2.\n",
            "Co-occur: 1.\n",
            "PMI: 6.009592758274126.\n",
            "\n",
            "government: 418.\n",
            "crisis: 82.\n",
            "Co-occur: 2.\n",
            "PMI: 1.7423431474289197.\n",
            "\n",
            "space: 184.\n",
            "world: 787.\n",
            "Co-occur: 6.\n",
            "PMI: 1.248435003284543.\n",
            "\n",
            "dividend: 6.\n",
            "calculation: 12.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "victim: 27.\n",
            "emergency: 39.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "luxury: 21.\n",
            "car: 274.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "tool: 40.\n",
            "implement: 4.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "competition: 63.\n",
            "price: 108.\n",
            "Co-occur: 5.\n",
            "PMI: 5.397014953351864.\n",
            "\n",
            "psychology: 14.\n",
            "doctor: 100.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "gender: 2.\n",
            "equality: 12.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "listing: 7.\n",
            "category: 23.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "video: 2.\n",
            "archive: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "oil: 95.\n",
            "stock: 147.\n",
            "Co-occur: 1.\n",
            "PMI: 2.0377263309605733.\n",
            "\n",
            "governor: 83.\n",
            "office: 255.\n",
            "Co-occur: 2.\n",
            "PMI: 2.4378614159221033.\n",
            "\n",
            "discovery: 45.\n",
            "space: 184.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "record: 137.\n",
            "number: 472.\n",
            "Co-occur: 1.\n",
            "PMI: -0.173420848194482.\n",
            "\n",
            "brother: 73.\n",
            "monk: 16.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "production: 148.\n",
            "crew: 36.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "nature: 191.\n",
            "man: 1207.\n",
            "Co-occur: 18.\n",
            "PMI: 2.162540496779428.\n",
            "\n",
            "family: 331.\n",
            "planning: 129.\n",
            "Co-occur: 7.\n",
            "PMI: 3.2326945439550188.\n",
            "\n",
            "disaster: 26.\n",
            "area: 324.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "food: 147.\n",
            "preparation: 54.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "preservation: 17.\n",
            "world: 787.\n",
            "Co-occur: 1.\n",
            "PMI: 2.09957161737006.\n",
            "\n",
            "movie: 29.\n",
            "popcorn: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "lover: 19.\n",
            "quarrel: 20.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "game: 123.\n",
            "series: 130.\n",
            "Co-occur: 3.\n",
            "PMI: 3.427334466481348.\n",
            "\n",
            "dollar: 46.\n",
            "loss: 86.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "weapon: 42.\n",
            "secret: 78.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "shower: 15.\n",
            "flood: 19.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "registration: 23.\n",
            "arrangement: 34.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "arrival: 23.\n",
            "hotel: 126.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "announcement: 24.\n",
            "warning: 44.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "game: 123.\n",
            "round: 75.\n",
            "Co-occur: 1.\n",
            "PMI: 2.635921088292765.\n",
            "\n",
            "baseball: 57.\n",
            "season: 105.\n",
            "Co-occur: 4.\n",
            "PMI: 5.260118752297021.\n",
            "\n",
            "drink: 82.\n",
            "mouth: 103.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "life: 715.\n",
            "lesson: 29.\n",
            "Co-occur: 4.\n",
            "PMI: 3.467473857334562.\n",
            "\n",
            "grocery: 9.\n",
            "money: 265.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "energy: 100.\n",
            "crisis: 82.\n",
            "Co-occur: 1.\n",
            "PMI: 2.8058460897350774.\n",
            "\n",
            "reason: 241.\n",
            "criterion: 11.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "equipment: 167.\n",
            "maker: 12.\n",
            "Co-occur: 3.\n",
            "PMI: 6.423549991653833.\n",
            "\n",
            "cup: 45.\n",
            "liquid: 48.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "deployment: 1.\n",
            "withdrawal: 6.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "tiger: 7.\n",
            "zoo: 9.\n",
            "Co-occur: 1.\n",
            "PMI: 9.82997436062797.\n",
            "\n",
            "journey: 28.\n",
            "car: 274.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "money: 265.\n",
            "laundering: 7.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "summer: 134.\n",
            "nature: 191.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "decoration: 8.\n",
            "valor: 1.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "Mars: 21.\n",
            "scientist: 17.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "alcohol: 13.\n",
            "chemistry: 16.\n",
            "Co-occur: 1.\n",
            "PMI: 8.106814565986793.\n",
            "\n",
            "disability: 5.\n",
            "death: 277.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "change: 240.\n",
            "attitude: 107.\n",
            "Co-occur: 3.\n",
            "PMI: 2.743859202839377.\n",
            "\n",
            "arrangement: 34.\n",
            "accommodation: 1.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "territory: 31.\n",
            "surface: 200.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "size: 138.\n",
            "prominence: 5.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "exhibit: 25.\n",
            "memorabilia: 2.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "credit: 64.\n",
            "information: 269.\n",
            "Co-occur: 1.\n",
            "PMI: 1.735791921571262.\n",
            "\n",
            "territory: 31.\n",
            "kilometer: 8.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "death: 277.\n",
            "row: 35.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "doctor: 100.\n",
            "liability: 7.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "impartiality: 2.\n",
            "interest: 330.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "energy: 100.\n",
            "laboratory: 40.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "secretary: 191.\n",
            "senate: 62.\n",
            "Co-occur: 6.\n",
            "PMI: 4.8605916464264185.\n",
            "\n",
            "death: 277.\n",
            "inmate: 1.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "monk: 16.\n",
            "oracle: 2.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "cup: 45.\n",
            "food: 147.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "journal: 42.\n",
            "association: 132.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "street: 244.\n",
            "children: 355.\n",
            "Co-occur: 1.\n",
            "PMI: -0.5951582678270447.\n",
            "\n",
            "car: 274.\n",
            "flight: 46.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "space: 184.\n",
            "chemistry: 16.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "situation: 196.\n",
            "conclusion: 59.\n",
            "Co-occur: 1.\n",
            "PMI: 2.3099013906508366.\n",
            "\n",
            "word: 274.\n",
            "similarity: 9.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "peace: 198.\n",
            "plan: 205.\n",
            "Co-occur: 1.\n",
            "PMI: 0.49841756454283037.\n",
            "\n",
            "consumer: 38.\n",
            "energy: 100.\n",
            "Co-occur: 1.\n",
            "PMI: 3.9154705809095756.\n",
            "\n",
            "ministry: 13.\n",
            "culture: 58.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "smart: 21.\n",
            "student: 131.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "investigation: 51.\n",
            "effort: 145.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "image: 119.\n",
            "surface: 200.\n",
            "Co-occur: 3.\n",
            "PMI: 2.853542831766374.\n",
            "\n",
            "life: 715.\n",
            "term: 79.\n",
            "Co-occur: 1.\n",
            "PMI: 0.02167410428503119.\n",
            "\n",
            "start: 154.\n",
            "match: 41.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "computer: 13.\n",
            "news: 102.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "board: 239.\n",
            "recommendation: 24.\n",
            "Co-occur: 12.\n",
            "PMI: 6.906387476147137.\n",
            "\n",
            "lad: 6.\n",
            "brother: 73.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "observation: 27.\n",
            "architecture: 11.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "coast: 61.\n",
            "hill: 72.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "deployment: 1.\n",
            "departure: 17.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "benchmark: 0.\n",
            "index: 81.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "attempt: 95.\n",
            "peace: 198.\n",
            "Co-occur: 1.\n",
            "PMI: 1.6080420557173283.\n",
            "\n",
            "consumer: 38.\n",
            "confidence: 56.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "start: 154.\n",
            "year: 658.\n",
            "Co-occur: 5.\n",
            "PMI: 1.5004520645851054.\n",
            "\n",
            "focus: 40.\n",
            "life: 715.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "development: 334.\n",
            "issue: 152.\n",
            "Co-occur: 2.\n",
            "PMI: 1.1756224782102478.\n",
            "\n",
            "theater: 52.\n",
            "history: 286.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "situation: 196.\n",
            "isolation: 16.\n",
            "Co-occur: 1.\n",
            "PMI: 4.192544440012678.\n",
            "\n",
            "profit: 28.\n",
            "warning: 44.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "media: 13.\n",
            "trading: 25.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "chance: 131.\n",
            "credibility: 1.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "precedent: 9.\n",
            "information: 269.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "architecture: 11.\n",
            "century: 207.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "population: 136.\n",
            "development: 334.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "stock: 147.\n",
            "live: 177.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "peace: 198.\n",
            "atmosphere: 79.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "morality: 29.\n",
            "marriage: 95.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "minority: 20.\n",
            "peace: 198.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "atmosphere: 79.\n",
            "landscape: 20.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "report: 174.\n",
            "gain: 74.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "music: 216.\n",
            "project: 93.\n",
            "Co-occur: 1.\n",
            "PMI: 1.5132079708563861.\n",
            "\n",
            "seven: 113.\n",
            "series: 130.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "experience: 276.\n",
            "music: 216.\n",
            "Co-occur: 4.\n",
            "PMI: 1.9438423251862482.\n",
            "\n",
            "school: 493.\n",
            "center: 224.\n",
            "Co-occur: 4.\n",
            "PMI: 1.0544555256923702.\n",
            "\n",
            "five: 286.\n",
            "month: 130.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "announcement: 24.\n",
            "production: 148.\n",
            "Co-occur: 1.\n",
            "PMI: 4.01283841777778.\n",
            "\n",
            "morality: 29.\n",
            "importance: 108.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "money: 265.\n",
            "operation: 113.\n",
            "Co-occur: 1.\n",
            "PMI: 0.9372267722621364.\n",
            "\n",
            "delay: 21.\n",
            "news: 102.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "governor: 83.\n",
            "interview: 33.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "practice: 94.\n",
            "institution: 41.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "century: 207.\n",
            "nation: 139.\n",
            "Co-occur: 2.\n",
            "PMI: 1.994826253905053.\n",
            "\n",
            "coast: 61.\n",
            "forest: 66.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "shore: 61.\n",
            "woodland: 2.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "drink: 82.\n",
            "car: 274.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "president: 382.\n",
            "medal: 7.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "prejudice: 11.\n",
            "recognition: 44.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "viewer: 4.\n",
            "serial: 7.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "peace: 198.\n",
            "insurance: 46.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "Mars: 21.\n",
            "water: 445.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "media: 13.\n",
            "gain: 74.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "precedent: 9.\n",
            "cognition: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "announcement: 24.\n",
            "effort: 145.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "line: 298.\n",
            "insurance: 46.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "crane: 5.\n",
            "implement: 4.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "drink: 82.\n",
            "mother: 216.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "opera: 47.\n",
            "industry: 171.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "volunteer: 9.\n",
            "motto: 4.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "listing: 7.\n",
            "proximity: 5.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "precedent: 9.\n",
            "collection: 84.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "cup: 45.\n",
            "article: 68.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "sign: 94.\n",
            "recess: 2.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "problem: 313.\n",
            "airport: 19.\n",
            "Co-occur: 1.\n",
            "PMI: 3.2693079237516818.\n",
            "\n",
            "reason: 241.\n",
            "hypertension: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "direction: 134.\n",
            "combination: 57.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "Wednesday: 35.\n",
            "news: 102.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "glass: 99.\n",
            "magician: 4.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "cemetery: 15.\n",
            "woodland: 2.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "possibility: 87.\n",
            "girl: 220.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "cup: 45.\n",
            "substance: 33.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "forest: 66.\n",
            "graveyard: 7.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "stock: 147.\n",
            "egg: 12.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "month: 130.\n",
            "hotel: 126.\n",
            "Co-occur: 1.\n",
            "PMI: 1.807606547599515.\n",
            "\n",
            "energy: 100.\n",
            "secretary: 191.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "precedent: 9.\n",
            "group: 390.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "production: 148.\n",
            "hike: 4.\n",
            "Co-occur: 1.\n",
            "PMI: 6.597800918498936.\n",
            "\n",
            "stock: 147.\n",
            "phone: 54.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "holy: 49.\n",
            "sex: 84.\n",
            "Co-occur: 1.\n",
            "PMI: 3.8002270172339174.\n",
            "\n",
            "stock: 147.\n",
            "CD: 8.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "drink: 82.\n",
            "ear: 29.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "delay: 21.\n",
            "racism: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "stock: 147.\n",
            "life: 715.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "stock: 147.\n",
            "jaguar: 5.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "monk: 16.\n",
            "slave: 30.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "lad: 6.\n",
            "wizard: 3.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "sugar: 34.\n",
            "approach: 123.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "rooster: 3.\n",
            "voyage: 17.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "noon: 25.\n",
            "string: 19.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "chord: 7.\n",
            "smile: 58.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "professor: 57.\n",
            "cucumber: 0.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n",
            "king: 88.\n",
            "cabbage: 4.\n",
            "Co-occur: 0.\n",
            "PMI: 0.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "from collections import defaultdict\n",
        "\n",
        "# compute_pmi takes a dictionary containing words as keys,\n",
        "# word1 and word2 to compute the pmi of,\n",
        "# and a smoothing parameter\n",
        "# if smoothing = 0 and two words do not co-occur, pmi = 0\n",
        "\n",
        "def compute_pmi(occurrences, word1, word2, smoothing):\n",
        "  w1_occurrences = occurrences[word1.lower()]\n",
        "  w2_occurrences = occurrences[word2.lower()]\n",
        "\n",
        "  w1_count = len(w1_occurrences)\n",
        "  w2_count = len(w2_occurrences)\n",
        "\n",
        "  # Flatten the list of lists of strings(words)\n",
        "  # into a single list of strings (sentences)\n",
        "  w1_sentences = []\n",
        "  w2_sentences = []\n",
        "  for i in range(w1_count):\n",
        "    w1_sentences.append(' '.join(w1_occurrences[i]))\n",
        "  for i in range(w2_count):\n",
        "    w2_sentences.append(' '.join(w2_occurrences[i]))\n",
        "\n",
        "  # Eliminate repeated sentences\n",
        "  w1_set = set(w1_sentences)\n",
        "  w2_set = set(w2_sentences)\n",
        "\n",
        "  # Get only the sentences that are in both lists = co-occurrences\n",
        "  intersection = list(w1_set & w2_set)\n",
        "\n",
        "  # Add-1 Smoothing\n",
        "  w1_count += smoothing\n",
        "  w2_count += smoothing\n",
        "  w1w2_count = len(intersection) + smoothing\n",
        "\n",
        "  total_sents = len(nltk.corpus.brown.sents())\n",
        "\n",
        "  pmi = 0\n",
        "  if(w1w2_count != 0):\n",
        "    pmi = math.log2((w1w2_count/total_sents) / ((w1_count/total_sents)*(w2_count/total_sents)))\n",
        "\n",
        "  print(f\"{word1}: {w1_count}.\")\n",
        "  print(f\"{word2}: {w2_count}.\")\n",
        "  print(f\"Co-occur: {w1w2_count}.\")\n",
        "  print(f\"PMI: {pmi}.\\n\")\n",
        "\n",
        "  return pmi\n",
        "\n",
        "\n",
        "# Dictionary to store occurrences for each word\n",
        "# Key: word\n",
        "# Value: list of sentences where the word occurrs\n",
        "occurrences = defaultdict(list)\n",
        "\n",
        "for s in nltk.corpus.brown.sents():\n",
        "  for w in s:\n",
        "    occurrences[w.lower()].append(s)\n",
        "\n",
        "compute_pmi(occurrences, \"baseball\", \"season\", 0)\n",
        "#compute_pmi(occurrences, \"baseball\", \"season\", 1)\n",
        "\n",
        "comp_pmis = []\n",
        "for wordPair in wordPairs:\n",
        "  comp_pmis.append(compute_pmi(occurrences, wordPair[0], wordPair[1], 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jY1GUULJjuc2",
        "outputId": "7ce72188-d88c-4616-f5a7-d584ab6203d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed Pearson correlation coefficient is 0.226519011137628.\n",
            "Computed p value is 0.00028869853627923954.\n",
            "computer     -keyboard      Score=7.62000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "Jerusalem    -Israel        Score=8.46000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "planet       -galaxy        Score=8.11000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "canyon       -landscape     Score=7.53000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "OPEC         -country       Score=5.63000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "day          -summer        Score=3.94000 \tPMI=2.12435 \tLCH=2.25129.\n",
            "day          -dawn          Score=7.53000 \tPMI=2.57573 \tLCH=2.53897.\n",
            "country      -citizen       Score=7.31000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "planet       -people        Score=5.75000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "environment  -ecology       Score=8.81000 \tPMI=0.00000 \tLCH=2.94444.\n",
            "Maradona     -football      Score=8.62000 \tPMI=0.00000 \tLCH=0.00000.\n",
            "OPEC         -oil           Score=8.59000 \tPMI=0.00000 \tLCH=0.99853.\n",
            "money        -bank          Score=8.50000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "computer     -software      Score=8.50000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "law          -lawyer        Score=8.38000 \tPMI=4.15699 \tLCH=1.23969.\n",
            "weather      -forecast      Score=8.34000 \tPMI=0.00000 \tLCH=0.99853.\n",
            "network      -hardware      Score=8.31000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "nature       -environment   Score=8.31000 \tPMI=2.80356 \tLCH=1.84583.\n",
            "FBI          -investigation Score=8.31000 \tPMI=0.00000 \tLCH=0.92954.\n",
            "money        -wealth        Score=8.27000 \tPMI=3.29797 \tLCH=2.94444.\n",
            "psychology   -Freud         Score=8.21000 \tPMI=0.00000 \tLCH=0.64185.\n",
            "news         -report        Score=8.16000 \tPMI=3.27685 \tLCH=2.94444.\n",
            "war          -troops        Score=8.13000 \tPMI=3.54328 \tLCH=1.33500.\n",
            "physics      -proton        Score=8.12000 \tPMI=0.00000 \tLCH=0.92954.\n",
            "bank         -money         Score=8.12000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "stock        -market        Score=8.08000 \tPMI=5.33146 \tLCH=1.69168.\n",
            "planet       -constellation Score=8.06000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "credit       -card          Score=8.06000 \tPMI=5.10681 \tLCH=1.84583.\n",
            "hotel        -reservation   Score=8.03000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "closet       -clothes       Score=8.00000 \tPMI=6.91648 \tLCH=1.55814.\n",
            "soap         -opera         Score=7.94000 \tPMI=5.79323 \tLCH=0.99853.\n",
            "planet       -astronomer    Score=7.94000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "planet       -space         Score=7.92000 \tPMI=3.89137 \tLCH=1.69168.\n",
            "movie        -theater       Score=7.92000 \tPMI=6.83380 \tLCH=1.69168.\n",
            "treatment    -recovery      Score=7.91000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "baby         -mother        Score=7.85000 \tPMI=2.09817 \tLCH=2.02815.\n",
            "money        -deposit       Score=7.73000 \tPMI=0.00000 \tLCH=2.53897.\n",
            "television   -film          Score=7.72000 \tPMI=3.57844 \tLCH=2.02815.\n",
            "psychology   -mind          Score=7.69000 \tPMI=3.65560 \tLCH=1.69168.\n",
            "game         -team          Score=7.69000 \tPMI=3.48970 \tLCH=1.33500.\n",
            "admission    -ticket        Score=7.69000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "Jerusalem    -Palestinian   Score=7.65000 \tPMI=0.00000 \tLCH=0.86500.\n",
            "Arafat       -terror        Score=7.65000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "boxing       -round         Score=7.61000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "computer     -internet      Score=7.58000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "money        -property      Score=7.57000 \tPMI=2.79393 \tLCH=2.53897.\n",
            "tennis       -racket        Score=7.56000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "telephone    -communication Score=7.50000 \tPMI=3.49324 \tLCH=1.23969.\n",
            "currency     -market        Score=7.50000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "psychology   -cognition     Score=7.48000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "seafood      -sea           Score=7.47000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "book         -paper         Score=7.46000 \tPMI=0.00000 \tLCH=2.53897.\n",
            "book         -library       Score=7.46000 \tPMI=4.55293 \tLCH=2.53897.\n",
            "psychology   -depression    Score=7.42000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "fighting     -defeating     Score=7.41000 \tPMI=0.00000 \tLCH=0.00000.\n",
            "movie        -star          Score=7.38000 \tPMI=7.30542 \tLCH=1.55814.\n",
            "hundred      -percent       Score=7.38000 \tPMI=2.66148 \tLCH=1.07264.\n",
            "dollar       -profit        Score=7.38000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "money        -possession    Score=7.29000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "cup          -drink         Score=7.25000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "psychology   -health        Score=7.23000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "summer       -drought       Score=7.16000 \tPMI=6.41924 \tLCH=2.25129.\n",
            "investor     -earning       Score=7.13000 \tPMI=0.00000 \tLCH=0.00000.\n",
            "company      -stock         Score=7.08000 \tPMI=3.01264 \tLCH=1.84583.\n",
            "stroke       -hospital      Score=7.03000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "liability    -insurance     Score=7.03000 \tPMI=9.06130 \tLCH=1.69168.\n",
            "game         -victory       Score=7.03000 \tPMI=4.51896 \tLCH=1.69168.\n",
            "psychology   -anxiety       Score=7.00000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "game         -defeat        Score=6.97000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "FBI          -fingerprint   Score=6.94000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "money        -withdrawal    Score=6.88000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "psychology   -fear          Score=6.85000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "drug         -abuse         Score=6.85000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "concert      -virtuoso      Score=6.81000 \tPMI=8.93689 \tLCH=1.07264.\n",
            "computer     -laboratory    Score=6.78000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "love         -sex           Score=6.77000 \tPMI=3.14815 \tLCH=2.94444.\n",
            "problem      -challenge     Score=6.75000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "movie        -critic        Score=6.73000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "Arafat       -peace         Score=6.73000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "bed          -closet        Score=6.72000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "lawyer       -evidence      Score=6.69000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "fertility    -egg           Score=6.69000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "precedent    -law           Score=6.65000 \tPMI=4.41333 \tLCH=2.94444.\n",
            "minister     -party         Score=6.63000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "psychology   -clinic        Score=6.58000 \tPMI=0.00000 \tLCH=0.99853.\n",
            "cup          -coffee        Score=6.58000 \tPMI=8.11746 \tLCH=1.84583.\n",
            "water        -seepage       Score=6.56000 \tPMI=6.00959 \tLCH=1.15268.\n",
            "government   -crisis        Score=6.56000 \tPMI=1.74234 \tLCH=1.69168.\n",
            "space        -world         Score=6.53000 \tPMI=1.24844 \tLCH=2.02815.\n",
            "dividend     -calculation   Score=6.48000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "victim       -emergency     Score=6.47000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "luxury       -car           Score=6.47000 \tPMI=0.00000 \tLCH=0.92954.\n",
            "tool         -implement     Score=6.46000 \tPMI=0.00000 \tLCH=2.94444.\n",
            "competition  -price         Score=6.44000 \tPMI=5.39701 \tLCH=1.55814.\n",
            "psychology   -doctor        Score=6.42000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "gender       -equality      Score=6.41000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "listing      -category      Score=6.38000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "video        -archive       Score=6.34000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "oil          -stock         Score=6.34000 \tPMI=2.03773 \tLCH=1.44036.\n",
            "governor     -office        Score=6.34000 \tPMI=2.43786 \tLCH=1.33500.\n",
            "discovery    -space         Score=6.34000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "record       -number        Score=6.31000 \tPMI=-0.17342 \tLCH=2.94444.\n",
            "brother      -monk          Score=6.27000 \tPMI=0.00000 \tLCH=2.94444.\n",
            "production   -crew          Score=6.25000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "nature       -man           Score=6.25000 \tPMI=2.16254 \tLCH=2.25129.\n",
            "family       -planning      Score=6.25000 \tPMI=3.23269 \tLCH=1.33500.\n",
            "disaster     -area          Score=6.25000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "food         -preparation   Score=6.22000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "preservation -world         Score=6.19000 \tPMI=2.09957 \tLCH=1.69168.\n",
            "movie        -popcorn       Score=6.19000 \tPMI=0.00000 \tLCH=0.99853.\n",
            "lover        -quarrel       Score=6.19000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "game         -series        Score=6.19000 \tPMI=3.42733 \tLCH=2.53897.\n",
            "dollar       -loss          Score=6.09000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "weapon       -secret        Score=6.06000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "shower       -flood         Score=6.03000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "registration -arrangement   Score=6.00000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "arrival      -hotel         Score=6.00000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "announcement -warning       Score=6.00000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "game         -round         Score=5.97000 \tPMI=2.63592 \tLCH=2.53897.\n",
            "baseball     -season        Score=5.97000 \tPMI=5.26012 \tLCH=0.92954.\n",
            "drink        -mouth         Score=5.96000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "life         -lesson        Score=5.94000 \tPMI=3.46747 \tLCH=1.55814.\n",
            "grocery      -money         Score=5.94000 \tPMI=0.00000 \tLCH=0.99853.\n",
            "energy       -crisis        Score=5.94000 \tPMI=2.80585 \tLCH=1.69168.\n",
            "reason       -criterion     Score=5.91000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "equipment    -maker         Score=5.91000 \tPMI=6.42355 \tLCH=1.44036.\n",
            "cup          -liquid        Score=5.90000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "deployment   -withdrawal    Score=5.88000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "tiger        -zoo           Score=5.87000 \tPMI=9.82997 \tLCH=1.55814.\n",
            "journey      -car           Score=5.85000 \tPMI=0.00000 \tLCH=0.74721.\n",
            "money        -laundering    Score=5.65000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "summer       -nature        Score=5.63000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "decoration   -valor         Score=5.63000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "Mars         -scientist     Score=5.63000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "alcohol      -chemistry     Score=5.54000 \tPMI=8.10681 \tLCH=2.02815.\n",
            "disability   -death         Score=5.47000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "change       -attitude      Score=5.44000 \tPMI=2.74386 \tLCH=1.84583.\n",
            "arrangement  -accommodation Score=5.41000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "territory    -surface       Score=5.34000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "size         -prominence    Score=5.31000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "exhibit      -memorabilia   Score=5.31000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "credit       -information   Score=5.31000 \tPMI=1.73579 \tLCH=2.25129.\n",
            "territory    -kilometer     Score=5.28000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "death        -row           Score=5.25000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "doctor       -liability     Score=5.19000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "impartiality -interest      Score=5.16000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "energy       -laboratory    Score=5.09000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "secretary    -senate        Score=5.06000 \tPMI=4.86059 \tLCH=0.99853.\n",
            "death        -inmate        Score=5.03000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "monk         -oracle        Score=5.00000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "cup          -food          Score=5.00000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "journal      -association   Score=4.97000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "street       -children      Score=4.94000 \tPMI=-0.59516 \tLCH=1.33500.\n",
            "car          -flight        Score=4.94000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "space        -chemistry     Score=4.88000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "situation    -conclusion    Score=4.81000 \tPMI=2.30990 \tLCH=1.69168.\n",
            "word         -similarity    Score=4.75000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "peace        -plan          Score=4.75000 \tPMI=0.49842 \tLCH=1.33500.\n",
            "consumer     -energy        Score=4.75000 \tPMI=3.91547 \tLCH=1.33500.\n",
            "ministry     -culture       Score=4.69000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "smart        -student       Score=4.62000 \tPMI=0.00000 \tLCH=0.99853.\n",
            "investigation-effort        Score=4.59000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "image        -surface       Score=4.56000 \tPMI=2.85354 \tLCH=2.02815.\n",
            "life         -term          Score=4.50000 \tPMI=0.02167 \tLCH=2.53897.\n",
            "start        -match         Score=4.47000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "computer     -news          Score=4.47000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "board        -recommendation Score=4.47000 \tPMI=6.90639 \tLCH=1.15268.\n",
            "lad          -brother       Score=4.46000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "observation  -architecture  Score=4.38000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "coast        -hill          Score=4.38000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "deployment   -departure     Score=4.25000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "benchmark    -index         Score=4.25000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "attempt      -peace         Score=4.25000 \tPMI=1.60804 \tLCH=1.33500.\n",
            "consumer     -confidence    Score=4.13000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "start        -year          Score=4.06000 \tPMI=1.50045 \tLCH=1.84583.\n",
            "focus        -life          Score=4.06000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "development  -issue         Score=3.97000 \tPMI=1.17562 \tLCH=2.25129.\n",
            "theater      -history       Score=3.91000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "situation    -isolation     Score=3.88000 \tPMI=4.19254 \tLCH=2.25129.\n",
            "profit       -warning       Score=3.88000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "media        -trading       Score=3.88000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "chance       -credibility   Score=3.88000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "precedent    -information   Score=3.85000 \tPMI=0.00000 \tLCH=2.53897.\n",
            "architecture -century       Score=3.78000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "population   -development   Score=3.75000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "stock        -live          Score=3.73000 \tPMI=0.00000 \tLCH=0.00000.\n",
            "peace        -atmosphere    Score=3.69000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "morality     -marriage      Score=3.69000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "minority     -peace         Score=3.69000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "atmosphere   -landscape     Score=3.69000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "report       -gain          Score=3.63000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "music        -project       Score=3.63000 \tPMI=1.51321 \tLCH=2.25129.\n",
            "seven        -series        Score=3.56000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "experience   -music         Score=3.47000 \tPMI=1.94384 \tLCH=1.84583.\n",
            "school       -center        Score=3.44000 \tPMI=1.05446 \tLCH=2.53897.\n",
            "five         -month         Score=3.38000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "announcement -production    Score=3.38000 \tPMI=4.01284 \tLCH=1.69168.\n",
            "morality     -importance    Score=3.31000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "money        -operation     Score=3.31000 \tPMI=0.93723 \tLCH=1.44036.\n",
            "delay        -news          Score=3.31000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "governor     -interview     Score=3.25000 \tPMI=0.00000 \tLCH=0.99853.\n",
            "practice     -institution   Score=3.19000 \tPMI=0.00000 \tLCH=2.53897.\n",
            "century      -nation        Score=3.16000 \tPMI=1.99483 \tLCH=1.55814.\n",
            "coast        -forest        Score=3.15000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "shore        -woodland      Score=3.08000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "drink        -car           Score=3.04000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "president    -medal         Score=3.00000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "prejudice    -recognition   Score=3.00000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "viewer       -serial        Score=2.97000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "peace        -insurance     Score=2.94000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "Mars         -water         Score=2.94000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "media        -gain          Score=2.88000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "precedent    -cognition     Score=2.81000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "announcement -effort        Score=2.75000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "line         -insurance     Score=2.69000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "crane        -implement     Score=2.69000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "drink        -mother        Score=2.65000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "opera        -industry      Score=2.63000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "volunteer    -motto         Score=2.56000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "listing      -proximity     Score=2.56000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "precedent    -collection    Score=2.50000 \tPMI=0.00000 \tLCH=2.53897.\n",
            "cup          -article       Score=2.40000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "sign         -recess        Score=2.38000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "problem      -airport       Score=2.38000 \tPMI=3.26931 \tLCH=0.99853.\n",
            "reason       -hypertension  Score=2.31000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "direction    -combination   Score=2.25000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "Wednesday    -news          Score=2.22000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "glass        -magician      Score=2.08000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "cemetery     -woodland      Score=2.08000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "possibility  -girl          Score=1.94000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "cup          -substance     Score=1.92000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "forest       -graveyard     Score=1.85000 \tPMI=0.00000 \tLCH=1.44036.\n",
            "stock        -egg           Score=1.81000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "month        -hotel         Score=1.81000 \tPMI=1.80761 \tLCH=1.15268.\n",
            "energy       -secretary     Score=1.81000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "precedent    -group         Score=1.77000 \tPMI=0.00000 \tLCH=2.25129.\n",
            "production   -hike          Score=1.75000 \tPMI=6.59780 \tLCH=1.55814.\n",
            "stock        -phone         Score=1.62000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "holy         -sex           Score=1.62000 \tPMI=3.80023 \tLCH=1.33500.\n",
            "stock        -CD            Score=1.31000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "drink        -ear           Score=1.31000 \tPMI=0.00000 \tLCH=1.55814.\n",
            "delay        -racism        Score=1.19000 \tPMI=0.00000 \tLCH=1.69168.\n",
            "stock        -life          Score=0.92000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "stock        -jaguar        Score=0.92000 \tPMI=0.00000 \tLCH=1.84583.\n",
            "monk         -slave         Score=0.92000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "lad          -wizard        Score=0.92000 \tPMI=0.00000 \tLCH=2.02815.\n",
            "sugar        -approach      Score=0.88000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "rooster      -voyage        Score=0.62000 \tPMI=0.00000 \tLCH=0.45953.\n",
            "noon         -string        Score=0.54000 \tPMI=0.00000 \tLCH=1.15268.\n",
            "chord        -smile         Score=0.54000 \tPMI=0.00000 \tLCH=1.23969.\n",
            "professor    -cucumber      Score=0.31000 \tPMI=0.00000 \tLCH=1.07264.\n",
            "king         -cabbage       Score=0.23000 \tPMI=0.00000 \tLCH=1.33500.\n",
            "Number of sentences in the Brown Corpus: 57340.\n"
          ]
        }
      ],
      "source": [
        "pmi_corr_coeff, pmi_p_val = compute_correlation(scores, comp_pmis)\n",
        "print(f\"Computed Pearson correlation coefficient is {pmi_corr_coeff}.\")\n",
        "print(f\"Computed p value is {pmi_p_val}.\")\n",
        "\n",
        "for i in range(len(wordPairs)):\n",
        "  print(f\"{wordPairs[i][0]:13s}-{wordPairs[i][1]:13s} Score={scores[i]:6.5f} \\tPMI={comp_pmis[i]:6.5f} \\tLCH={compsims[i]:6.5f}.\")\n",
        "\n",
        "print(f\"Number of sentences in the Brown Corpus: {len(nltk.corpus.brown.sents())}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing PMI with a Smoothing factor of 1:\n",
        "- Computed Pearson correlation coefficient is 0.1470437495408519.\n",
        "-Computed p value is 0.019525119848715456.\n",
        "\n",
        "Computing PMI with a Smoothing factor of 0\n",
        "(thus if co-occurrence = 0 --> pmi = 0, which would mean \"the two words are independent\")\n",
        "- Computed Pearson correlation coefficient is 0.226519011137628.\n",
        "- Computed p value is 0.00028869853627923954.\n",
        "\n",
        "The p value seems to be well under 0.05 in both cases, which would point towards correlation."
      ],
      "metadata": {
        "id": "WkOKzq0zIK5F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqO1dd4cOYQT"
      },
      "source": [
        "❓Inspect the scores output by the PMI method. Do they always make sense? If not, what are possible reasons? For which pairs of words do they work exceptionally well? Enter your answer into Vips.\n",
        "\n",
        "❓ Optional open-ended exercise: Come up with an automatic method to identify those (hint: using rankings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JTMOHeaGnnD"
      },
      "source": [
        "## References\n",
        "\n",
        "Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, Aitor Soroa, A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches, In Proceedings of NAACL-HLT 2009."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}