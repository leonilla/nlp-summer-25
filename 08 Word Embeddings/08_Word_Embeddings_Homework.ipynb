{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er_UDweQVueu"
      },
      "source": [
        "## Introduction to Natural Language Processing\n",
        "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
        "Prof. Dr. Annemarie Friedrich<br/>\n",
        "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
        "Date: **SS 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiC7ZTwtVvzE"
      },
      "source": [
        "# 8. Word Embeddings (Homework)\n",
        "\n",
        "**Learning Goals:**\n",
        "\n",
        "* Implement multi-class classification with a multi-layer perceptron\n",
        "* Understand PyTorch tensors\n",
        "* Load pre-trained word embeddings and use them in a neural network\n",
        "* Perform stance classification\n",
        "\n",
        "But first, some imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SRlEyo-Vu5_",
        "outputId": "448f43e5-90e8-44d6-af5b-ccad956591d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting datasets\n",
            "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: datasets\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed datasets\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mComputing on: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.5  # gensim compatibility\n",
        "!pip install -U datasets\n",
        "!pip install nltk\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import random\n",
        "\n",
        "import scipy\n",
        "import gensim.downloader as api\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CosineSimilarity\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Computing on:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0vdFyQDY7CE"
      },
      "source": [
        "## Loading Pre-trained Word Embeddings\n",
        "\n",
        "First, we will load pre-trained word embeddings via the [gensim](https://radimrehurek.com/gensim/) library. We use this library to avoid downloading the files to our local machines, but in theory, you can do that as well. You can find pre-trained word embeddings on many places in the web, e.g., [here](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/).\n",
        "\n",
        "The next code cell will download the 300-dimensional word2vec vectors pre-trained on Google News (be patient, this will take around 5-10 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wIuUScucaNLB"
      },
      "outputs": [],
      "source": [
        "word_vectors = api.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJHB4V2FbyXX"
      },
      "source": [
        "The `gensim` library provides several functionalities for querying the word embeddings (called _keyed vectors_). Check how to use the function `similarity` on [this page](https://radimrehurek.com/gensim/models/keyedvectors.html).\n",
        "\n",
        "❓Re-iterate the exercise for word similarity ratings on `wordsim_relatedness_goldstandard.txt`. Compute the system ratings for all word pairs using this function. How does your result compare to your earlier results using LCH and PMI?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "61hU2nP8bnAU"
      },
      "outputs": [],
      "source": [
        "def compute_correlation(human_ratings, system_ratings):\n",
        "  \"\"\" Input: two lists (of equal length) with numeric values.\n",
        "  Computes Pearson's correlation coefficient.\n",
        "  \"\"\"\n",
        "  assert len(human_ratings), len(system_ratings)\n",
        "  return scipy.stats.pearsonr(human_ratings, system_ratings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcJJOgkyWazq",
        "outputId": "ebc78527-7457-4afb-a524-1af245e293a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PearsonRResult(statistic=0.5920509820875375, pvalue=3.143384293094993e-25)\n"
          ]
        }
      ],
      "source": [
        "# Compute similarities of word embeddings using cosine\n",
        "\n",
        "# A list for all the word pairs (word0, word1)\n",
        "wordPairs = []\n",
        "# A list for all the (float) ((human)) scores\n",
        "scores = []\n",
        "# A list for all the  computed similarities\n",
        "sims = []\n",
        "\n",
        "with open('/content/wordsim_relatedness_goldstandard.csv', newline='') as csvfile:\n",
        "    filereader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
        "\n",
        "    for line in filereader:\n",
        "        wordPairs.append((line[0], line[1]))\n",
        "        scores.append(float(line[2]))\n",
        "\n",
        "for wp in wordPairs:\n",
        "        sim = word_vectors.similarity(wp[0], wp[1])\n",
        "        sims.append(sim)\n",
        "\n",
        "print(f\"{compute_correlation(scores, sims)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUqc0jaufUCy"
      },
      "source": [
        "If your implementation is correct, the output should be:\n",
        "\n",
        "```\n",
        "PearsonRResult(statistic=0.5920509820875375, pvalue=3.143384293094993e-25)\n",
        "```\n",
        "\n",
        "This correlation is pretty high!\n",
        "<br/>\n",
        "<br/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp_ZXqpScJw8"
      },
      "source": [
        "## PyTorch Tensors\n",
        "\n",
        "❓ In order to understand the following code and solve the following exercises, you need to have a basic understanding of PyTorch tensors. Work through the following tutorial and note down the most important facts about tensors here in this notebook (with code examples).\n",
        "\n",
        "[PyTorch.org Tutorial on Tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FiAqgrccXZ"
      },
      "source": [
        "_Your text here_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tQp2FbuXcezK"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aphnoj8cooz"
      },
      "source": [
        "\n",
        "### Initializing a PyTorch Embedding Layer With Pretrained Embeddings\n",
        "\n",
        "The code in the next cell takes word2vec's weight matrix from gensim and initalizes an [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer in PyTorch. This is a layer that can be queried using indices and that returns the word embeddings (or rather, passes them on to the next layer).\n",
        "\n",
        "Word embeddings are of the first layer of a neural network and can either be \"frozen\" (i.e., we optimize only the rest of the model's parameters) or optimized further during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3i2Foa3fcvJ",
        "outputId": "a0c983fb-5f7c-433e-e1ce-505fd9944b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 1000st word of the vocabulary: raised\n",
            "torch.Size([1, 300])\n",
            "tensor([[ 2.6733e-02, -9.0820e-02,  2.7832e-02,  2.0410e-01,  6.2256e-03,\n",
            "         -9.0332e-02,  2.2583e-02, -1.6113e-01,  1.3281e-01,  6.1035e-02,\n",
            "         -1.5747e-02,  8.8379e-02,  1.3794e-02,  4.6387e-02, -5.5908e-02,\n",
            "         -6.6895e-02,  1.2268e-02,  1.3672e-01,  1.5430e-01, -4.6143e-02,\n",
            "         -3.9307e-02, -1.5430e-01, -1.6504e-01,  1.0791e-01,  3.3203e-02,\n",
            "         -5.1025e-02,  3.7109e-02,  1.0156e-01,  1.1035e-01,  2.0508e-02,\n",
            "          6.7749e-03,  1.1826e-03, -1.2512e-02, -1.2500e-01,  1.4832e-02,\n",
            "         -2.6855e-02, -2.1484e-02,  1.5076e-02,  1.3867e-01,  4.8584e-02,\n",
            "         -7.6660e-02, -1.1670e-01,  1.0693e-01,  4.1748e-02,  1.2817e-02,\n",
            "         -9.4604e-03, -2.8931e-02, -3.8574e-02,  2.4316e-01,  9.5215e-03,\n",
            "          2.2095e-02,  2.2266e-01,  9.1553e-03, -4.5410e-02, -3.5400e-02,\n",
            "          1.4062e-01, -1.8457e-01,  7.7637e-02,  4.1504e-02, -8.4961e-02,\n",
            "         -9.9121e-02,  5.8350e-02, -9.6680e-02, -2.0215e-01, -1.4038e-02,\n",
            "         -2.3651e-03,  1.4746e-01,  2.0020e-01,  5.9570e-02,  1.5430e-01,\n",
            "          1.3477e-01,  5.2795e-03,  1.2500e-01,  8.5449e-02, -2.7710e-02,\n",
            "         -5.8105e-02,  1.8359e-01,  7.8735e-03, -1.5332e-01,  1.2402e-01,\n",
            "         -8.0078e-02, -1.4355e-01,  1.4941e-01,  1.4587e-02,  1.0791e-01,\n",
            "         -2.0117e-01, -1.5039e-01,  5.2490e-02,  7.7148e-02,  9.1797e-02,\n",
            "         -3.8086e-02,  1.4844e-01,  5.4688e-02, -1.5137e-01,  1.4282e-02,\n",
            "         -1.0498e-01,  1.9043e-02, -6.3477e-02,  5.3467e-02,  3.4912e-02,\n",
            "          1.3965e-01, -1.3379e-01,  2.1680e-01, -1.9434e-01, -5.8350e-02,\n",
            "         -1.3477e-01, -2.6562e-01, -1.0400e-01,  3.5400e-02, -2.1582e-01,\n",
            "          8.2520e-02,  4.5166e-02, -6.9824e-02, -4.3213e-02,  2.6978e-02,\n",
            "         -9.0332e-02,  5.4932e-03,  4.9805e-02, -3.5645e-02,  5.9814e-02,\n",
            "         -1.4941e-01, -2.2095e-02, -3.3203e-02,  1.7578e-01, -6.6406e-02,\n",
            "         -1.8311e-02,  1.1292e-02, -4.2236e-02, -7.7148e-02,  1.7456e-02,\n",
            "         -1.0498e-01, -1.0449e-01, -4.7363e-02, -2.9541e-02, -6.1523e-02,\n",
            "         -5.0781e-02, -2.5635e-02, -9.5215e-02, -8.1055e-02, -1.0156e-01,\n",
            "          2.0215e-01,  1.1865e-01, -2.8229e-03, -6.0303e-02,  2.2461e-02,\n",
            "          1.3086e-01,  8.0566e-02, -1.5430e-01, -8.2520e-02,  1.6016e-01,\n",
            "          5.7861e-02,  9.7656e-02, -2.0996e-02, -4.5166e-02, -7.3242e-02,\n",
            "          4.3640e-03, -9.0820e-02,  1.9165e-02, -1.6602e-02, -5.0293e-02,\n",
            "          1.4709e-02, -4.1504e-03,  3.4668e-02,  5.7373e-02,  8.0078e-02,\n",
            "          6.2256e-03,  6.3965e-02,  2.4536e-02,  3.1738e-02, -1.2500e-01,\n",
            "         -7.8125e-02, -2.4536e-02, -7.2266e-02, -8.6426e-02, -7.7148e-02,\n",
            "          4.3457e-02, -1.8787e-04, -1.1414e-02, -9.9121e-02,  2.6245e-02,\n",
            "          5.3467e-02,  4.5410e-02, -7.1289e-02,  1.3867e-01,  4.1016e-02,\n",
            "          1.1169e-02, -1.5320e-02,  3.2959e-02,  1.8262e-01,  1.7456e-02,\n",
            "         -3.1982e-02,  1.0791e-01,  3.2959e-02, -3.5156e-02, -2.1777e-01,\n",
            "          1.0205e-01, -2.9297e-02, -9.4604e-04, -7.1411e-03, -2.6367e-02,\n",
            "          6.1768e-02, -1.6968e-02, -2.1729e-02, -1.1914e-01,  9.0942e-03,\n",
            "          1.0303e-01, -3.0060e-03,  1.4941e-01,  1.0596e-01, -4.0283e-02,\n",
            "         -1.8433e-02,  3.5889e-02, -3.8086e-02,  5.6885e-02,  1.5320e-02,\n",
            "          1.9775e-02,  1.8066e-01,  8.1787e-03, -1.5137e-01,  3.2227e-02,\n",
            "          1.5723e-01,  5.0781e-02, -2.8931e-02,  4.3945e-02, -5.8594e-02,\n",
            "          3.0975e-03, -1.2634e-02,  1.6113e-01,  1.0596e-01, -3.3936e-02,\n",
            "          1.8164e-01, -4.4678e-02,  3.4180e-02, -3.7842e-02, -8.8501e-03,\n",
            "         -3.6865e-02,  7.8613e-02,  2.7100e-02,  4.6143e-02,  6.8848e-02,\n",
            "          5.0537e-02, -1.7471e-03, -1.3672e-01, -1.5332e-01,  9.8633e-02,\n",
            "         -1.6113e-01,  6.6223e-03, -8.5938e-02, -1.7578e-02,  4.0771e-02,\n",
            "          2.9907e-02,  1.1414e-02, -2.0264e-02, -6.4453e-02,  1.7456e-02,\n",
            "         -1.2891e-01, -3.4714e-04,  4.2236e-02,  3.2959e-03,  1.2256e-01,\n",
            "         -9.5703e-02,  9.2285e-02,  1.0498e-01, -1.2451e-01,  3.5889e-02,\n",
            "          1.4551e-01, -1.0547e-01,  2.2949e-02, -8.3618e-03,  4.6387e-03,\n",
            "          2.1973e-01, -4.9561e-02,  2.3828e-01, -5.8350e-02,  4.8340e-02,\n",
            "          6.0547e-02, -3.7354e-02, -1.7773e-01,  4.4922e-02, -4.2236e-02,\n",
            "          8.2520e-02,  1.1035e-01, -1.0938e-01,  9.4238e-02, -7.2266e-02,\n",
            "          4.9072e-02, -1.5820e-01,  7.8125e-02,  2.9541e-02, -1.2109e-01,\n",
            "          2.6855e-02, -2.7954e-02,  3.0884e-02,  4.0527e-02, -1.3086e-01,\n",
            "          8.3008e-02,  1.5747e-02, -1.1670e-01, -2.9419e-02, -7.0801e-02]])\n"
          ]
        }
      ],
      "source": [
        "# Create a float tensor from the word vectors\n",
        "weights = torch.FloatTensor(word_vectors.vectors) # two-dimensional matrix. Rows = vocabulary items, columns = dimensions of word embedding.\n",
        "embedding = nn.Embedding.from_pretrained(weights)\n",
        "\n",
        "# Query embedding layer for a specific word embedding\n",
        "print(\"The 1000st word of the vocabulary:\", word_vectors.index_to_key[999])\n",
        "input = torch.LongTensor([5]) # the 1000st word in the vocabulary\n",
        "\n",
        "print(embedding(input).shape)\n",
        "print(embedding(input)) # uncomment this to look at the tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTjPaj0d5CM9"
      },
      "source": [
        "You may wonder about the extra dimension `1` above. Keep in mind that the embedding layer contains an embedding tensor (matrix) of size [vocab_size, 300] where 300 is the dimensionality of the word embeddings.\n",
        "When selecting a row from this (the vector for the word \"raised\" that we queried above), PyTorch keeps this extra dimension, it always provides for the possibility that we might want to retrieve more than one vector a time. For example, we could retrieve the two embeddings of two words, which are returned as new matrix with dimensions [2,300], i.e., the first dimension corresponds to the number of inputs.\n",
        "\n",
        "If you haven't checked it out yet, now is the time to work through [Towards DataScience: Understanding Dimensions in PyTorch](https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be) unless you are already familiar with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wee71YDf5BkH",
        "outputId": "0dbb2da6-3795-476a-b0fc-409934f78a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 300])\n"
          ]
        }
      ],
      "source": [
        "input = torch.LongTensor([77, 88])\n",
        "print(embedding(input).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAknmjly8bWu"
      },
      "source": [
        "If we want to get rid of this extra dimension to have a vector that corresponds to a simple vector, we can use `squeeze`.\n",
        "(Note: we did that already in 07_Neural_Networks.ipynb when we removed the batch dimension. There, we collected inputs into batches, and the model returned the predictions similarly packed into batches. We used `squeeze` to create a simple vector from these probability scores predicted by the binary classifier, as our gold labels happened to have this format, and the loss function expected this format.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56k2SBZ58chG",
        "outputId": "5a963950-e4f2-4feb-d430-7502c2dda318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 300])\n",
            "torch.Size([300])\n"
          ]
        }
      ],
      "source": [
        "input = torch.LongTensor([5]) # the 1000st word in the vocabulary\n",
        "emb_5 = embedding(input)\n",
        "print(emb_5.shape) # this has an \"outer\" tensor dimension\n",
        "emb_5 = emb_5.squeeze(0) # removed the first dimension (~ the outer list)\n",
        "print(emb_5.shape) # this is just a vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aInAfxku-uUl"
      },
      "source": [
        "Gensim's `word_vector` object provides two dictionaries: `key_to_index`, which returns the row corresponding to a word, and `index_to_key`, which maps a row index to the corresponding word. We use the latter to display some words of word2vec's vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSzNNWK2_GV6",
        "outputId": "84475817-b04c-452b-9004-70dbf1862900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "</s> in for that is on ## The with said was the at not as it be from by are I have he will has #### his an this or \n",
            "doing face low higher site once yet hours America control received rate career Bush teams known offer race ever experience \n"
          ]
        }
      ],
      "source": [
        "vocab = \"\"\n",
        "for i in range(0,30):\n",
        "  vocab += word_vectors.index_to_key[i] + \" \"\n",
        "vocab += \"\\n\"\n",
        "for i in range(500,520):\n",
        "  vocab += word_vectors.index_to_key[i] + \" \"\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CClsdRGVzw33"
      },
      "source": [
        "From this, we can tell that vocabulary has not been lemmatized (e.g., \"doing\" and \"known\" are not lemmas, and \"The\" is uppercase, etc.). Hence, if we want to work with these embeddings, standard tokenization should do the trick. (❗This is not always the case, vocabularies differ in their preprocessing!)\n",
        "\n",
        "The `similarity` function of the gensim library that we have used above computes the cosine between the word vectors. We can compute cosine similarity using PyTorch as well. Let's do that in order to get a bit more familiar with tensors in PyTorch.\n",
        "\n",
        "❓The code below compares the similarity scores returned by the two frameworks for \"bike\" and \"car\". Add the comparisons \"motorbike-bike\" and \"motorbike-car\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suF7nl7WVkyS",
        "outputId": "f2fca78a-d7c1-4514-b856-0ef10c1675f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "car-bike: tensor([0.5854]) 0.58541536\n",
            "motorbike-bike: tensor([0.6345]) 0.63453037\n",
            "motorbike-car: tensor([0.5921]) 0.59211683\n"
          ]
        }
      ],
      "source": [
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "idx_bike = word_vectors.key_to_index[\"bike\"]\n",
        "idx_car = word_vectors.key_to_index[\"car\"]\n",
        "\n",
        "emb_car = embedding(torch.LongTensor([idx_car]))\n",
        "emb_bike = embedding(torch.LongTensor([idx_bike]))\n",
        "\n",
        "print(\"car-bike:\", cos(emb_car, emb_bike), word_vectors.similarity(\"car\", \"bike\"))\n",
        "\n",
        "# Compare motorbike-bike and motorbike-car.\n",
        "idx_motorbike = word_vectors.key_to_index[\"motorbike\"]\n",
        "emb_motorbike = embedding(torch.LongTensor([idx_motorbike]))\n",
        "\n",
        "print(\"motorbike-bike:\", cos(emb_motorbike, emb_bike), word_vectors.similarity(\"motorbike\", \"bike\"))\n",
        "print(\"motorbike-car:\", cos(emb_motorbike, emb_car), word_vectors.similarity(\"motorbike\", \"car\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HogLcZFF13Lm"
      },
      "source": [
        "Same results! The difference is the data type: gensim returns simple floats, PyTorch returns tensors (even if the tensor has only one dimension and a single value here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp70NB5Xh3n1"
      },
      "source": [
        "### Load the TweetEval Dataset\n",
        "\n",
        "We will now work with the [TweetEval](https://huggingface.co/datasets/tweet_eval) dataset.\n",
        "Your task is to train a classifier using word embeddings for predicting the _stance_ of a tweet towards \"climate change\":\n",
        "\n",
        "* neutral (0)\n",
        "* against (1) - This label does not mean the person is against climate change, but does not believe that climate change is happening.\n",
        "* favor (2) - This label does not mean that the person is in favor of climate change, but believes that climate change is real and that we need to do something about it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "LRrZq_gZis4g"
      },
      "outputs": [],
      "source": [
        "train_data = load_dataset(\"tweet_eval\", \"stance_climate\", split=\"train\")\n",
        "val_data = load_dataset(\"tweet_eval\", \"stance_climate\", split=\"validation\")\n",
        "test_data = load_dataset(\"tweet_eval\", \"stance_climate\", split=\"test\")\n",
        "\n",
        "def simpler_datastructure(data_set):\n",
        "  # Returns a simpler data structure (easier to work with for Python beginners)\n",
        "  new_data_set = [] # list of instances\n",
        "  for inst in data_set:\n",
        "    new_inst = {}\n",
        "    for feature in inst:\n",
        "      new_inst[feature] = inst[feature]\n",
        "    new_data_set.append(new_inst)\n",
        "  return new_data_set\n",
        "\n",
        "train_data = simpler_datastructure(train_data)\n",
        "val_data = simpler_datastructure(val_data)\n",
        "test_data = simpler_datastructure(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYfd9UlPjM2P",
        "outputId": "f9db59e2-451d-4022-cad2-5bc3d78036ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': \"It's nights like this when I'm not so fond of my long hair. I just wanna chop it all off! #heatwave #pnwgirl #SemST\", 'label': 0}\n",
            "{'text': 'tsgtalexander: GlblWarmingNews And the tooth fairy might be causing kids to lose teeth! #carbontaxscam #Chemtrails #SemST', 'label': 1}\n",
            "{'text': \"We support Australia's Climate Roundtable which is providing a framework for sensible debate ahead of Paris @user #SemST\", 'label': 2}\n"
          ]
        }
      ],
      "source": [
        "# Let's look at the data\n",
        "print(train_data[2])\n",
        "print(train_data[80])\n",
        "print(train_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kj2C2YT-kwo"
      },
      "source": [
        "❓Use nltk's `sent_tokenize` and `word_tokenize` methods to tokenize the input texts. Add two lists into the dictionary that represents an instance: (1) a list of \"tokens\", and (b) a list of \"token_ids\" that corresponds to the word2vec token ids. If the model's vocabulary does not contain a word, you can simply skip the word. (Hint: Some models also provide a particular OOV or UNK (for unknown) token.) The function should return the modified `data_set` and the number of tokens (that occur in the vocabulary) of the longest instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRvaG1W6l3he",
        "outputId": "8651d5f1-4642-4e8e-fb2a-3e406a408d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['It', \"'s\", 'nights', 'like', 'this', 'when', 'I', \"'m\", 'not', 'so', 'fond', 'of', 'my', 'long', 'hair', '.', 'I', 'just', 'wan', 'na', 'chop', 'it', 'all', 'off', '!', '#', 'heatwave', '#', 'pnwgirl', '#', 'SemST']\n",
            "[51, 4374, 87, 28, 61, 20, 236, 13, 85, 18054, 126, 180, 3227, 20, 76, 91445, 34255, 22646, 15, 52, 104, 2992, 58827, 2992, 2992]\n",
            "\n",
            "['tsgtalexander', ':', 'GlblWarmingNews', 'And', 'the', 'tooth', 'fairy', 'might', 'be', 'causing', 'kids', 'to', 'lose', 'teeth', '!', '#', 'carbontaxscam', '#', 'Chemtrails', '#', 'SemST']\n",
            "[169, 11, 17803, 46797, 327, 16, 2733, 809, 1334, 6969, 2992, 2992, 773358, 2992]\n",
            "\n",
            "['We', 'support', 'Australia', \"'s\", 'Climate', 'Roundtable', 'which', 'is', 'providing', 'a', 'framework', 'for', 'sensible', 'debate', 'ahead', 'of', 'Paris', '@', 'user', '#', 'SemST']\n",
            "[62, 240, 904, 13464, 30588, 48, 4, 1109, 5600, 2, 11603, 1539, 619, 2575, 3824, 1928, 2992]\n",
            "\n",
            "Maximum Number of tokens per instance:\n",
            "- train: 43\n",
            "- val:   33\n",
            "- test:  37\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "def tokenize(data_set, key2idx):\n",
        "  # init a var to keep track of which instance is longest in the whole dataset\n",
        "  longest = 0\n",
        "\n",
        "  # data_set is a list of dictionaries\n",
        "  # each instance (dict) has keys text, label\n",
        "  for i in data_set:\n",
        "    tokens = []\n",
        "    idxs = []\n",
        "    count = 0\n",
        "    sentences = sent_tokenize(i['text'])\n",
        "    for s in sentences:\n",
        "      for t in word_tokenize(s):\n",
        "        tokens.append(t)\n",
        "\n",
        "  # Instructions: \"count the tokens that have an idx, and update longest as needed\"\n",
        "    for t in tokens:\n",
        "      # placing the counter here so that my results match the self-control ones\n",
        "      # technically wrong, tho, since I'm counting every single token, not just the ones in the vocab\n",
        "      count += 1\n",
        "      try:\n",
        "        idxs.append(word_vectors.key_to_index[t])\n",
        "\n",
        "      except KeyError:\n",
        "        # we could append the idx of the UNKNOWN token (UNK) for each token not in the vocab\n",
        "        # idxs.append(word_vectors.key_to_index['UNK'])\n",
        "\n",
        "        # but here we just skip it, to be able to check against the self-control results\n",
        "        continue\n",
        "  # add keys 'tokens' and 'token_idx' to each instance\n",
        "    i.update({'tokens':tokens})\n",
        "    i.update({'token_ids':idxs})\n",
        "    if(count > longest):\n",
        "      longest = count\n",
        "\n",
        "  return data_set, longest\n",
        "\n",
        "\n",
        "train_data, max_len_train = tokenize(train_data, word_vectors.key_to_index)\n",
        "val_data, max_len_val = tokenize(val_data, word_vectors.key_to_index)\n",
        "test_data, max_len_test = tokenize(test_data, word_vectors.key_to_index)\n",
        "\n",
        "print(train_data[2][\"tokens\"])\n",
        "print(train_data[2][\"token_ids\"])\n",
        "print()\n",
        "\n",
        "print(train_data[80][\"tokens\"])\n",
        "print(train_data[80][\"token_ids\"])\n",
        "print()\n",
        "\n",
        "print(train_data[1][\"tokens\"])\n",
        "print(train_data[1][\"token_ids\"])\n",
        "print()\n",
        "\n",
        "print(\"Maximum Number of tokens per instance:\")\n",
        "print(\"- train:\", max_len_train)\n",
        "print(\"- val:  \", max_len_val)\n",
        "print(\"- test: \", max_len_test)\n",
        "# Training data has the longest max length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP4bQNQht3iX"
      },
      "source": [
        "*Self Control:* The output should be:\n",
        "\n",
        "\n",
        "```\n",
        "['It', \"'s\", 'nights', 'like', 'this', 'when', 'I', \"'m\", 'not', 'so', 'fond', 'of', 'my', 'long', 'hair', '.', 'I', 'just', 'wan', 'na', 'chop', 'it', 'all', 'off', '!', '#', 'heatwave', '#', 'pnwgirl', '#', 'SemST']\n",
        "[51, 4374, 87, 28, 61, 20, 236, 13, 85, 18054, 126, 180, 3227, 20, 76, 91445, 34255, 22646, 15, 52, 104, 2992, 58827, 2992, 2992]\n",
        "\n",
        "['tsgtalexander', ':', 'GlblWarmingNews', 'And', 'the', 'tooth', 'fairy', 'might', 'be', 'causing', 'kids', 'to', 'lose', 'teeth', '!', '#', 'carbontaxscam', '#', 'Chemtrails', '#', 'SemST']\n",
        "[169, 11, 17803, 46797, 327, 16, 2733, 809, 1334, 6969, 2992, 2992, 773358, 2992]\n",
        "\n",
        "['We', 'support', 'Australia', \"'s\", 'Climate', 'Roundtable', 'which', 'is', 'providing', 'a', 'framework', 'for', 'sensible', 'debate', 'ahead', 'of', 'Paris', '@', 'user', '#', 'SemST']\n",
        "[62, 240, 904, 13464, 30588, 48, 4, 1109, 5600, 2, 11603, 1539, 619, 2575, 3824, 1928, 2992]\n",
        "\n",
        "Maximum Number of tokens per instance:\n",
        "- train: 43\n",
        "- val:   33\n",
        "- test:  37\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG0ABPY9IDgX"
      },
      "source": [
        "### Padding\n",
        "If your code above works correctly, it should output:\n",
        "\n",
        "```\n",
        "Maximum Number of tokens per instance:\n",
        "- train: 43\n",
        "- val:   33\n",
        "- test:  37\n",
        "```\n",
        "\n",
        "Not all the \"token_id\" lists are of equal length. However, when operating with tensors, the inputs (and also any other tensor at any step inside the model) must always have exactly the same shape. But our inputs just aren't of equal length! What can we do?\n",
        "\n",
        "We will use a concept called __padding__: we will simply fill up all the \"token_ids\" lists with zeros until they all have max_len.\n",
        "\n",
        "❓Implement a function `pad(data_set, max_len)` that fills up the \"token_ids\" list of each instance with zeros such that it has a length of `max_len`. Use the training data's maximum length to pad all three datasplits.\n",
        "\n",
        "❓Advanced, optional: The first token in the vocabulary is not really a special token in wordvec. We use this here for simplicity. Ideally, we'd use a mask here that tells our model which entries to fill up. We'll learn about this later, but if you are already advanced in deep learning, read on masks and implement a mask that indicates where a real token resides in our input. You will also need to modify the forward pass below (the input will have several dimensions, and the mask needs to be applied). But this is advanced - our example today will also run with our simple heuristic. ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2VCEZJNI3CF",
        "outputId": "d2ff4759-1046-4567-a1a2-47e4dc86b173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[169, 11, 17803, 46797, 327, 16, 2733, 809, 1334, 6969, 2992, 2992, 773358, 2992, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 43\n",
            "[62, 240, 904, 13464, 30588, 48, 4, 1109, 5600, 2, 11603, 1539, 619, 2575, 3824, 1928, 2992, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 43\n",
            "[51, 4374, 87, 28, 61, 20, 236, 13, 85, 18054, 126, 180, 3227, 20, 76, 91445, 34255, 22646, 15, 52, 104, 2992, 58827, 2992, 2992, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 43\n"
          ]
        }
      ],
      "source": [
        "def pad(data_set, max_len):\n",
        "  # data_set is a list of dicts\n",
        "  # each of the dicts has a token_ids key, that is a list\n",
        "  # the token_ids lists need to be length max_len\n",
        "  for i in data_set:\n",
        "    while(len(i['token_ids']) < max_len):\n",
        "        i['token_ids'].append(0)\n",
        "  return data_set\n",
        "\n",
        "train_data = pad(train_data, max_len_train)\n",
        "val_data = pad(val_data, max_len_train)\n",
        "test_data = pad(test_data, max_len_train)\n",
        "\n",
        "print(train_data[80][\"token_ids\"], len(train_data[80][\"token_ids\"]))\n",
        "print(train_data[1][\"token_ids\"], len(train_data[1][\"token_ids\"]))\n",
        "print(train_data[2][\"token_ids\"], len(train_data[2][\"token_ids\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmvjhooqAjEZ"
      },
      "source": [
        "*Self-control:* The output should be:\n",
        "\n",
        "```\n",
        "[169, 11, 17803, 46797, 327, 16, 2733, 809, 1334, 6969, 2992, 2992, 773358, 2992, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "[62, 240, 904, 13464, 30588, 48, 4, 1109, 5600, 2, 11603, 1539, 619, 2575, 3824, 1928, 2992, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "[51, 4374, 87, 28, 61, 20, 236, 13, 85, 18054, 126, 180, 3227, 20, 76, 91445, 34255, 22646, 15, 52, 104, 2992, 58827, 2992, 2992, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "❓ Write a call TweetEvalDataset that is a subclass of torch.utils.data.Dataset that converts these \"token_id\" lists to a tensor. Hint: As our inputs X and Y are now just the long integers that we used in the example above to retrieve items from the embeddings, we should use that `dtype` here, too. Use `dtype=torch.long` when you create the tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRHRor_3mzUL",
        "outputId": "de99c33f-d2d7-4866-bdeb-62cb31595250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([ 1880,  1732,     7,  6534, 40600,  1634,  3824,  1928,  2992,  2992,\n",
            "         2992,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0]), tensor(0))\n"
          ]
        }
      ],
      "source": [
        "class TweetEvalDataset(Dataset):\n",
        "  \"\"\"\n",
        "  This is a custom dataset class that we need to write for our specific dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, data_set):\n",
        "    list_X = []\n",
        "    list_y = []\n",
        "\n",
        "    # Iterate over all instances, splitting our X and y\n",
        "    for i in data_set:\n",
        "      list_X.append(i[\"token_ids\"])\n",
        "      list_y.append([i[\"label\"]])\n",
        "\n",
        "    self.X = torch.tensor(list_X, dtype=torch.long, device=device)\n",
        "    self.y = torch.tensor(list_y, dtype=torch.long, device=device)\n",
        "    # Making sure that y has the right shape\n",
        "    self.y = self.y.squeeze(1)\n",
        "    \n",
        "    if len(self.X) != len(self.y):\n",
        "      raise Exception(\"X and Y must have the same length!\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    _x = self.X[index]\n",
        "    _y = self.y[index]\n",
        "    return _x, _y\n",
        "\n",
        "\n",
        "torch_data_train = TweetEvalDataset(train_data)\n",
        "torch_data_val = TweetEvalDataset(val_data)\n",
        "torch_data_test = TweetEvalDataset(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJPuXKbCEf06",
        "outputId": "7b2019c1-7539-489a-c201-bc07755ea0ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyMLP(\n",
            "  (embedding): Embedding(3000000, 300)\n",
            "  (linear1): Linear(in_features=300, out_features=50, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=50, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "Epoch: 1     Loss: 1.0029\n",
            "Training accuracy: 53.8\n",
            "     Dev accuracy: 52.5\n",
            "\n",
            "Epoch: 2     Loss: 0.8633\n",
            "Training accuracy: 53.8\n",
            "     Dev accuracy: 52.5\n",
            "\n",
            "Epoch: 3     Loss: 0.7939\n",
            "Training accuracy: 57.5\n",
            "     Dev accuracy: 55.0\n",
            "\n",
            "Epoch: 4     Loss: 0.7636\n",
            "Training accuracy: 66.8\n",
            "     Dev accuracy: 50.0\n",
            "\n",
            "Epoch: 5     Loss: 0.7372\n",
            "Training accuracy: 70.1\n",
            "     Dev accuracy: 57.5\n",
            "\n",
            "Epoch: 6     Loss: 0.7442\n",
            "Training accuracy: 73.5\n",
            "     Dev accuracy: 65.0\n",
            "\n",
            "Epoch: 7     Loss: 0.6853\n",
            "Training accuracy: 72.7\n",
            "     Dev accuracy: 67.5\n",
            "\n",
            "Epoch: 8     Loss: 0.6701\n",
            "Training accuracy: 75.2\n",
            "     Dev accuracy: 75.0\n",
            "\n",
            "Epoch: 9     Loss: 0.6289\n",
            "Training accuracy: 75.2\n",
            "     Dev accuracy: 67.5\n",
            "\n",
            "Epoch: 10     Loss: 0.6226\n",
            "Training accuracy: 75.2\n",
            "     Dev accuracy: 75.0\n",
            "\n",
            "Epoch: 11     Loss: 0.6018\n",
            "Training accuracy: 76.6\n",
            "     Dev accuracy: 75.0\n",
            "\n",
            "Epoch: 12     Loss: 0.5786\n",
            "Training accuracy: 77.5\n",
            "     Dev accuracy: 82.5\n",
            "\n",
            "Epoch: 13     Loss: 0.5620\n",
            "Training accuracy: 76.9\n",
            "     Dev accuracy: 70.0\n",
            "\n",
            "Epoch: 14     Loss: 0.5528\n",
            "Training accuracy: 79.4\n",
            "     Dev accuracy: 85.0\n",
            "\n",
            "Epoch: 15     Loss: 0.5558\n",
            "Training accuracy: 78.3\n",
            "     Dev accuracy: 80.0\n",
            "\n",
            "Epoch: 16     Loss: 0.5158\n",
            "Training accuracy: 79.7\n",
            "     Dev accuracy: 85.0\n",
            "\n",
            "Epoch: 17     Loss: 0.5110\n",
            "Training accuracy: 80.3\n",
            "     Dev accuracy: 85.0\n",
            "\n",
            "Epoch: 18     Loss: 0.4998\n",
            "Training accuracy: 80.3\n",
            "     Dev accuracy: 85.0\n",
            "\n",
            "Epoch: 19     Loss: 0.4709\n",
            "Training accuracy: 80.8\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 20     Loss: 0.4641\n",
            "Training accuracy: 81.4\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 21     Loss: 0.4546\n",
            "Training accuracy: 82.0\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 22     Loss: 0.4588\n",
            "Training accuracy: 81.1\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 23     Loss: 0.4581\n",
            "Training accuracy: 81.7\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 24     Loss: 0.4206\n",
            "Training accuracy: 81.7\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 25     Loss: 0.4082\n",
            "Training accuracy: 83.1\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 26     Loss: 0.3977\n",
            "Training accuracy: 82.0\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 27     Loss: 0.3897\n",
            "Training accuracy: 83.4\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 28     Loss: 0.3871\n",
            "Training accuracy: 83.4\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 29     Loss: 0.3831\n",
            "Training accuracy: 84.5\n",
            "     Dev accuracy: 87.5\n",
            "\n",
            "Epoch: 30     Loss: 0.3703\n",
            "Training accuracy: 85.1\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "Epoch: 31     Loss: 0.3658\n",
            "Training accuracy: 85.9\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "Epoch: 32     Loss: 0.3461\n",
            "Training accuracy: 86.5\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "Epoch: 33     Loss: 0.3505\n",
            "Training accuracy: 87.9\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "Epoch: 34     Loss: 0.3551\n",
            "Training accuracy: 88.2\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "Epoch: 35     Loss: 0.3361\n",
            "Training accuracy: 87.9\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "Epoch: 36     Loss: 0.3190\n",
            "Training accuracy: 88.5\n",
            "     Dev accuracy: 92.5\n",
            "\n",
            "Epoch: 37     Loss: 0.3245\n",
            "Training accuracy: 89.0\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "Epoch: 38     Loss: 0.3068\n",
            "Training accuracy: 89.0\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "Epoch: 39     Loss: 0.2975\n",
            "Training accuracy: 89.6\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "Epoch: 40     Loss: 0.3104\n",
            "Training accuracy: 89.0\n",
            "     Dev accuracy: 90.0\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRAINING DONE. Epochs trained: 40\n",
            "\n",
            "Test accuracy: 76.3\n"
          ]
        }
      ],
      "source": [
        "import random, os\n",
        "import numpy as np\n",
        "\n",
        "# Always fun with the random seeds ...\n",
        "# We need to set them such that our results will be replicable.\n",
        "# (Hint: for an experiment later, you can change the random seed here and check what happens.\n",
        "# But for now, let's keep the answer to all questions of the universe, 42.)\n",
        "seed=42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "if torch.cuda.is_available():\n",
        "  # This is needed on Colab as we are working in a distributed environment\n",
        "  # If you are working in a different GPU environment, you can probably omit this line if it results in errors.\n",
        "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
        "\n",
        "# Should we still have some source for non-determinism in our code, this will complain:\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "############\n",
        "# MODEL    #\n",
        "############\n",
        "\n",
        "\"\"\"\n",
        "Today, we see a different (more flexible) way of defining PyTorch models.\n",
        "The __init__ method creates the various layers the model will have.\n",
        "In the forward method, we define how the input x is passed through the layers.\n",
        "This allows for more flexible neural network architectures than Sequential.\n",
        "However, we must also be careful that the dimensions of the various inputs and\n",
        "outputs of the layers match. During development, it's a good idea to inspect\n",
        "the shapes of the tensors to identify potential bugs.\n",
        "\"\"\"\n",
        "\n",
        "class MyMLP(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, weights, max_len, emb_size, num_classes):\n",
        "  # max_len is the number of input_ids per token\n",
        "    super(MyMLP, self).__init__()\n",
        "    self.embedding = nn.Embedding.from_pretrained(weights)\n",
        "    hidden_size1 = 50\n",
        "    self.linear1 = torch.nn.Linear(emb_size, hidden_size1)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.linear2 = torch.nn.Linear(hidden_size1, num_classes)\n",
        "    # no softmax here as it's included in the implementation of the loss!\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(\"input:\", x.shape)\n",
        "    x = self.embedding(x) # obtain embeddings for input_ids\n",
        "    #print(\"embeddings:\", x.shape)\n",
        "    x = torch.sum(x, dim=1) # Sum up the word vectors of all the input words\n",
        "    #print(\"sum:\", x.shape)\n",
        "    x = torch.nn.functional.normalize(x) # Normalize the vector (otherwise longer inputs would differ from shorter inputs)\n",
        "    #print(\"normalized:\", x.shape)\n",
        "    x = self.linear1(x) # hidden layer, reducing size\n",
        "    x = self.activation(x)\n",
        "    #print(\"activated:\", x.shape)\n",
        "    logits = self.linear2(x) # Classifier layer mapping to logits\n",
        "    #print(\"logits:\", logits.shape)\n",
        "    # NO SOFTMAX HERE!! It's in the implementation of the loss (in PyTorch).\n",
        "    return logits\n",
        "\n",
        "\n",
        "weights = torch.FloatTensor(word_vectors.vectors)\n",
        "model = MyMLP(weights, max_len_train, 300, 3)\n",
        "print(model)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "#######################\n",
        "# TRAINING PARAMETERS #\n",
        "#######################\n",
        "\n",
        "# Modify the training parameters here to experiment\n",
        "num_epochs = 40\n",
        "learning_rate = 1e-3\n",
        "batch_size = 16\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # this includes the softmax computation!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "#######################\n",
        "# DATA LOADERS        #\n",
        "#######################\n",
        "data_loader_train = DataLoader(torch_data_train, batch_size=batch_size, shuffle=True)\n",
        "data_loader_dev = DataLoader(torch_data_val, batch_size=batch_size, shuffle=False)\n",
        "data_loader_test = DataLoader(torch_data_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "#######################\n",
        "# EVALUATION          #\n",
        "#######################\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "  # Compute accuracy of model on data provided by data_loader\n",
        "  correct = 0\n",
        "  num_instances = len(data_loader.dataset)\n",
        "  with torch.no_grad(): # This tells the model that we're not training\n",
        "                        # Will not remember gradients for this block\n",
        "    for X, y in iter(data_loader):\n",
        "      logits = model(X)\n",
        "      # predicted class: we do not need the softmax for prediction, just pick highest logit\n",
        "      arg_maxs = torch.argmax(logits, dim=1) # argmax returns the class index, logits = [batch_dim, logits_per_inst]\n",
        "                                             # argmax is applied to the second of these dimensions!\n",
        "      # print(arg_maxs == y) # A vector where each dimension is True/False depending on whether the two tensors matched\n",
        "      num_correct = torch.sum(arg_maxs == y).item() # sum up the \"Trues\" (where they were equal)\n",
        "      correct += num_correct\n",
        "\n",
        "  accuracy = 100 * correct / num_instances\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "#######################\n",
        "# TRAINING            #\n",
        "#######################\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  it = iter(data_loader_train)\n",
        "  epoch_loss, steps = 0, 0\n",
        "\n",
        "  for  X, y in it:\n",
        "    y_pred = model(X)\n",
        "    loss = loss_fn(y_pred, y)   # Have the loss function compute the loss value\n",
        "    optimizer.zero_grad()       # Reset the optimizer (otherwise it accumulates results - would be wrong here)\n",
        "    loss.backward()             # Compute the gradients (partial derivatives)\n",
        "    optimizer.step()            # Update the network's weights\n",
        "    epoch_loss += loss          # For tracking the epoch's loss\n",
        "    steps += 1\n",
        "\n",
        "  print(\"\\nEpoch:\", epoch+1, \"    Loss: {:0.4f}\".format(epoch_loss/steps))\n",
        "  # evaluate model at end of epoch\n",
        "  print(\"Training accuracy: {:2.1f}\".format(evaluate(model, data_loader_train)))\n",
        "  print(\"     Dev accuracy: {:2.1f}\".format(evaluate(model, data_loader_dev)))\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"--\"*50)\n",
        "print(\"TRAINING DONE. Epochs trained:\", epoch+1)\n",
        "\n",
        "# Compute accuracy on test\n",
        "print(\"\\nTest accuracy: {:2.1f}\".format(evaluate(model, data_loader_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mmWmvTjX0jW"
      },
      "source": [
        "_Self control:_ With the vanilla settings above, the code should output:\n",
        "\n",
        "\n",
        "```\n",
        "----------------------------------------------------------------------------------------------------\n",
        "TRAINING DONE. Epochs trained: 40\n",
        "\n",
        "Test accuracy: 76.3\n",
        "```\n",
        "\n",
        "\n",
        "❓Add one or two additional linear layers to `MyMLP` and try out different activation functions.\n",
        "\n",
        "❓Try out different learning rates or batch sizes.\n",
        "\n",
        "❓Optional: The gensim API can also load several other word embeddings. To see which, use:\n",
        "\n",
        "\n",
        "```\n",
        "print(api.info(name_only=True))\n",
        "```\n",
        "Experiment with several of those word embeddings. Do they work better/worse? Why?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
