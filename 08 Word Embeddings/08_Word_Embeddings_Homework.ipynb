{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er_UDweQVueu"
   },
   "source": [
    "## Introduction to Natural Language Processing\n",
    "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
    "Prof. Dr. Annemarie Friedrich<br/>\n",
    "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
    "Date: **SS 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiC7ZTwtVvzE"
   },
   "source": [
    "# 8. Word Embeddings (Homework)\n",
    "\n",
    "**Learning Goals:**\n",
    "\n",
    "* Implement multi-class classification with a multi-layer perceptron\n",
    "* Understand PyTorch tensors\n",
    "* Load pre-trained word embeddings and use them in a neural network\n",
    "* Perform stance classification\n",
    "\n",
    "But first, some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SRlEyo-Vu5_"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.23.5  # gensim compatibility\n",
    "!pip install -U datasets\n",
    "!pip install nltk\n",
    "!pip install --upgrade gensim\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "import gensim.downloader as api\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CosineSimilarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt_tab\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Computing on:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0vdFyQDY7CE"
   },
   "source": [
    "## Loading Pre-trained Word Embeddings\n",
    "\n",
    "First, we will load pre-trained word embeddings via the [gensim](https://radimrehurek.com/gensim/) library. We use this library to avoid downloading the files to our local machines, but in theory, you can do that as well. You can find pre-trained word embeddings on many places in the web, e.g., [here](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/).\n",
    "\n",
    "The next code cell will download the 300-dimensional word2vec vectors pre-trained on Google News (be patient, this will take around 5-10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIuUScucaNLB"
   },
   "outputs": [],
   "source": [
    "word_vectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJHB4V2FbyXX"
   },
   "source": [
    "The `gensim` library provides several functionalities for querying the word embeddings (called _keyed vectors_). Check how to use the function `similarity` on [this page](https://radimrehurek.com/gensim/models/keyedvectors.html).\n",
    "\n",
    "❓Re-iterate the exercise for word similarity ratings on `wordsim_relatedness_goldstandard.txt`. Compute the system ratings for all word pairs using this function. How does your result compare to your earlier results using LCH and PMI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61hU2nP8bnAU"
   },
   "outputs": [],
   "source": [
    "def compute_correlation(human_ratings, system_ratings):\n",
    "  \"\"\" Input: two lists (of equal length) with numeric values.\n",
    "  Computes Pearson's correlation coefficient.\n",
    "  \"\"\"\n",
    "  assert len(human_ratings), len(system_ratings)\n",
    "  return scipy.stats.pearsonr(human_ratings, system_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcJJOgkyWazq"
   },
   "outputs": [],
   "source": [
    "# Compute similarities of word embeddings using cosine\n",
    "# Check the \"most similar words\", using the default \"cosine similarity\" measure.\n",
    "\n",
    "# A list of instances to read the /t-separated value strings into\n",
    "instances = []\n",
    "# A list of only the word pairs of the instances (useful later)\n",
    "wordPairs = []\n",
    "# A list of only the (float) scores of the instances (useful for min, max, mean)\n",
    "scores = []\n",
    "\n",
    "with open('/content/wordsim_relatedness_goldstandard.csv', newline='') as csvfile:\n",
    "    filereader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "\n",
    "\n",
    "    for line in filereader:\n",
    "        instances.append(line)\n",
    "        wordPairs.append((line[0], line[1]))\n",
    "        scores.append(float(line[2]))\n",
    "\n",
    "# A list of computed similarities\n",
    "sims = []\n",
    "\n",
    "for wp in wordPairs:\n",
    "        sim = word_vectors.similarity(wp[0], wp[1])\n",
    "        print(f\"{wp[0]} - {wp[1]} = {sim:.4f}\")\n",
    "        sims.append(sim)\n",
    "\n",
    "print(f\"{compute_correlation(scores, sims)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUqc0jaufUCy"
   },
   "source": [
    "If your implementation is correct, the output should be:\n",
    "\n",
    "```\n",
    "PearsonRResult(statistic=0.5920509820875375, pvalue=3.143384293094993e-25)\n",
    "```\n",
    "\n",
    "This correlation is pretty high!\n",
    "<br/>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tp_ZXqpScJw8"
   },
   "source": [
    "## PyTorch Tensors\n",
    "\n",
    "❓ In order to understand the following code and solve the following exercises, you need to have a basic understanding of PyTorch tensors. Work through the following tutorial and note down the most important facts about tensors here in this notebook (with code examples).\n",
    "\n",
    "[PyTorch.org Tutorial on Tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1FiAqgrccXZ"
   },
   "source": [
    "_Your text here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQp2FbuXcezK"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aphnoj8cooz"
   },
   "source": [
    "\n",
    "### Initializing a PyTorch Embedding Layer With Pretrained Embeddings\n",
    "\n",
    "The code in the next cell takes word2vec's weight matrix from gensim and initalizes an [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer in PyTorch. This is a layer that can be queried using indices and that returns the word embeddings (or rather, passes them on to the next layer).\n",
    "\n",
    "Word embeddings are of the first layer of a neural network and can either be \"frozen\" (i.e., we optimize only the rest of the model's parameters) or optimized further during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3i2Foa3fcvJ"
   },
   "outputs": [],
   "source": [
    "# Create a float tensor from the word vectors\n",
    "weights = torch.FloatTensor(word_vectors.vectors) # two-dimensional matrix. Rows = vocabulary items, columns = dimensions of word embedding.\n",
    "embedding = nn.Embedding.from_pretrained(weights)\n",
    "\n",
    "# Query embedding layer for a specific word embedding\n",
    "print(\"The 1000st word of the vocabulary:\", word_vectors.index_to_key[999])\n",
    "input = torch.LongTensor([5]) # the 1000st word in the vocabulary\n",
    "\n",
    "print(embedding(input).shape)\n",
    "# print(embedding(input)) # uncomment this to look at the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTjPaj0d5CM9"
   },
   "source": [
    "You may wonder about the extra dimension `1` above. Keep in mind that the embedding layer contains an embedding tensor (matrix) of size [vocab_size, 300] where 300 is the dimensionality of the word embeddings.\n",
    "When selecting a row from this (the vector for the word \"raised\" that we queried above), PyTorch keeps this extra dimension, it always provides for the possibility that we might want to retrieve more than one vector a time. For example, we could retrieve the two embeddings of two words, which are returned as new matrix with dimensions [2,300], i.e., the first dimension corresponds to the number of inputs.\n",
    "\n",
    "If you haven't checked it out yet, now is the time to work through [Towards DataScience: Understanding Dimensions in PyTorch](https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be) unless you are already familiar with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wee71YDf5BkH"
   },
   "outputs": [],
   "source": [
    "input = torch.LongTensor([77, 88])\n",
    "print(embedding(input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAknmjly8bWu"
   },
   "source": [
    "If we want to get rid of this extra dimension to have a vector that corresponds to a simple vector, we can use `squeeze`.\n",
    "(Note: we did that already in 07_Neural_Networks.ipynb when we removed the batch dimension. There, we collected inputs into batches, and the model returned the predictions similarly packed into batches. We used `squeeze` to create a simple vector from these probability scores predicted by the binary classifier, as our gold labels happened to have this format, and the loss function expected this format.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56k2SBZ58chG"
   },
   "outputs": [],
   "source": [
    "input = torch.LongTensor([5]) # the 1000st word in the vocabulary\n",
    "emb_5 = embedding(input)\n",
    "print(emb_5.shape) # this has an \"outer\" tensor dimension\n",
    "emb_5 = emb_5.squeeze(0) # removed the first dimension (~ the outer list)\n",
    "print(emb_5.shape) # this is just a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aInAfxku-uUl"
   },
   "source": [
    "Gensim's `word_vector` object provides two dictionaries: `key_to_index`, which returns the row corresponding to a word, and `index_to_key`, which maps a row index to the corresponding word. We use the latter to display some words of word2vec's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSzNNWK2_GV6"
   },
   "outputs": [],
   "source": [
    "vocab = \"\"\n",
    "for i in range(0,30):\n",
    "  vocab += word_vectors.index_to_key[i] + \" \"\n",
    "vocab += \"\\n\"\n",
    "for i in range(500,520):\n",
    "  vocab += word_vectors.index_to_key[i] + \" \"\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CClsdRGVzw33"
   },
   "source": [
    "From this, we can tell that vocabulary has not been lemmatized (e.g., \"doing\" and \"known\" are not lemmas, and \"The\" is uppercase, etc.). Hence, if we want to work with these embeddings, standard tokenization should do the trick. (❗This is not always the case, vocabularies differ in their preprocessing!)\n",
    "\n",
    "The `similarity` function of the gensim library that we have used above computes the cosine between the word vectors. We can compute cosine similarity using PyTorch as well. Let's do that in order to get a bit more familiar with tensors in PyTorch.\n",
    "\n",
    "❓The code below compares the similarity scores returned by the two frameworks for \"bike\" and \"car\". Add the comparisons \"motorbike-bike\" and \"motorbike-car\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suF7nl7WVkyS"
   },
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "idx_bike = word_vectors.key_to_index[\"bike\"]\n",
    "idx_car = word_vectors.key_to_index[\"car\"]\n",
    "\n",
    "emb_car = embedding(torch.LongTensor([idx_car]))\n",
    "emb_bike = embedding(torch.LongTensor([idx_bike]))\n",
    "\n",
    "print(\"car-bike:\", cos(emb_car, emb_bike), word_vectors.similarity(\"car\", \"bike\"))\n",
    "\n",
    "# Compare motorbike-bike and motorbike-car.\n",
    "idx_motorbike = word_vectors.key_to_index[\"motorbike\"]\n",
    "emb_motorbike = embedding(torch.LongTensor([idx_motorbike]))\n",
    "\n",
    "print(\"motorbike-bike:\", cos(emb_motorbike, emb_bike), word_vectors.similarity(\"motorbike\", \"bike\"))\n",
    "print(\"motorbike-car:\", cos(emb_motorbike, emb_car), word_vectors.similarity(\"motorbike\", \"car\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HogLcZFF13Lm"
   },
   "source": [
    "Same results! The difference is the data type: gensim returns simple floats, PyTorch returns tensors (even if the tensor has only one dimension and a single value here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp70NB5Xh3n1"
   },
   "source": [
    "### Load the TweetEval Dataset\n",
    "\n",
    "We will now work with the [TweetEval](https://huggingface.co/datasets/tweet_eval) dataset.\n",
    "Your task is to train a classifier using word embeddings for predicting the _stance_ of a tweet towards \"climate change\":\n",
    "\n",
    "* neutral (0)\n",
    "* against (1) - This label does not mean the person is against climate change, but does not believe that climate change is happening.\n",
    "* favor (2) - This label does not mean that the person is in favor of climate change, but believes that climate change is real and that we need to do something about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRrZq_gZis4g"
   },
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"tweet_eval\", \"stance_climate\", split=\"train\")\n",
    "val_data = load_dataset(\"tweet_eval\", \"stance_climate\", split=\"validation\")\n",
    "test_data = load_dataset(\"tweet_eval\", \"stance_climate\", split=\"test\")\n",
    "\n",
    "def simpler_datastructure(data_set):\n",
    "  # Returns a simpler data structure (easier to work with for Python beginners)\n",
    "  new_data_set = [] # list of instances\n",
    "  for inst in data_set:\n",
    "    new_inst = {}\n",
    "    for feature in inst:\n",
    "      new_inst[feature] = inst[feature]\n",
    "    new_data_set.append(new_inst)\n",
    "  return new_data_set\n",
    "\n",
    "train_data = simpler_datastructure(train_data)\n",
    "val_data = simpler_datastructure(val_data)\n",
    "test_data = simpler_datastructure(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYfd9UlPjM2P"
   },
   "outputs": [],
   "source": [
    "# Let's look at the data\n",
    "print(train_data[2])\n",
    "print(train_data[80])\n",
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kj2C2YT-kwo"
   },
   "source": [
    "❓Use nltk's `sent_tokenize` and `word_tokenize` methods to tokenize the input texts. Add two lists into the dictionary that represents an instance: (1) a list of \"tokens\", and (b) a list of \"token_ids\" that corresponds to the word2vec token ids. If the model's vocabulary does not contain a word, you can simply skip the word. (Hint: Some models also provide a particular OOV or UNK (for unknown) token.) The function should return the modified `data_set` and the number of tokens (that occur in the vocabulary) of the longest instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRvaG1W6l3he"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def tokenize(data_set, key2idx):\n",
    "  pass\n",
    "  # Your code here\n",
    "\n",
    "\n",
    "train_data, max_len_train = tokenize(train_data, word_vectors.key_to_index)\n",
    "val_data, max_len_val = tokenize(val_data, word_vectors.key_to_index)\n",
    "test_data, max_len_test = tokenize(test_data, word_vectors.key_to_index)\n",
    "\n",
    "print(train_data[2][\"tokens\"])\n",
    "print(train_data[2][\"token_ids\"])\n",
    "print()\n",
    "\n",
    "print(train_data[80][\"tokens\"])\n",
    "print(train_data[80][\"token_ids\"])\n",
    "print()\n",
    "\n",
    "print(train_data[1][\"tokens\"])\n",
    "print(train_data[1][\"token_ids\"])\n",
    "print()\n",
    "\n",
    "print(\"Maximum Number of tokens per instance:\")\n",
    "print(\"- train:\", max_len_train)\n",
    "print(\"- val:  \", max_len_val)\n",
    "print(\"- test: \", max_len_test)\n",
    "# Training data has the longest max length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP4bQNQht3iX"
   },
   "source": [
    "*Self Control:* The output should be:\n",
    "\n",
    "\n",
    "```\n",
    "['It', \"'s\", 'nights', 'like', 'this', 'when', 'I', \"'m\", 'not', 'so', 'fond', 'of', 'my', 'long', 'hair', '.', 'I', 'just', 'wan', 'na', 'chop', 'it', 'all', 'off', '!', '#', 'heatwave', '#', 'pnwgirl', '#', 'SemST']\n",
    "[51, 4374, 87, 28, 61, 20, 236, 13, 85, 18054, 126, 180, 3227, 20, 76, 91445, 34255, 22646, 15, 52, 104, 2992, 58827, 2992, 2992]\n",
    "\n",
    "['tsgtalexander', ':', 'GlblWarmingNews', 'And', 'the', 'tooth', 'fairy', 'might', 'be', 'causing', 'kids', 'to', 'lose', 'teeth', '!', '#', 'carbontaxscam', '#', 'Chemtrails', '#', 'SemST']\n",
    "[169, 11, 17803, 46797, 327, 16, 2733, 809, 1334, 6969, 2992, 2992, 773358, 2992]\n",
    "\n",
    "['We', 'support', 'Australia', \"'s\", 'Climate', 'Roundtable', 'which', 'is', 'providing', 'a', 'framework', 'for', 'sensible', 'debate', 'ahead', 'of', 'Paris', '@', 'user', '#', 'SemST']\n",
    "[62, 240, 904, 13464, 30588, 48, 4, 1109, 5600, 2, 11603, 1539, 619, 2575, 3824, 1928, 2992]\n",
    "\n",
    "Maximum Number of tokens per instance:\n",
    "- train: 43\n",
    "- val:   33\n",
    "- test:  37\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AG0ABPY9IDgX"
   },
   "source": [
    "### Padding\n",
    "If your code above works correctly, it should output:\n",
    "\n",
    "```\n",
    "Maximum Number of tokens per instance:\n",
    "- train: 43\n",
    "- val:   33\n",
    "- test:  37\n",
    "```\n",
    "\n",
    "Not all the \"token_id\" lists are of equal length. However, when operating with tensors, the inputs (and also any other tensor at any step inside the model) must always have exactly the same shape. But our inputs just aren't of equal length! What can we do?\n",
    "\n",
    "We will use a concept called __padding__: we will simply fill up all the \"token_ids\" lists with zeros until they all have max_len.\n",
    "\n",
    "❓Implement a function `pad(data_set, max_len)` that fills up the \"token_ids\" list of each instance with zeros such that it has a length of `max_len`. Use the training data's maximum length to pad all three datasplits.\n",
    "\n",
    "❓Advanced, optional: The first token in the vocabulary is not really a special token in wordvec. We use this here for simplicity. Ideally, we'd use a mask here that tells our model which entries to fill up. We'll learn about this later, but if you are already advanced in deep learning, read on masks and implement a mask that indicates where a real token resides in our input. You will also need to modify the forward pass below (the input will have several dimensions, and the mask needs to be applied). But this is advanced - our example today will also run with our simple heuristic. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2VCEZJNI3CF"
   },
   "outputs": [],
   "source": [
    "def pad(data_set, max_len):\n",
    "  # Your code here\n",
    "  pass\n",
    "\n",
    "\n",
    "train_data = pad(train_data, max_len_train)\n",
    "val_data = pad(val_data, max_len_train)\n",
    "test_data = pad(test_data, max_len_train)\n",
    "\n",
    "print(train_data[80][\"token_ids\"])\n",
    "print(train_data[1][\"token_ids\"])\n",
    "print(train_data[2][\"token_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmvjhooqAjEZ"
   },
   "source": [
    "*Self-control:* The output should be:\n",
    "\n",
    "```\n",
    "[169, 11, 17803, 46797, 327, 16, 2733, 809, 1334, 6969, 2992, 2992, 773358, 2992, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "[62, 240, 904, 13464, 30588, 48, 4, 1109, 5600, 2, 11603, 1539, 619, 2575, 3824, 1928, 2992, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "[51, 4374, 87, 28, 61, 20, 236, 13, 85, 18054, 126, 180, 3227, 20, 76, 91445, 34255, 22646, 15, 52, 104, 2992, 58827, 2992, 2992, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "❓ Write a call TweetEvalDataset that is a subclass of torch.utils.data.Dataset that converts these \"token_id\" lists to a tensor. Hint: As our inputs X and Y are now just the long integers that we used in the example above to retrieve items from the embeddings, we should use that `dtype` here, too. Use `dtype=torch.long` when you create the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRHRor_3mzUL"
   },
   "outputs": [],
   "source": [
    "class TweetEvalDataset(Dataset):\n",
    "  \"\"\"\n",
    "  This is a custom dataset class that we need to write for our specific dataset.\n",
    "  \"\"\"\n",
    "  def __init__(self, data_set):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "  def __len__(self):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # This returns an instance for a particular index.\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "torch_data_train = TweetEvalDataset(train_data)\n",
    "torch_data_val = TweetEvalDataset(val_data)\n",
    "torch_data_test = TweetEvalDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJPuXKbCEf06"
   },
   "outputs": [],
   "source": [
    "import random, os\n",
    "\n",
    "# Always fun with the random seeds ...\n",
    "# We need to set them such that our results will be replicable.\n",
    "# (Hint: for an experiment later, you can change the random seed here and check what happens.\n",
    "# But for now, let's keep the answer to all questions of the universe, 42.)\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "if torch.cuda.is_available():\n",
    "  # This is needed on Colab as we are working in a distributed environment\n",
    "  # If you are working in a different GPU environment, you can probably omit this line if it results in errors.\n",
    "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "\n",
    "# Should we still have some source for non-determinism in our code, this will complain:\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "############\n",
    "# MODEL    #\n",
    "############\n",
    "\n",
    "\"\"\"\n",
    "Today, we see a different (more flexible) way of defining PyTorch models.\n",
    "The __init__ method creates the various layers the model will have.\n",
    "In the forward method, we define how the input x is passed through the layers.\n",
    "This allows for more flexible neural network architectures than Sequential.\n",
    "However, we must also be careful that the dimensions of the various inputs and\n",
    "outputs of the layers match. During development, it's a good idea to inspect\n",
    "the shapes of the tensors to identify potential bugs.\n",
    "\"\"\"\n",
    "\n",
    "class MyMLP(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, weights, max_len, emb_size, num_classes):\n",
    "  # max_len is the number of input_ids per token\n",
    "    super(MyMLP, self).__init__()\n",
    "    self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "    hidden_size1 = 50\n",
    "    self.linear1 = torch.nn.Linear(emb_size, hidden_size1)\n",
    "    self.activation = torch.nn.ReLU()\n",
    "    self.linear2 = torch.nn.Linear(hidden_size1, num_classes)\n",
    "    # no softmax here as it's included in the implementation of the loss!\n",
    "\n",
    "  def forward(self, x):\n",
    "    #print(\"input:\", x.shape)\n",
    "    x = self.embedding(x) # obtain embeddings for input_ids\n",
    "    #print(\"embeddings:\", x.shape)\n",
    "    x = torch.sum(x, dim=1) # Sum up the word vectors of all the input words\n",
    "    #print(\"sum:\", x.shape)\n",
    "    x = torch.nn.functional.normalize(x) # Normalize the vector (otherwise longer inputs would differ from shorter inputs)\n",
    "    #print(\"normalized:\", x.shape)\n",
    "    x = self.linear1(x) # hidden layer, reducing size\n",
    "    x = self.activation(x)\n",
    "    #print(\"activated:\", x.shape)\n",
    "    logits = self.linear2(x) # Classifier layer mapping to logits\n",
    "    #print(\"logits:\", logits.shape)\n",
    "    # NO SOFTMAX HERE!! It's in the implementation of the loss (in PyTorch).\n",
    "    return logits\n",
    "\n",
    "\n",
    "weights = torch.FloatTensor(word_vectors.vectors)\n",
    "model = MyMLP(weights, max_len_train, 300, 3)\n",
    "print(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "#######################\n",
    "# TRAINING PARAMETERS #\n",
    "#######################\n",
    "\n",
    "# Modify the training parameters here to experiment\n",
    "num_epochs = 40\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # this includes the softmax computation!\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#######################\n",
    "# DATA LOADERS        #\n",
    "#######################\n",
    "data_loader_train = DataLoader(torch_data_train, batch_size=batch_size, shuffle=True)\n",
    "data_loader_dev = DataLoader(torch_data_val, batch_size=batch_size, shuffle=False)\n",
    "data_loader_test = DataLoader(torch_data_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#######################\n",
    "# EVALUATION          #\n",
    "#######################\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "  # Compute accuracy of model on data provided by data_loader\n",
    "  correct = 0\n",
    "  num_instances = len(data_loader.dataset)\n",
    "  with torch.no_grad(): # This tells the model that we're not training\n",
    "                        # Will not remember gradients for this block\n",
    "    for X, y in iter(data_loader):\n",
    "      logits = model(X)\n",
    "      # predicted class: we do not need the softmax for prediction, just pick highest logit\n",
    "      arg_maxs = torch.argmax(logits, dim=1) # argmax returns the class index, logits = [batch_dim, logits_per_inst]\n",
    "                                             # argmax is applied to the second of these dimensions!\n",
    "      # print(arg_maxs == y) # A vector where each dimension is True/False depending on whether the two tensors matched\n",
    "      num_correct = torch.sum(arg_maxs == y).item() # sum up the \"Trues\" (where they were equal)\n",
    "      correct += num_correct\n",
    "\n",
    "  accuracy = 100 * correct / num_instances\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "#######################\n",
    "# TRAINING            #\n",
    "#######################\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  it = iter(data_loader_train)\n",
    "  epoch_loss, steps = 0, 0\n",
    "\n",
    "  for  X, y in it:\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)   # Have the loss function compute the loss value\n",
    "    optimizer.zero_grad()       # Reset the optimizer (otherwise it accumulates results - would be wrong here)\n",
    "    loss.backward()             # Compute the gradients (partial derivatives)\n",
    "    optimizer.step()            # Update the network's weights\n",
    "    epoch_loss += loss          # For tracking the epoch's loss\n",
    "    steps += 1\n",
    "\n",
    "  print(\"\\nEpoch:\", epoch+1, \"    Loss: {:0.4f}\".format(epoch_loss/steps))\n",
    "  # evaluate model at end of epoch\n",
    "  print(\"Training accuracy: {:2.1f}\".format(evaluate(model, data_loader_train)))\n",
    "  print(\"     Dev accuracy: {:2.1f}\".format(evaluate(model, data_loader_dev)))\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"--\"*50)\n",
    "print(\"TRAINING DONE. Epochs trained:\", epoch+1)\n",
    "\n",
    "# Compute accuracy on test\n",
    "print(\"\\nTest accuracy: {:2.1f}\".format(evaluate(model, data_loader_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mmWmvTjX0jW"
   },
   "source": [
    "_Self control:_ With the vanilla settings above, the code should output:\n",
    "\n",
    "\n",
    "```\n",
    "----------------------------------------------------------------------------------------------------\n",
    "TRAINING DONE. Epochs trained: 40\n",
    "\n",
    "Test accuracy: 76.3\n",
    "```\n",
    "\n",
    "\n",
    "❓Add one or two additional linear layers to `MyMLP` and try out different activation functions.\n",
    "\n",
    "❓Try out different learning rates or batch sizes.\n",
    "\n",
    "❓Optional: The gensim API can also load several other word embeddings. To see which, use:\n",
    "\n",
    "\n",
    "```\n",
    "print(api.info(name_only=True))\n",
    "```\n",
    "Experiment with several of those word embeddings. Do they work better/worse? Why?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNlAK34nCoT3Pj8AF8yBWvo",
   "provenance": [
    {
     "file_id": "1ReKAOnSSWf5Nc0Z-h_s-Nf4SsgChul0G",
     "timestamp": 1688046703225
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
