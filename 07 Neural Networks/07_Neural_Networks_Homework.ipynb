{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBTTbTKkWkwS"
   },
   "source": [
    "## Introduction to Natural Language Processing\n",
    "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
    "Prof. Dr. Annemarie Friedrich<br/>\n",
    "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
    "Date: **SS 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tXM4HfOWmyg"
   },
   "source": [
    "# 7. Neural Networks\n",
    "\n",
    "__Learning Goals:__\n",
    "\n",
    "* Learn some basics of Pytorch.\n",
    "* Implement a multi-layer perceptron (MLP) in Pytorch.\n",
    "* Implement early stopping.\n",
    "* Experiment with the MLP on the SMS Spam dataset.\n",
    "\n",
    "<br/>\n",
    "\n",
    "In this notebook, we'll train a spam classifier using simple neural network.\n",
    "We will use the [SMS Spam dataset](https://huggingface.co/datasets/sms_spam) to train and evaluate our classifier.\n",
    "This dataset consists of text messages labeled with whether they are spam or not (1=SPAM, 0=NO_SPAM).\n",
    "\n",
    "```\n",
    "1 Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
    "\n",
    "0 U dun say so early hor... U c already then say...\n",
    "\n",
    "0 Nah I don't think he goes to usf, he lives around here though\n",
    "\n",
    "1 FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
    "```\n",
    "\n",
    "\n",
    "When running on Colab, choose a GPU runtime:\n",
    "\n",
    "```\n",
    "Runtime --> Change Runtime --> GPU --> T4\n",
    "```\n",
    "\n",
    "If you're working locally, this time, it will probably still work. The factor in speed-up of GPU vs. CPU when training (or evaluating) neural networks is about 10, though. The model in this notebook takes around 30 seconds to train with a GPU backend. If you're working on a CPU, that will be 300 seconds - this makes quite a difference in development time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coHSsGfPWlNz"
   },
   "outputs": [],
   "source": [
    "#  Imports\n",
    "!pip install -U datasets # HuggingFace datasets\n",
    "!pip install nltk\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pprint as pp\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGMDln-VzULy"
   },
   "source": [
    "### Dataset and Datasplits\n",
    "\n",
    "Let's load the dataset and create a train, dev, and test split. You know this procedure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tphjo0jSW8Fy"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "sms_spam = load_dataset(\"sms_spam\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rd5_uBQX-0i"
   },
   "outputs": [],
   "source": [
    "# Look at some instances\n",
    "for i in range(10):\n",
    "  label = sms_spam[\"label\"][i]\n",
    "  text = sms_spam[\"sms\"][i]\n",
    "  print(label, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFJKtWPsYDTU"
   },
   "outputs": [],
   "source": [
    "# Number of instances\n",
    "print(len(sms_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdaZWVa9ZIs4"
   },
   "outputs": [],
   "source": [
    "# Let's define a training, a development, and a test set\n",
    "#train_data, dev_data, test_data = {}, {}, {}\n",
    "train_data = {\"text\":sms_spam[\"sms\"][:2000]}\n",
    "train_data[\"label\"] = sms_spam[\"label\"][:2000]\n",
    "dev_data = {\"text\":sms_spam[\"sms\"][4000:5000]}\n",
    "dev_data[\"label\"] = sms_spam[\"label\"][4000:5000]\n",
    "test_data ={\"text\":sms_spam[\"sms\"][5000:]}\n",
    "test_data[\"label\"] = sms_spam[\"label\"][5000:]\n",
    "\n",
    "\n",
    "# Label distributions in these datasplits\n",
    "num_spam_train = train_data[\"label\"].count(1)\n",
    "print(\"% spam in train:\", round(100*num_spam_train/len(train_data[\"label\"]), 1))\n",
    "num_spam_dev = dev_data[\"label\"].count(1)\n",
    "print(\"% spam in dev:\", round(100*num_spam_dev/len(dev_data[\"label\"]), 1))\n",
    "num_spam_test = test_data[\"label\"].count(1)\n",
    "print(\"% spam in test:\", round(100*num_spam_test/len(test_data[\"label\"]), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSYASWhzzMIw"
   },
   "source": [
    "### Vocabulary and Text \"Embeddings\"\n",
    "\n",
    "We will represent each instance (text message) by its word counts, i.e., each dimension in the input vector corresponds to one word (token). As using the full vocabulary would result in too many dimensions, we will filter it first.\n",
    "A numeric vector representation of a text is also called an __embedding__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJqqeALjZ6oQ"
   },
   "outputs": [],
   "source": [
    "# Build vocabulary from training data\n",
    "# We'll collect the most frequent tokens separately for each class (question: why does that make sense?)\n",
    "complete_vocab = set()\n",
    "for label in [0, 1]:\n",
    "  vocab = defaultdict(int) # collect counts so we can filter the vocabulary\n",
    "  for i, text in enumerate(train_data[\"text\"]):\n",
    "    if label == train_data[\"label\"][i]:\n",
    "      for sent in sent_tokenize(text):\n",
    "        for token in word_tokenize(sent):\n",
    "          vocab[token] += 1\n",
    "  # Sorty by frequency\n",
    "  vocab_list = sorted([(v, k) for k, v in vocab.items()], reverse=True)\n",
    "  print(len(vocab_list))\n",
    "  print(vocab_list[:10])\n",
    "  # Add the 50 most frequent words to the complete vocabulary (hint: experiment with larger vocabulary sizes!)\n",
    "  complete_vocab.update(set([v for k, v in vocab_list[:50]])) # update is the a.union(b) of two sets, adds the items of b into a\n",
    "\n",
    "print(len(complete_vocab))\n",
    "vocab = sorted(complete_vocab) # this order defines the dimensions of our input vectors\n",
    "vocab2idx = {v:i for i, v in enumerate(vocab)} # obtain dimension of a word\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQVg9pY6jMHg"
   },
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "def tokenize_data(data_set):\n",
    "  data_set[\"tokens\"] = []\n",
    "  for text in data_set[\"text\"]:\n",
    "    tokens = []\n",
    "    for sent in sent_tokenize(text):\n",
    "      tokens += list(word_tokenize(sent))\n",
    "    data_set[\"tokens\"].append(tokens)\n",
    "\n",
    "\n",
    "tokenize_data(train_data)\n",
    "tokenize_data(dev_data)\n",
    "tokenize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcxpDfCfdQvG"
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data_set, vocab2idx):\n",
    "  count = 0\n",
    "  data_x = []\n",
    "  vocab_size = len(vocab2idx.keys())\n",
    "  data_set[\"x\"] = []\n",
    "  data_set[\"x_tf\"] = []\n",
    "  for tokens in data_set[\"tokens\"]:\n",
    "    x = np.zeros(vocab_size)\n",
    "    for token in tokens:\n",
    "      if token in vocab2idx:\n",
    "        x[vocab2idx[token]] += 1\n",
    "    if np.sum(x) == 0:\n",
    "      count+= 1\n",
    "\n",
    "    # numpy vector\n",
    "    data_set[\"x\"].append(x) # appending a numpy vector\n",
    "\n",
    "  print(count) # this many instances are not representable - just 0s in the vector\n",
    "\n",
    "\n",
    "vectorize_data(train_data, vocab2idx)\n",
    "vectorize_data(dev_data, vocab2idx)\n",
    "vectorize_data(test_data, vocab2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZxzZqFUlQsd"
   },
   "source": [
    "## Multi-Layer Perceptron in PyTorch\n",
    "\n",
    "Today, we will look at a code example of how to implement a multi-layer perceptron in [PyTorch](https://pytorch.org/), a deep learning library for Python that will help us to run our models on a GPU and that provides lots of pre-defined functionalities, e.g., neural network layers, optimizers, and loss functions.\n",
    "\n",
    "### Dataset\n",
    "First, we will map out SMS Spam dataset to a [torch.util.data.Dataset](https://pytorch.org/docs/stable/data.html) object. For doing so, we first need to define a class (here called `SmsSpanDataset`) that is a subclass of `Dataset`.\n",
    "This class must implement two methods: First, `__init__` for putting the x and y values in two lists (of equal length and in the same order, of course). This method will be called exactly once: when an object of this type is created.\n",
    "The second method, `__getitem__` is a called whenever the object of type `SmsSpanDataset` is asked to return a tuple corresponding to a single x and a single y value.\n",
    "\n",
    "Note: `__getitem__` is a magic method in Python that will be called when we access an object with an index: `train_data[i]` will call this method with `index=i`.\n",
    "\n",
    "The most important part happens in these lines:\n",
    "```\n",
    "self.X = [torch.tensor(x, dtype=torch.float32) for x in data_set[\"x\"]]\n",
    "self.Y = [torch.tensor(y, dtype=torch.float32) for y in data_set[\"label\"]]\n",
    "```\n",
    "Here, we map each input vector `x` to an input vector object that torch can use on the GPU. In PyTorch, vectors, matrices, and higher-dimensional tensors are all modelled using the [`torch.tensor`](https://pytorch.org/docs/stable/tensors.html) datatype. The `dtype` parameter tells PyTorch what kind of values we want to store in this tensor, here, it's 32-bit floating point values.\n",
    "\n",
    "If you are new to PyTorch and tensors, I recommend this excellent explanation:\n",
    "[Towards DataScience: Understanding Dimensions in PyTorch](https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be)\n",
    "\n",
    "Next, if you have access to a GPU, let's use it! (This notebook will still run quickly on the CPU as we use only very few features. But in the future, you'll appreciate a GPU. So let's learn how to do that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zns4GIKbKVoI"
   },
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available() # checks if a GPU is available\n",
    "\n",
    "# Set the device variable: quite handy for the code below.\n",
    "# The device variable will either point to the CPU or to the GPU.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(\"We will compute on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrMgSrd9Gg0x"
   },
   "outputs": [],
   "source": [
    "############\n",
    "# DATASET  #\n",
    "############\n",
    "\n",
    "class SmsSpanDataset(Dataset):\n",
    "  \"\"\"\n",
    "  This is a custom dataset class that we need to write for our specific dataset.\n",
    "  \"\"\"\n",
    "  def __init__(self, data_set):\n",
    "    # Note the device=device assignment for the tensors. It will create the tensors\n",
    "    # on the device that we configured above.\n",
    "    self.X = [torch.tensor(x, dtype=torch.float32, device=device) for x in data_set[\"x\"]]\n",
    "    self.Y = [torch.tensor(y, dtype=torch.float32, device=device) for y in data_set[\"label\"]]\n",
    "    if len(self.X) != len(self.Y):\n",
    "      raise Exception(\"X and Y must have the same length!\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # This returns an instance for a particular index.\n",
    "    _x = self.X[index]\n",
    "    _y = self.Y[index]\n",
    "\n",
    "    return _x, _y\n",
    "\n",
    "torch_data_train = SmsSpanDataset(train_data)\n",
    "torch_data_dev = SmsSpanDataset(dev_data)\n",
    "torch_data_test = SmsSpanDataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0Ssga8lGiM9"
   },
   "source": [
    "### Model\n",
    "\n",
    "Next, we will define our model. Our model is a simple sequence of layers, hence, we'll use [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).\n",
    "We define the layers of our model just in the order in that they occur:\n",
    "\n",
    "* An input layer taking in vectors of size `vocab_size` (as we defined them above) and mapping them to a hidden layer of size 2\n",
    "* A ReLU activation applied element-wise to the output of the previous layer\n",
    "* A second linear layer (also often called the \"classifier layer\"), which takes the output of the ReLU layer (here 2 dimensions) and maps it to a single logit. This is our score $z$!\n",
    "* A sigmoid layer computing the probability for $z$. This layer also takes the output of its preceding layer as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sxh5jLS6ILcw"
   },
   "outputs": [],
   "source": [
    "############\n",
    "# MODEL    #\n",
    "############\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(vocab_size, 2), # maps inputs to layer of hidden size 2\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2, 1),          # maps inputs of size 2 to a single activation value (z, aka logit)\n",
    "    nn.Sigmoid()              # maps value to a number between 0 and 1 (\"probability\"/\"confidence\")\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zihTJDv2IIxP"
   },
   "outputs": [],
   "source": [
    "# Is our model on the CPU or on the GPU?\n",
    "print(\"CUDA available?\", cuda_available)\n",
    "print(\"Model on GPU?\", next(model.parameters()).device)\n",
    "# Models are first created on the CPU.\n",
    "\n",
    "model = model.to(device)  # Move the model to the GPU (or keep in to the CPU).\n",
    "print(\"And now, model on GPU?\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPZgvp3ClT0l"
   },
   "outputs": [],
   "source": [
    "# Always fun with the random seeds ...\n",
    "# We need to set them such that our results will be replicable.\n",
    "# (Hint: for an experiment later, you can change the random seed here and check what happens.\n",
    "# But for now, let's keep the answer to all questions of the universe, 42.)\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "if cuda_available:\n",
    "  # This is needed on Colab as we are working in a distributed environment\n",
    "  # If you are working in a different GPU environment, you can probably omit this line if it results in errors.\n",
    "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "\n",
    "# Should we still have some source for non-determinism in our code, this will complain:\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "############\n",
    "# MODEL    #\n",
    "############\n",
    "\n",
    "# I am repeating the model code here such that it gets set up properly before each\n",
    "# training run (otherwise, we'd keep training the same parameters). Modify the model here.\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(vocab_size, 2), # maps inputs to layer of hidden size 2\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2, 1),          # maps inputs of size 2 to a single activation value (z, aka logit)\n",
    "    nn.Sigmoid()              # maps value to a number between 0 and 1 (\"probability\"/\"confidence\")\n",
    ")\n",
    "print(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "#######################\n",
    "# TRAINING PARAMETERS #\n",
    "#######################\n",
    "\n",
    "# Modify the training parameters here to experiment\n",
    "num_epochs = 35\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "loss_fn = nn.BCELoss() # Binary cross entropy loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#######################\n",
    "# DATA LOADERS        #\n",
    "#######################\n",
    "\"\"\"\n",
    "In Pytorch, the DataLoader takes a Dataset and creates batches from it.\n",
    "We then create an iterator over this dataset (at this time, it is shuffled!).\n",
    "We can obtain one batch at a time by calling next() on the iterator.\n",
    "Or, as below, we can directly use this iterator in for loop.\n",
    "The Dataloader returns matching X and y values, but not just one pair. It returns\n",
    "a tuple of tensors (imagine it like a list in this case), where the first\n",
    "entry is a tensor of size batch_size * [dimensions of x], and the second is\n",
    "batch_size * [dimensions of y].\n",
    "The advantage over a list is that we immediately get a multi-dimensional tensor\n",
    "object on which the GPU can perform very fast operations.\n",
    "\"\"\"\n",
    "\n",
    "# We should always randomly shuffle the training dataset for each epoch.\n",
    "# Don't worry, we fixed the random seeds above.\n",
    "data_loader_train = DataLoader(torch_data_train, batch_size=batch_size, shuffle=True)\n",
    "data_loader_dev = DataLoader(torch_data_dev, batch_size=batch_size, shuffle=False)\n",
    "data_loader_test = DataLoader(torch_data_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#######################\n",
    "# EVALUATION          #\n",
    "#######################\n",
    "\n",
    "# Let's define a very simple evaluation method that computes accuracy\n",
    "# Hint: it may be easier to read the training code below first, and then\n",
    "# come back here.\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "  # Compute accuracy of model on data provided by data_loader\n",
    "  correct = 0\n",
    "  num_instances = len(data_loader.dataset)\n",
    "  with torch.no_grad(): # This tells the model that we're not training\n",
    "                        # Will not remember gradients for this block\n",
    "    for X, y in iter(data_loader):\n",
    "      y_probs = model(X) # make prediction\n",
    "      y_probs = y_probs.squeeze(1) # removes the batch dimension\n",
    "      y_pred = torch.where(y_probs >= 0.5, 1, 0.) # creates new vector based on the condition statement\n",
    "      correct += (y_pred == y).float().sum() # count how many predictions were correct\n",
    "\n",
    "  accuracy = 100 * correct / num_instances\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "#######################\n",
    "# TRAINING            #\n",
    "#######################\n",
    "\n",
    "# Early Stopping: \n",
    "epochs_no_change = 1  # A counter for epochs gone without change,\n",
    "prev_dev_acc = 0      # an acc variable for storing the last accuracy reached on the dev set,\n",
    "k = 5                 # and a parameter k for setting the patience\n",
    "\n",
    "# Here, the training happens!\n",
    "for epoch in range(num_epochs): # One epoch = step once over the entire training dataset\n",
    "  it = iter(data_loader_train)  # Create the iterator from the training dataset\n",
    "  epoch_loss, steps = 0, 0      # To keep track of the current epoch's loss\n",
    "\n",
    "  for  X, y in it:              # Obtain a tensor X = batch of X-values, y accordingly\n",
    "    y_pred = model(X)           # Have our model with current weights make a prediction\n",
    "    y_pred = y_pred.squeeze(1)  # Removes the extra batch dimension (trick, more on this later)\n",
    "    loss = loss_fn(y_pred, y)   # Have the loss function compute the loss value\n",
    "    optimizer.zero_grad()       # Reset the optimizer (otherwise it accumulates results - would be wrong here)\n",
    "    loss.backward()             # Compute the gradients (partial derivatives)\n",
    "    optimizer.step()            # Update the network's weights\n",
    "    epoch_loss += loss          # For tracking the epoch's loss\n",
    "    steps += 1\n",
    "\n",
    "  print(\"\\nEpoch:\", epoch+1, \"    Loss: {:0.4f}\".format(epoch_loss/steps))\n",
    "  # evaluate model at end of epoch\n",
    "  print(\"Training accuracy: {:2.1f}\".format(evaluate(model, data_loader_train)))\n",
    "  dev_acc = evaluate(model, data_loader_dev)\n",
    "  print(\"Dev accuracy: {:2.1f}\".format(dev_acc))\n",
    "\n",
    "  # Early stopping\n",
    "  if dev_acc == prev_dev_acc:\n",
    "    epochs_no_change += 1\n",
    "    print(f\"Epochs no change: {epochs_no_change}\")\n",
    "  else:\n",
    "    epochs_no_change = 1\n",
    "\n",
    "  if epochs_no_change >= k:\n",
    "    print(f\"No change in accuracy for dev set in the last {k} epochs.\")\n",
    "    print(\"Accuracy(dev) = {:2.1f}.\".format(dev_acc))\n",
    "    print(f\"Stopping early.\")\n",
    "    break\n",
    "\n",
    "  prev_dev_acc = dev_acc\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"--\"*50)\n",
    "print(\"TRAINING DONE. Epochs trained:\", epoch+1)\n",
    "\n",
    "# Compute accuracy on test\n",
    "print(\"\\nTest accuracy: {:2.1f}\".format(evaluate(model, data_loader_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hwcdkwr6WW5c"
   },
   "source": [
    "__Coding Task:__\n",
    "\n",
    "âCheck above after which epoch the training actually ends (defined as no improvements on the development set). The training loss keeps going down: we are overfitting to the training set! Implement _early stopping_: Track the changes of the accuracy on the development set. If this accuracy has not increased for 5 epochs, stop the training. (We call this a _patience_ or _tolerance_ of 5.)\n",
    "\n",
    "__Suggested Experiments:__\n",
    "\n",
    "âExperiment with the hyperparameters: `batch_size`, `num_epochs`, `learning_rate`. Note down your findings. When do results get better/worse? When does learning get faster/slowlier?\n",
    "\n",
    "âMake the hidden layer size (above: 2) bigger. Important: you need to change the output size of the first linear layer and the input size of the second linear layer! Increase the number of epochs. Are your results getting better?\n",
    "\n",
    "âTry a different optimizier: `optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)` What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeoP-ZPJzIWR"
   },
   "source": [
    "Great job! Are you wondering now why commercial spam filters are often so bad? ð"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPv/rGR01QUMngRaruIi9QV",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "13ni0sMx4e9qEHK-sAeaLqF6Ci72I09e1",
     "timestamp": 1687727169094
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
