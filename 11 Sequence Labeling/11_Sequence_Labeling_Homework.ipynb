{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWTaayb9q5ns"
      },
      "source": [
        "## Introduction to Natural Language Processing\n",
        "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
        "Prof. Dr. Annemarie Friedrich<br/>\n",
        "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
        "Date: **SS 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5w2Jlq6q7Ri"
      },
      "source": [
        "# 11. Sequence Labeling (Homework)\n",
        "\n",
        "__Recommendation:__ Use a GPU for notebook, e.g., in Google Colab Runtime --> Change Runtime --> GPU --> T4.\n",
        "\n",
        "**Learning Goals**\n",
        "\n",
        "* Explain part-of-speech tagging\n",
        "* Explain named entity recognition\n",
        "* Implement masking in PyTorch\n",
        "* Train and evaluate a sequence labeling model\n",
        "\n",
        "❗ Upon completion, upload your code (this notebook) to your GitLab repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiPVhD0Aq4aH"
      },
      "outputs": [],
      "source": [
        "# Installations\n",
        "!pip install -U datasets\n",
        "!pip install transformers\n",
        "!pip install seqeval\n",
        "!pip install evaluate\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import evaluate\n",
        "from torch import optim\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the device we'll use for tensor computations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Computing on:\", device)\n",
        "\n",
        "# Should we still have some source for non-determinism in our code, this will complain:\n",
        "torch.use_deterministic_algorithms(True, warn_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrYnIXjLCYM"
      },
      "source": [
        "## Named Entity Recognition\n",
        "\n",
        "In this homework, we will work with the CoNLL 2003 Named Entity dataset. The dataset was developed for a shared task described in [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition (Tjong Kim Sang & De Meulder, CoNLL 2003)](https://aclanthology.org/W03-0419/).\n",
        "\n",
        "❓ Check out the [Dataset card of the CoNLL 2003 dataset](https://huggingface.co/datasets/conll2003) on HuggingFace models.\n",
        "\n",
        "❓ If you are still unsure how the BIO scheme works, now is the time to do a brief web search and figure it out! Advanced: The BILOU variant is currently achieving state-of-the-art results. Read about it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuYy33miX-Vo"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "train_data = load_dataset(\"conll2003\", split=\"train\")\n",
        "val_data =  load_dataset(\"conll2003\", split=\"validation\")\n",
        "test_data =  load_dataset(\"conll2003\", split=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ3p6c7-S7_a"
      },
      "source": [
        "❓ Print out the number of instances in each datasplit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAn_fhB8TCuc"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40UkPgRKSRzA"
      },
      "source": [
        "The NER tags in the CoNLL 2003 NER dataset are (with their class indices):\n",
        "\n",
        "```\n",
        "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
        "```\n",
        "\n",
        "❓ Create a list `ner_tags` of the labels in the order indicated by their values in the dictionary above. Create two dictionaries `tag2idx` and `idx2tag` that map from tag to class index and from class index to tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0z6QwVVSvQi"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLGY_VynEMC"
      },
      "source": [
        "❓ LOOK AT THE DATA. Print out a few examples and make sure you understand the data structure.\n",
        "\n",
        "Example Output (training set instance with index 189):\n",
        "```\n",
        "Iraqi           B-MISC\n",
        "President       O    \n",
        "Saddam          B-PER\n",
        "Hussein         I-PER\n",
        "has             O    \n",
        "told            O    \n",
        "visiting        O    \n",
        "Russian         B-MISC\n",
        "ultra-nationalist O    \n",
        "Vladimir        B-PER\n",
        "Zhirinovsky     I-PER\n",
        "that            O    \n",
        "Baghdad         B-LOC\n",
        "wanted          O    \n",
        "to              O    \n",
        "maintain        O    \n",
        "\"               O    \n",
        "friendship      O    \n",
        "and             O    \n",
        "cooperation     O    \n",
        "\"               O    \n",
        "with            O    \n",
        "Moscow          B-LOC\n",
        ",               O    \n",
        "official        O    \n",
        "Iraqi           B-MISC\n",
        "newspapers      O    \n",
        "said            O    \n",
        "on              O    \n",
        "Thursday        O    \n",
        ".               O    \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqaJCAJSnhHS"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSmRwid1pOSk"
      },
      "source": [
        "\n",
        "### Tokenization\n",
        "\n",
        "For tokenization, we use the [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto) class of the HuggingFace transformers library. It conveniently instantiates an object of the correct class depending on the model configured in the path.\n",
        "The parameter setting `add_special_tokens=True` means that the `[CLS]` and the `[SEP]` tokens are added to the input. Even if we are not performing sentence classification or sentence pair classification, we have to add these tokens because BERT saw them during pre-training, and now expects to see them, too.\n",
        "\n",
        "To save some computational resources, we will work with [TinyBERT](https://arxiv.org/abs/1909.10351) using the [model](https://huggingface.co/prajjwal1/bert-tiny) provided by [Bhargava et al., 2021](https://aclanthology.org/2021.insights-1.18/). TinyBERT has been trained using distillation to mimick the behavior of BERT, but it is a much smaller and more efficient model.\n",
        "\n",
        "If `tokenizer.is_fast` is `True`, we are using a [Fast tokenizer](https://huggingface.co/learn/nlp-course/chapter6/3) that provides some special features that are quite handy when working with pre-trained models.\n",
        "\n",
        "* As input, a fast tokenizer accepts either a string (a single input instance), a list of strings (input texts), or a list of a list of tokens. The latter is particularly helpful if our data is already pretokenized. In the CoNLL 2003 dataset, this is the case. (Note that we will use a gold standard tokenization for the \"real\" tokens. In a real-world setting with an automatic tokenizer, performance might be less. However, as long as we note this in our experimental section as a caveat, doing this is accepted nowadays because tokenizers work pretty well for many domains and genres.)\n",
        "\n",
        "* If given more than one instance, the fast tokenizer automatically process the inputs in a batch, being MUCH fast than when processing individual instances. (Hint (optional exercise): Modify the code below such that it only processes one sentence at a time and see how much slower it gets.)\n",
        "\n",
        "* The tokenizer accepts a wide range of useful parameter settings. The `return_tensors='pt'` options means that we want to get back PyTorch tensors. We add special tokens and we also want to get attention masks. Finally, we tell the tokenizer that the input is already split into words such that it does not attempt to create a tokenization that would contradict the given tokenization, which would be problematic if we want to map the outputs to the gold standard labels. Can you figure out what the `truncation` and the `padding` options do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj1GHyxrTw1X"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\") # Load the tokenizer that comes with the TinyBERT model.\n",
        "print(\"Is fast encoder (should be True):\", tokenizer.is_fast)\n",
        "\n",
        "input_examples = [[\"I\", \"visited\", \"the\", \"Augsburger\", \"Puppenkiste\", \".\"], \\\n",
        "                  [\"The\", \"Rathaus\", \"is\", \"much\", \"more\", \"interesting\", \"in\", \"my\", \"opinion\", \".\"]]\n",
        "encodings = tokenizer(input_examples, return_tensors='pt', add_special_tokens=True, \\\n",
        "                      return_attention_mask=True, is_split_into_words=True, \\\n",
        "                      truncation=True, padding=True, max_length=32)\n",
        "\n",
        "print(type(encodings))\n",
        "print(encodings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfyqqyFTSOT2"
      },
      "source": [
        "Inspect the data structures returned by the tokenizer. They are of type [`BatchEncoding`](https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/tokenizer#transformers.BatchEncoding).\n",
        "The maximum length of the sequence has been determined based on the maximum input length of the training set / maximum model input size / the value of max_length that we defined.\n",
        "\n",
        "The BatchEncoding object can be viewed as a dictionary where each entry contains a tensor that contains the data for one input sequence per row, i.e., the `input_ids`, `attention_mask` etc. of each instance are split across these tensors. However, the same row in these tensors always corresponds to the same instance.\n",
        "\n",
        "You can print them out as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggxst3X_SNgu"
      },
      "outputs": [],
      "source": [
        "num = 0 # Change to 1 and check the outputs.\n",
        "print(encodings[\"input_ids\"][num])\n",
        "print(encodings[\"attention_mask\"][num])\n",
        "print(encodings.tokens(num)) # This prints the word piece tokens (input_ids --> convert_to_token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ff92MkDWzDK"
      },
      "source": [
        "Now, let's assume that our gold standard labels are as follows:\n",
        "\n",
        "```\n",
        "I           O\n",
        "visited     O\n",
        "the         O\n",
        "Augsburger  B-ORG\n",
        "Puppenkiste I-ORG\n",
        ".           O\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02IhkGmoZQGT"
      },
      "source": [
        "At this point, we have a problem: The original labels above relate to the \"real\" tokens, but the word-piece tokens have split up some of them into several tokens. Recall that we have only labels for \"real\" tokens. There are two possible strategies to address this:\n",
        "\n",
        "1. Use the first subword token of a \"real\" token to represent the embedding, only compare the predicted label for this token to the gold standard token and back-propagate the loss accordingly only from these tokens. This can be achieved by setting the labels of all non-used subword tokens to `-100`, a magic label index ignored by PyTorch loss functions (i.e., no loss is backpropagated from these outputs).\n",
        "\n",
        "2. Duplicate the label of the \"real\" token for all its subword tokens.\n",
        "\n",
        "Today, we will implement version 1. Luckily, the `BatchEncoding` object has a method called `word_ids` which returns a list showing to which \"real\" token a subword token corresponds. Each entry in this list corresponds to the subword token at the same position. The numbers in this list indicate the position of the corresponding \"real\" token in the original sequence.\n",
        "\n",
        "❓ Make sure you understand the output of the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5Io3N0NOmFb"
      },
      "outputs": [],
      "source": [
        "num = 0 # change to 1 to inspect data\n",
        "print([(i, w) for i, w in enumerate(input_examples[num])])\n",
        "print(encodings.tokens(num))\n",
        "word_ids = encodings.word_ids(num)\n",
        "print(word_ids)\n",
        "print([(orig_word_id, subword_token) for orig_word_id, subword_token in zip(word_ids, encodings.tokens(num))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgf320MHYIwf"
      },
      "source": [
        "### Perform Tokenization and Creating Label Tensors\n",
        "\n",
        "❓ Write a function that tokenizes the text of each input example using the tokenizer in the configuration as above. Use a `max_length` of 64. The original datasets provide the label information (NER tags) in the field `\"ner_tags\"`. Indices in this list correspond to the pre-tokenized \"real\" tokens. Create list of labels for each instance that contains the integer values of the class indices for each subwork token. Follow the idea that all subword tokens that do not correspond to the first subword token of a \"real\" token should get a label of `-100`. Tokenize and create label lists for each of the data splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA7gS_FQZZ2H"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHOXx-Hjr0i8"
      },
      "source": [
        "### Custom PyTorch Dataset for NER\n",
        "\n",
        "❓ Write a custom PyTorch dataset, e.g., called `NERDataset` that inherits from `Dataset` and implements the `__init__`, `__len__`, and `__getitem__` methods. (Hint: Check out the notebooks from the last session if you are unsure how to do this.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OYBFx38ZsvP"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_27O_fnsV_o"
      },
      "source": [
        "### Adapting BERT for NER\n",
        "\n",
        "❓ Next, complete the code for the `BertNerModel` below. The `forward` method should use one linear layer that outputs one logit per valid label (referring to the `ner_tags`, -100 is not a valid label). The input to this linear layer is simply the `last_hidden_state` of the BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4mV2x6Aawdr"
      },
      "outputs": [],
      "source": [
        "hidden_size = 128\n",
        "num_tags = len(ner_tags)\n",
        "\n",
        "class BertNerModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Your code here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp8HyRxva8ME"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now, before we actually start the learning, we should define how we measure performance.\n",
        "\n",
        "❓ Could we simply use token-wise accuracy? Why is this not meaningful?\n",
        "\n",
        "❓ Work through [Evaluate sequence models in python](https://www.depends-on-the-definition.com/evaluate-sequence-models/) by Torbias Sterbak.\n",
        "We can load the `seqeval` metric in PyTorch as follows.\n",
        "\n",
        "```\n",
        "import evaluate\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "```\n",
        "\n",
        "The `metric` object can be called as follows:\n",
        "\n",
        "`metric.compute(predictions=predictions, references=true_labels)`\n",
        "\n",
        "Note that you should ignore any tokens that have a label of `-100`, i.e., from the model's output, you first need to create the `predictions` and `true_labels` lists. For example, assume the model output and the list of true labels (that you created above) look like this:\n",
        "\n",
        "```\n",
        "gold_labels =      [-100, 0, 0, 2, 3, -100, 3, 0, 4, -100, 0, 7, -100]\n",
        "predicted_labels = [9,    0, 0, 1, 3,    3, 3, 0, 5,    5, 0, 7,    0]\n",
        "```\n",
        "\n",
        "You need to filter this list of labels such that all the `-100` values are removed:\n",
        "```\n",
        "gold_labels =      [0, 0, 2, 3, 3, 0, 4, 0, 7]\n",
        "predicted_labels = [0, 0, 1, 3, 3, 0, 5, 0, 7]\n",
        "```\n",
        "\n",
        "❓ Implement a function (or two if you prefer) that (given a model):\n",
        "\n",
        "1. Collect the gold standard labels and the predicted labels for all instances of a given dataset (hint: assume that the input is a DataLoader).\n",
        "\n",
        "2. Filter the lists for valid tokens and labels as described above.\n",
        "\n",
        "3. Load the sequeval metric and use `metric.compute` to compute class-wise and overall evaluation scores.\n",
        "\n",
        "IMPORTANT HINT: Read through the next exercise first (implementation of the training loop) or adapt your functions later to make it fit to the model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kbpF-_6dkzk"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jARARto0doPH"
      },
      "source": [
        "### Training\n",
        "\n",
        "At the beginning of your training cell, include the following code for setting the random seeds. (It is important to execute any time you start the training to ensure the random seeds are reset at this point in time.)\n",
        "\n",
        "❓ Implement the training loop (adapt if from last week's notebook). Use AdamW as optimizer (`optimizer = AdamW(model.parameters(), lr=learning_rate, betas=betas, eps=epsilon)`).\n",
        "Suggested parameters:\n",
        "\n",
        "```\n",
        "num_epochs = 16\n",
        "batch_size = 64\n",
        "learning_rate = 3e-5\n",
        "betas=(0.9,0.999)\n",
        "epsilon=1e-08\n",
        "```\n",
        "\n",
        "Make sure to shuffle the training data in your training DataLoader.\n",
        "The training takes about X minutes on a GPU T4 in Google Colab with my implementation, and the results for the test set are as follows (yours might differ a bit due to some randomness in the loss function works).\n",
        "\n",
        "```\n",
        "LOC\n",
        "\t      precision 0.8064516129032258\n",
        "\t         recall 0.8750754375377188\n",
        "\t             f1 0.8393632416787264\n",
        "\t         number 1657\n",
        "MISC\n",
        "\t      precision 0.6221928665785997\n",
        "\t         recall 0.6709401709401709\n",
        "\t             f1 0.6456477039067854\n",
        "\t         number 702\n",
        "ORG\n",
        "\t      precision 0.6782922429344558\n",
        "\t         recall 0.6807483403741702\n",
        "\t             f1 0.6795180722891565\n",
        "\t         number 1657\n",
        "PER\n",
        "\t      precision 0.8291062801932367\n",
        "\t         recall 0.872299872935197\n",
        "\t             f1 0.8501547987616099\n",
        "\t         number 1574\n",
        "\t\t\t \n",
        "\toverall_precision 0.7528089887640449\n",
        "\t overall_recall 0.7910554561717352\n",
        "\t     overall_f1 0.7714584787159804\n",
        "\toverall_accuracy 0.9562975321214222\n",
        "```\n",
        "\n",
        "As loss function, use `torch.nn.CrossEntropyLoss()`, which INCLUDES the softmax already. When you determine the predictions from the output in your evaluation functions above, you do this simply by finding the argmax over the logit values.\n",
        "\n",
        "Here is a snippet of PyTorch code that you may find useful:\n",
        "\n",
        "```\n",
        "    y_pred = model(X)  # Have our model with current weights make a prediction\n",
        "    # outputs should be of shape [batch, sequence, logits] (where sequence values indicate the token indices within one sequence)\n",
        "    y_pred = torch.permute(y_pred, (0, 2, 1))  # swap the sequence and the logit dimensions\n",
        "    loss = loss_fn(y_pred, y)  # ... sucht that the loss function can take care of the rest for us!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEh65nauTZjk"
      },
      "outputs": [],
      "source": [
        "# Always fun with the random seeds ...\n",
        "# We need to set them such that our results will be replicable.\n",
        "# (Hint: for an experiment later, you can change the random seed here and check what happens.\n",
        "# But for now, let's keep the answer to all questions of the universe, 42.)\n",
        "seed=42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "if torch.cuda.is_available():\n",
        "  # This is needed on Colab as we are working in a distributed environment\n",
        "  # If you are working in a different GPU environment, you can probably omit this line if it results in errors.\n",
        "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Instantiate the model             #\n",
        "#####################################\n",
        "\n",
        "# Your code here\n",
        "\n",
        "#####################################\n",
        "# Training / Fine-tuning the model  #\n",
        "#####################################\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q8X8MRG9Os7"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLPH9QV2kpAy"
      },
      "source": [
        "❗ Upon completion, upload your code (this notebook) to your GitLab repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
