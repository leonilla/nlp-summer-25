{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSrL7x_OcObI"
   },
   "source": [
    "## Introduction to Natural Language Processing\n",
    "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
    "Prof. Dr. Annemarie Friedrich<br/>\n",
    "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
    "Date: **SS 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEigPgRIcS7a"
   },
   "source": [
    "# 5.+ 6. Homework\n",
    "## Training and Evaluating a Logistic Regression Classifier\n",
    "\n",
    "In this homework, we will:\n",
    "* implement tf.idf-based features to represent texts\n",
    "* train a multinomial logistic regression classifier using scikit-learn\n",
    "* tune our classifier based on a validation set\n",
    "* observe the learning curves\n",
    "* report final results on a pre-defined test set\n",
    "\n",
    "## Requirements\n",
    "Hand in a 2-page report (upload into the Homework Submission folder in the folder `06 Gradient Descent` answering the questions marked below. Appendices are not allowed. All tables, figures (and potentially references) must occur within the 3 pages.\n",
    "For this submission, you must use the [ACL Style Templates](https://github.com/acl-org/acl-style-files) (available in Latex and for MS Word).\n",
    "\n",
    "❗Upload your report in PDF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72WsRl_4lHRm"
   },
   "outputs": [],
   "source": [
    "# Installations (remove these two lines and install packages if you work locally)\n",
    "!pip install --user -U nltk\n",
    "!pip install -U datasets\n",
    "\n",
    "# Imports: You may use exactly these imports in your code below.\n",
    "import pprint as pp\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt_tab') # Downlad the Punkt sentence segmentation algorithm\n",
    "nltk.download('stopwords') # English stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import string\n",
    "punctuations = set([p for p in string.punctuation])\n",
    "punctuations.add('’')\n",
    "punctuations.add('``')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StEKuAG6d77R"
   },
   "source": [
    "## 20 Newsgroup Dataset\n",
    "\n",
    "In this homework, we will work with the [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/). Go to the original website of the dataset and read about the dataset.\n",
    "\n",
    "❓In your report, _briefly_ describe the dataset. What is the goal of the classification task? (Is this a genre or a topic classification task?) Optional: report how long the news items are in number of sentences, tokens, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf0yUbhhfO6H"
   },
   "outputs": [],
   "source": [
    "# Load the 20 Newsgroup dataset: a training and a test set are provided\n",
    "train_data = load_dataset('rungalileo/20_Newsgroups_Fixed', split=\"train\")\n",
    "test_data = load_dataset('rungalileo/20_Newsgroups_Fixed', split=\"test\")\n",
    "\n",
    "def create_simpler_object(dataset):\n",
    "  \"\"\"Creates a simple list of dictionaries from the original datasets. Dataset\n",
    "  object (for simplicity reasons for now). Removes instances that do not have\n",
    "  a label. \"\"\"\n",
    "  dataset_simple = []\n",
    "  for instance in dataset:\n",
    "    if instance[\"label\"] != \"None\" and instance[\"label\"] is not None:\n",
    "      dataset_simple.append(instance)\n",
    "  return dataset_simple\n",
    "\n",
    "train_data = create_simpler_object(train_data)\n",
    "test_data = create_simpler_object(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ET6R2hYvn1tC"
   },
   "outputs": [],
   "source": [
    "# It's always a good idea to inspect your data:\n",
    "pp.pprint(train_data[0])\n",
    "\n",
    "for k in train_data[3]:\n",
    "  print(k, train_data[3][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgTg4AMBfPPl"
   },
   "source": [
    "The 20 Newsgroups dataset comes with a training split and a test split. For our experiment here, we want to work cleanly and avoid optimizing our model for the test set. Hence, we split off a __validation set__ from the training data.\n",
    "\n",
    "❓How many instances to the training set, the validation set, and the test have, respectively? Report these numbers in a small table in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yb9acQhAffQt"
   },
   "outputs": [],
   "source": [
    "# Let's split off a validation set from the training sets\n",
    "val_data = train_data[int(0.8*len(train_data)):]\n",
    "train_data = train_data[:int(0.8*len(train_data))]\n",
    "\n",
    "# Lengths (in number of instances) of the train, validation, and test splits\n",
    "# Your code here\n",
    "print(\"Instances in train data:\", len(train_data))\n",
    "print(\"Instances in val data:\", len(val_data))\n",
    "print(\"Instances in test data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rh-OkxLf4q0"
   },
   "source": [
    "Next, write a function `get_label_counts` that takes a set of instances as input and returns a dictionary that contains the number of instances per label. Use this function to compute three dictionaries with label counts, one for each datasplit (train, val, test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJzj6gYlfid-"
   },
   "outputs": [],
   "source": [
    "def get_label_counts(data):\n",
    "  # Your code here\n",
    "  pass\n",
    "  # Solution\n",
    "\n",
    "\n",
    "label_counts_train = None\n",
    "label_counts_val = None\n",
    "label_counts_test = None\n",
    "\n",
    "# Solution\n",
    "label_counts_train = get_label_counts(train_data)\n",
    "label_counts_val = get_label_counts(val_data)\n",
    "label_counts_test = get_label_counts(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoFU42_ZhAK6"
   },
   "source": [
    "The following code plots the label distributions of the training splits as a grouped barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfuolxRqfkn7"
   },
   "outputs": [],
   "source": [
    "# sort values by their keys (alphabetically)\n",
    "#sorted_items = sorted(list(label_counts_train.items()), reverse=True)\n",
    "labels = sorted(label_counts_train.keys())\n",
    "train_values = np.array([label_counts_train[l] for l in labels])\n",
    "val_values = np.array([label_counts_val[l] for l in labels])\n",
    "test_values = np.array([label_counts_test[l] for l in labels])\n",
    "\n",
    "# normalize so it's meaningful to compare the distributions\n",
    "train_values = train_values * 100 / np.sum(train_values)\n",
    "val_values = val_values * 100 / np.sum(val_values)\n",
    "test_values = test_values * 100 / np.sum(test_values)\n",
    "\n",
    "train_values = [round(v, 2) for v in train_values]\n",
    "val_values = [round(v, 2) for v in val_values]\n",
    "test_values = [round(v, 2) for v in test_values]\n",
    "\n",
    "values = {\n",
    "    \"train\" : train_values,\n",
    "    \"val\"   : val_values,\n",
    "    \"test\"  : test_values\n",
    "}\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(20,6))\n",
    "\n",
    "for datasplit, value in values.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, value, width, label=datasplit)\n",
    "    ax.bar_label(rects, padding=3, rotation=\"vertical\")\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Number of instances')\n",
    "ax.set_title('Label distributions by datasplit')\n",
    "ax.set_xticks(x + width, labels, rotation=\"vertical\")\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "ax.set_ylim(0, 8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kIiYf1mhHBi"
   },
   "source": [
    "❓Include the label distribution image in your report. Hint: in latex, you can achieve wide figures using:\n",
    "\n",
    "```\n",
    "\\begin{figure*}\n",
    "...\n",
    "\\end{figure*}\n",
    "```\n",
    "\n",
    "_Briefly_ discuss the following questions: Are the instances distributed equally over the labels? Are the label distributions of training, validation, and test split comparable?\n",
    "\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Your next task is to tokenize the texts into words using the Punkt algorithm from `nltk` (see imports above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb3Z9AI1mai2"
   },
   "outputs": [],
   "source": [
    "def tokenize(dataset):\n",
    "  \"\"\"\n",
    "  Take a dataset (train, validation, or test) and tokenizes the text.\n",
    "  Adds a key \"tokens\" to each instance dict that contains the word-tokenized text.\n",
    "  Hint: the following code will be easier if you just use one list here that contains\n",
    "  the tokens of all sentences.\n",
    "  Returns the update dataset.\n",
    "  \"\"\"\n",
    "  # Your code here\n",
    "  pass\n",
    "\n",
    "\n",
    "# Tokenize all datasplits\n",
    "train_data = tokenize(train_data)\n",
    "test_data = tokenize(test_data)\n",
    "val_data = tokenize(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT2xcLyZi0ZZ"
   },
   "source": [
    "## Vocabulary\n",
    "\n",
    "We compute the __vocabulary__, i.e., the set of words that is used, from the training set.\n",
    "\n",
    "❓ Why do we not include the validation or test set when computing the vocabulary for our classifier?\n",
    "\n",
    "❓In your report, first report the total number of different \"words\" (according to the tokenizer) in the training set.\n",
    "Then, remove all words that are either stopwords (using the ste of stopwords imported above), that are a punctuation character or that occur less than 200 times in the training set. Report the size of this filtered vocabulary. We will work with this filtered version from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixc5B5ENolpA"
   },
   "outputs": [],
   "source": [
    "# What is the full vocabulary of the training set?\n",
    "# Your code here\n",
    "\n",
    "\n",
    "# Remove all stopwords and words that occur less than 200 times in the training set\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3m2HEFbj5Xk"
   },
   "source": [
    "### Inverse Document Frequency\n",
    "Next, we compute the inverse document frequency of each vocabulary term $t$ as follows:\n",
    "\n",
    "$\\displaystyle idf(t) = log[\\frac{n}{df(t)}] + 1 $\n",
    "\n",
    "Here, $n$ is the total number of documents (in the training set), and $df(t)$ is in how many documents the term (word) $t$ occurs.\n",
    "Use `math.log(x)` to compute the inverse document frequency for all vocabulary items.\n",
    "\n",
    "❓In your report, list a few terms with a _high_ and a few terms with a _low_ inverse document frequency. Which terms are more/less common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4fa4I6kmGhE"
   },
   "outputs": [],
   "source": [
    "# Compute the document frequency (df) for each word in the vocabulary from the training data\n",
    "# Your code here\n",
    "\n",
    "\n",
    "# Compute the inverse document frequency (idf) for each word in the vocabulary\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrISl9UemUH_"
   },
   "source": [
    "Write a function `set_tfidf(data_set)` that takes a datasplit as input and adds an entry with the key `tf.idf` to each instance. This entry is a vector whose dimensions correspond to the vocabulary items in some fixed (e.g., alphabetical) order. The values of each entry of this vector are the tf.idf values fort he respective vocabulary word. This is computed as:\n",
    "\n",
    "$tf.idf(t, d) = tf(t, d) * idf(t)$\n",
    "\n",
    "The _term frequency_ $tf(t, d)$ is simply how often the word occurs in the particular instance. You can look up the idf value of the term in the dictionary you have created in the last step. You multiply them together.\n",
    "Call this function on the training, test, and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhHb7I75pRXr"
   },
   "outputs": [],
   "source": [
    "def set_tfidf(data_set):\n",
    "  # Adds a tf.idf vector to each instance dictionary using the key \"tf.idf\".\n",
    "  # Returns the updated data_set.\n",
    "  # feature dimensions = words of the vocabulary\n",
    "  # feature values = tf.idf value of the word in the document\n",
    "  # Your code here\n",
    "\n",
    "\n",
    "  return data_set\n",
    "\n",
    "# use the df from the train data to set all the tf.idf values\n",
    "train_data = set_tfidf(train_data)\n",
    "val_data = set_tfidf(val_data)\n",
    "test_data = set_tfidf(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkkjc4_3naVw"
   },
   "source": [
    "## Mapping class labels to indices\n",
    "We have now prepared the feature representations of all instances. Next, we need to encode the labels of each instance. In the dataset given above, they are encoded as strings (take a look at the field \"label\" of an instance). We can map them to integer values as follows:\n",
    "1. Create an alphabetically sorted list of the labels.\n",
    "2. Create a dictionary that maps each string class label to an integer. Hint: the `enumerate` function can be helpful here, but you can also use standard `for` loops.\n",
    "3. Implement the function  `set_class_idx` below that takes a dataset and the indes from step 2 as input. For output see comment string below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuKXmwITwA-q"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "label2ix = None\n",
    "\n",
    "# set the class index for each instance\n",
    "labels = sorted(labels)\n",
    "label2idx = {l:i for i, l in enumerate(labels)}\n",
    "idx2label = {i:l for i, l in enumerate(labels)}\n",
    "\n",
    "def set_class_idx(data_set, label2idx):\n",
    "  \"\"\"\n",
    "  Takes a dataset and a dictionary that maps label strings to integer indexes.\n",
    "  Adds an entry for the key \"classIdx\" to each instance that encodes the class label as an integer.\n",
    "  \"\"\"\n",
    "  # Your code here\n",
    "\n",
    "\n",
    "set_class_idx(train_data, label2idx)\n",
    "set_class_idx(val_data, label2idx)\n",
    "set_class_idx(test_data, label2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAkKbiijvp24"
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We will use scikit-learn's implementation of logistic regression classifier. (In fact, we will use its Stochastic Gradient Classifier (SGDClassifier) with a configuration that corresponds to a logistic regression classifier. Scikit-learn also has a LogisticRegression classifier, but this implementation uses a different optimizer.\n",
    "\n",
    "Now we have prepared the data, each instance is presented as a feature vector of td.idf values, and encoded the label as a class idx. Yet, the overall data structures of the datasplits are still lists. Scikit learn expects matrices and vectors as its input (or at least lists or multi-dimensional lists with only numeric values, such that it can create matrices and vectors from the inputs).\n",
    "For each datasplit:\n",
    "* We create one vector with all the gold labels (using the \"classIdx\" entries from above). In the scikit-learn documentation, this vector is called y.\n",
    "* We create one matrix with all the feature representations of the instances. The first dimension (~ rows) are the instances, the second dimension (~ columns) are the features (here: each feature dimension corresponds to one vocabulary word). The values in the instance representations matrix are the \"tf.idf\" values from above. The scikit-learn documentation calls this matrix that contains all the numeric representations of the training data instances X.\n",
    "\n",
    "Implement the `vectorize_data` function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCat9cqZMYMa"
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data_set):\n",
    "  # Your code here\n",
    "  y = None\n",
    "  X = None\n",
    "\n",
    "  return X, y\n",
    "\n",
    "train_data_X, train_data_y = vectorize_data(train_data)\n",
    "test_data_X, test_data_y = vectorize_data(test_data)\n",
    "val_data_X, val_data_y = vectorize_data(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sffRrDqrFhE"
   },
   "source": [
    "### Learning / Optimization and Hyperparameter Tuning\n",
    "\n",
    "Now, we are ready to train our logistic regression classifier. Compare to the spelled-out example of binary logistic regression that you have enountered in the in-class activity, using scikit-learn's implementation is surprisingly little code (though not as configurable).\n",
    "\n",
    "```\n",
    "clf = SGDClassifier(random_state=0, max_iter=10, warm_start=True)\n",
    "```\n",
    "\n",
    "This line creates a classifer object of the [SGDClassifier](https://scikit-learn.org/stable/modules/sgd.html) class.\n",
    "We perform the training using our training data in the format X, y by calling the function `fit`.\n",
    "\n",
    "The `max_iter` parameter defines how many _epochs_ (passes through the dataset) the optimizer will make at most (if it does not converge). The parameter `warm_start=True` means that the model will keep its current parameter values and further optimize them, even if we re-start training by calling `fit` again on the classifier object.\n",
    "\n",
    "Generally, it's a bad idea to ignore warnings.\n",
    "I'll do it here because I am interrupting the learning on purpose and hence the \"ConvergenceWarning\" (that the classifier isn't done training) is fine with me.\n",
    "\n",
    "❓In the code below, compute the accuracy of the classifier for the training and for the validation set after each epoch of training and add the values to `performance_values_train` and to `performance_values_val`. We will use these lists to plot learning curves in the next step.\n",
    "Hint: Check out the [API of SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) - it contains a function that will easily compute this for you if called correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tInQHgKfqtSn"
   },
   "outputs": [],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "def train_classifier(train_data_X, train_data_y, learning_rate, num_epochs):\n",
    "  steps = []\n",
    "  performance_values_train = []\n",
    "  performance_values_val = []\n",
    "\n",
    "  clf = SGDClassifier(random_state=0, max_iter=1, warm_start=True, loss=\"log_loss\", learning_rate=\"constant\", eta0=learning_rate)\n",
    "  # max_iter = 1: when calling fit(), the model will be trained for at most 1 steps\n",
    "\n",
    "  for train_step in range(num_epochs): # we will pass through the entire (shuffled) dataset this many times\n",
    "\n",
    "    clf.fit(train_data_X, train_data_y) # train the model using the training data (for max_iter=10 steps)\n",
    "\n",
    "    # Compute the validation and training accuracy\n",
    "    val_acc = clf.score(val_data_X, val_data_y)\n",
    "    train_acc = clf.score(train_data_X, train_data_y)\n",
    "\n",
    "    steps.append(train_step+1)\n",
    "    performance_values_train.append(train_acc)\n",
    "    performance_values_val.append(val_acc)\n",
    "  return clf, steps, performance_values_train, performance_values_val\n",
    "\n",
    "# For your experiments, adapt the parameters here.\n",
    "num_epochs = 15\n",
    "learning_rate = 0.0001\n",
    "clf, steps, performance_values_train, performance_values_val = train_classifier(train_data_X, train_data_y, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v0hW824o_fD"
   },
   "source": [
    "__When to stop the training?__\n",
    "\n",
    "Ideally, our model will converge, i.e., the loss will not change any more (or it will not change any more by a small threshold). Often, this is not case, or will not result in the model that generalizes best (it might have overfitte on the training data).\n",
    "Alternative strategies to end training:\n",
    "* Use a pre-defined number of epochs.\n",
    "* Perform _early stopping_: monitor the performance on the validation set and stop the training if this has not improved for $K$ epochs. We also call $K$ the _patience_ of the early stopping algorithm. We usually choose the model with the best performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jop1FZa3JX6t"
   },
   "source": [
    "## Learning Curves\n",
    "\n",
    "A learning curve shows the performance (or performance on a training set vs. on a validation set) for a particular criterion. This criterion could be:\n",
    "* number of steps / epochs taken in training\n",
    "* number features\n",
    "* percentage of training dataset used\n",
    "\n",
    "In the latter two cases, you would always train until the model converges (i.e., performance on the training set does not get better) or until our stopping criterion (see above) has been fulfilled.\n",
    "\n",
    "\n",
    "For advanced learning curve plotting using scikit-learn, see [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html).\n",
    "\n",
    "❓For the vanilla setting of our classifier that I proposed above, describe in your experiment report: Has the model stopped learning on the training dataset? Does it overfit to the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v99hJ1ofIy5j"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(title, criterion, criterion_steps, performance_metric, performance_values_train, performance_values_val):\n",
    "  \"\"\"\n",
    "  Plots a learning curve, i.e., the performance values for different values of a criterion.\n",
    "  criterion_steps: the values of the points on the x-axis, e.g., step size, percentage of training data used, ...\n",
    "  criterion: a string, used as label for the x-axis\n",
    "  performance_metric: a string defining the performance metric, used as label for the y-axis\n",
    "  performance_values_train: the values of the points on the y-axis, e.g., accuracy values for the x-values given in criterion_steps for the training set.\n",
    "  performance_values_val: same as above but for the validation set.\n",
    "  \"\"\"\n",
    "\n",
    "  plt.plot(criterion_steps, performance_values_train, linestyle='-', marker='o', color=\"blue\")\n",
    "  plt.plot(criterion_steps, performance_values_val, linestyle='-', marker='o', color=\"orange\")\n",
    "\n",
    "  plt.title(title)\n",
    "  plt.xlabel(criterion)\n",
    "  plt.ylabel(performance_metric)\n",
    "  plt.ylim(0,1)\n",
    "\n",
    "  p1 = mpatches.Patch(color='blue', label='train')\n",
    "  p2 = mpatches.Patch(color='orange', label='val')\n",
    "  plt.legend(handles=[p1, p2])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "print(len(steps), len(performance_values_train))\n",
    "plot_learning_curve(\"Learning Curve\", \"steps\", steps, \"acc\", performance_values_train, performance_values_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfNypNxSqlM0"
   },
   "source": [
    "❓Plot a few different learning curves: (1) depending on the number of steps, (2) depending on a few different settings for the learning rate (when keeping all else constant), (3) depending on the size of the vocabulary. Add them as figures to your report. The captions should describe clearly what the plots show.\n",
    "\n",
    "\n",
    "## Are you up for a challenge?\n",
    "\n",
    "❓Improve the logistic regression classifier above and clearly (but briefly) describe your variations in your experimental report. Here are a few ideas to try:\n",
    "\n",
    "* increase the number of epochs\n",
    "* perform early stopping\n",
    "* change the vocabulary / pick terms per class?\n",
    "* use lemmas\n",
    "* tune the learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVCpD7IAp7j-"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "AFTER you have completed the hyperparameter tuning __using only the training and the validation set__, evaluate the final results.\n",
    "\n",
    "❓In your experimental report, include a table reporting overall accuracy, and macro-average precision, recall, and F1 scores for several different settings (including the vanilla setting). Your table should clearly state which settings were used in a row. (Hint: You can use `\\begin{table*} ... \\end{table*}` to create wide tables in Latex.)\n",
    "\n",
    "Optional: `pandas` provides some nice functions to turn tables into latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1qQBLiFVDRo"
   },
   "outputs": [],
   "source": [
    "pred_labels = clf.predict(test_data_X)\n",
    "print(pred_labels)\n",
    "\n",
    "print(classification_report(test_data_y, pred_labels, digits=3))\n",
    "for i, l in enumerate(labels):\n",
    "  print(i, l)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNgtLGUPLbVmrhtegjU4NXz",
   "provenance": [
    {
     "file_id": "1-eJ4cdVvYc6N_wuurlDTYrKx7tG5iE_b",
     "timestamp": 1687346338363
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
