{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M2aoMhdVBr5"
   },
   "source": [
    "## Introduction to Natural Language Processing\n",
    "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
    "Prof. Dr. Annemarie Friedrich<br/>\n",
    "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
    "Date: **SS 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HT2Tc8f9phBs"
   },
   "source": [
    "# 5. Logistic Regression\n",
    "\n",
    "**Learning Goals:**<br/>\n",
    "1. Represent text using distributional features​\n",
    "2. Classify an instance using a linear classifier​\n",
    "3. Describe how a logistic regression classifier works (for binary + multi-class classification)​\n",
    "4. Evaluate a classifier using precision, recall, and F1\n",
    "\n",
    "The aim of this notebook is to understand logistic regression using a concrete example.\n",
    "\n",
    "Assume that our training set consists of three data points, two SPAM e-mails (red points) and one NO_SPAM e-mail (blue points).\n",
    "We can treat this as a __binary__ classification setting: \"Is a given new e-mail a SPAM mail?\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSi0ZUJVWUcG"
   },
   "outputs": [],
   "source": [
    "# Imports and helper functions\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import arange # for ranges with decimal steps\n",
    "from math import log\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt # for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKyZfX9DVs0L"
   },
   "source": [
    "### Training Data\n",
    "\n",
    "Each training instance consists of a two-dimensional feature vector $x$, composed of $x_1$ and $x_2$.\n",
    "In our lecture on Logistic Regression, the values of these features were the counts of some informative lemmas (\"information\" and \"account\").\n",
    "\n",
    "For each instance, we also have a __one-hot encoding__ $y$ representing the class label, i.e., the first dimension corresponds to the first class (\"NO_SPAM\") and the second dimension corresponds to the second class (\"SPAM\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbHT-KDvAamu"
   },
   "outputs": [],
   "source": [
    "train_data = [([2, 1], [0,1]),\n",
    "              ([5,2], [0,1]),\n",
    "              ([3, 4], [1,0])]\n",
    "train_data_x = [l[0] for l in train_data]\n",
    "train_data_y = [l[1] for l in train_data]\n",
    "print(\"training data x:\", train_data_x)\n",
    "print(\"training data y:\", train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOS_Qf1lXP_0"
   },
   "outputs": [],
   "source": [
    "# Some helper functions and indices for plotting and inspecting results\n",
    "\n",
    "# Given a one-hot encoded vector, return the class label as a string\n",
    "# 0 means the first entry of the one-hot-encoded class label is 1 ==> [1, 0]\n",
    "# 1 means the second entry of the one-hot-encoded class label is 1 ==> [0, 1]\n",
    "idx2label = {0: \"NO_SPAM\", 1: \"SPAM\"}\n",
    "idx2color = {0: \"blue\", 1: \"red\"} # we need this for plotting only\n",
    "\n",
    "def get_label_string(one_hot_label):\n",
    "  return idx2label[one_hot_label.index(1)]\n",
    "\n",
    "def get_color_string(one_hot_label):\n",
    "  return idx2color[one_hot_label.index(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sx3SpBr5EyRP"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the training data.\n",
    "# red = SPAM, blue = NO_SPAM\n",
    "\n",
    "# Configure our plot\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "plt.rcParams[\"figure.figsize\"] = [4, 4]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.grid()\n",
    "\n",
    "# Plot the training data points\n",
    "for (x1,x2), label in train_data:\n",
    "  labelstring = get_label_string(label)\n",
    "  labelcolor = get_color_string(label)\n",
    "  plt.plot(x1, x2, marker=\"o\", markersize=15, markeredgecolor=\"black\", markerfacecolor=labelcolor)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-seCb7EOXiIc"
   },
   "source": [
    "### Linear Classifier\n",
    "\n",
    "Let's plot a possible decision boundary between the instances of our class.\n",
    "\n",
    "The green line in the image below describes a boundary separating the feature space into two areas: anything below the line is classified as the NO_SPAM category, anything above (strictly speaking, above or exactly on) the line is classified as a SPAM e-mail.\n",
    "\n",
    "The line can be described by the following function: $x_2 = 0.25 \\cdot x_1 + 2$\n",
    "\n",
    "Good explanation: https://en.wikipedia.org/wiki/Linear_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3YRYeEvpm0e"
   },
   "outputs": [],
   "source": [
    "def decision_boundary(x1):\n",
    "  # Returns the value of x2\n",
    "  return 0.25*x1 + 2\n",
    "\n",
    "# Configure our plot\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.grid()\n",
    "\n",
    "# Plot the training data points\n",
    "for (x1,x2), label in train_data:\n",
    "  labelstring = get_label_string(label)\n",
    "  labelcolor = get_color_string(label)\n",
    "  plt.plot(x1, x2, marker=\"o\", markersize=15, markeredgecolor=\"black\", markerfacecolor=labelcolor)\n",
    "\n",
    "# classifier line (x-values chosen to match plot boundaries for x1)\n",
    "# The first parameter of the plot function are the x-values, the second are the y-values.\n",
    "plt.plot([0, 6], [decision_boundary(0), decision_boundary(6)], color=\"green\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz6dLbPWsTgs"
   },
   "source": [
    "Our decision boundary is defined as:\n",
    "\n",
    "$x_2 = 0.25 * x_1 + 2$\n",
    "\n",
    "Any data point that is below the line in our feature space will be classified as SPAM, anything else will be classified as NO_SPAM.\n",
    "\n",
    "Logistic regression classifiers are __linear classifiers__ because they split the feature space into regions representing instances of one class each. If we had three features, the decision boundary would be a plane. If we had more than three features, the decision boundary would be a _hyperplane_ in our high-dimensional space (hard to imagine, just stick the to two- or three-dimensional example in your head).\n",
    "\n",
    "\n",
    "Let's do some re-arrangements of our formula.\n",
    "\n",
    "$x_2 < 0.25 \\cdot x_1 + 2$ --> return SPAM, return NO_SPAM otherwise.\n",
    "\n",
    "$0 < 0.25 \\cdot x_1 -x_2 + 2$ --> return SPAM, return NO_SPAM otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GV5dwzCptc4"
   },
   "outputs": [],
   "source": [
    "# Classifier function, making use of the linear equation above\n",
    "# Returns the predicted one-hot encoded label vector\n",
    "def classify(x, w, b):\n",
    "  x1, x2 = x[0], x[1]\n",
    "  w1, w2 = w[0], w[1]\n",
    "  if 0 < w1*x1 + w2*x2 + b:\n",
    "    return [0,1]\n",
    "  else: # w1*x1 + w2*x2 +b <= 0\n",
    "    return [1,0]\n",
    "\n",
    "# Let's classify our training data with this classifier\n",
    "w = [0.25, -1]\n",
    "b = 2\n",
    "for x, label in train_data:\n",
    "  prediction = classify(x, w, b)\n",
    "  print(x, \"predicted:\", get_label_string(prediction), \"| gold:\", get_label_string(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n3B1QoKbRJe"
   },
   "outputs": [],
   "source": [
    "print(\"What if we had chosen the weights w for our function less cleverly?\")\n",
    "w = [2, -1]\n",
    "b = 0\n",
    "for x, label in train_data:\n",
    "  prediction = classify(x, w, b)\n",
    "  print(x, \"predicted:\", get_label_string(prediction), \"| gold:\", get_label_string(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqhptZ2Bbc9B"
   },
   "source": [
    "### Class Probabilities and the Sigmoid Function\n",
    "\n",
    "Above, we have defined our classifier as follows:\n",
    "\n",
    "$0 < 0.25 \\cdot x_1 -x_2 + 2$ --> return SPAM, return NO_SPAM otherwise.\n",
    "\n",
    "Let us call the right-hand side of this equation the _score_ $z$. The higher this score, the more sure our classifier is that the instance is really a SPAM e-mail. If $z$ is strongly negative, the classifier will be more _confident_ that the e-mail is NO_SPAM.\n",
    "\n",
    "$z = 0.25 \\cdot x_1 -x_2 + 2$\n",
    "\n",
    "What are the values of $z$ for our training instances above?\n",
    "Look at the plot and compare the distances to the decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0w_vD-FGcU_1"
   },
   "outputs": [],
   "source": [
    "def z(x, w, b):\n",
    "  x1, x2 = x[0], x[1]\n",
    "  w1, w2 = w[0], w[1]\n",
    "  return x1*w1 + x2*w2 + b\n",
    "\n",
    "w = [0.25, -1]\n",
    "b = 2\n",
    "for x, label in train_data:\n",
    "  print(\"x=\", x, \"  gold label: {:8s}\".format(get_label_string(label)), \" z=\", z(x, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uByR_G3rdc1x"
   },
   "outputs": [],
   "source": [
    "# Let's do this once again but use numpy for the calculations\n",
    "def z(x, w, b):\n",
    "  # numpy.dot computes the dot product between x and y\n",
    "  return np.dot(x, w) + b\n",
    "\n",
    "w = [0.25, -1]\n",
    "b = 2\n",
    "for x, label in train_data:\n",
    "  print(\"x=\", x, \"  gold label: {:8s}\".format(get_label_string(label)), \" z=\", z(x, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JqCM0H4c6sw"
   },
   "source": [
    "Now, we have one score per instance. If our binary classifier classifies the instances as belonging to the _positive class_, in our case SPAM, this score will be positive. If the score is negative, the instance will be classified into the _negative class_, NO_SPAM.\n",
    "In order to create a probability value for these decisions, we can use the __sigmoid__ function, which looks like this:\n",
    "\n",
    "$\\displaystyle \\sigma(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eR6iCOF-dzGT"
   },
   "outputs": [],
   "source": [
    "# in code ...\n",
    "def sigmoid(z):\n",
    "  # Sigmoid function\n",
    "  return 1 / (1 + math.exp(-z))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 4]\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = [sigmoid(z) for z in x]\n",
    "plt.plot(x,sig)\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylabel(\"$\\sigma(z)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IQSLL_8eyMY"
   },
   "source": [
    "If the score $z$ is exactly 0, the model does not know to which class the instance belongs. Similar to how we chose z>0 to classify into the positive class above, here, we make the decision to classify our instance as SPAM only if $\\sigma(z) > 0.5$.\n",
    "\n",
    "If $z$ is positive, $\\sigma(z)$ will output a score greater than 0.5 but less than 1 (approaching 1 for high $z$).\n",
    "If $z$ is negative, $\\sigma(z)$ will output a score less than 0.5 but greater 0 (approaching 0 for very negative $z$).\n",
    "Voilà! We just converted our $z$ scores into something that we can interpret as a probability.\n",
    "\n",
    "Let us use this sigmoid-based classifier to classify our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ8xYkfFUQ_L"
   },
   "outputs": [],
   "source": [
    "def get_prob(x, w, b):\n",
    "  # Returns the probability that a point x belongs to the POSITIVE class\n",
    "  return sigmoid(z(x, w, b))\n",
    "\n",
    "def get_class(prob):\n",
    "  \"\"\" Binary classifier: given a class probability, returns 1 if the instance\n",
    "  is classifies as belonging to the positive class, and 0 otherwise.\n",
    "  \"\"\"\n",
    "  if prob > 0.5:\n",
    "    return [0, 1] # SPAM\n",
    "  return [1, 0] # NO_SPAM\n",
    "\n",
    "#w = np.array([-0.2, 0.5])\n",
    "#b = -0.5\n",
    "w = [0.25, -1]\n",
    "b = 2\n",
    "\n",
    "for x, label in train_data:\n",
    "  x = np.array(x)\n",
    "  prob = get_prob(x, w, b)\n",
    "  predicted_label = get_label_string(get_class(prob))\n",
    "  print(round(prob, 3), \"| predicted:\", predicted_label, \"| gold:\", get_label_string(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZhallP8j755"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "As we can see in the plot above, our classifier defined by the green decision boundary does quite well, but not perfectly. How well does it do?\n",
    "The code below defines a `test_set`, i.e., instances defined by their x-coordinates and a gold (true) label. Use this test set to evaluate our classifier.\n",
    "\n",
    "❓ Compute overall __accuracy__, __precision__, __recall__ and __F1__ for the SPAM and NO_SPAM classes, and the overall __macro-average__ precision, recall, and F1 scores. You can use `numpy` to implement your solution, or just lists. _Self-control: Macro-average F1 (computed as the average of the per-class F1 scores) should be 85.8%)_.\n",
    "\n",
    "❓[`scikit-learn`](https://scikit-learn.org/stable/modules/model_evaluation.html) provides implementations for a wide range of evaluation metrics. Use its `classification_report` function and compare the results you obtained. Do they match up? Research: What is the __support__ for each class? To which of your scores does _micro-average precision/recall_ correspond?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB-DszOElxZO"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [4.5, 4]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.xlim(0, 12)\n",
    "plt.ylim(0, 8)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.grid()\n",
    "\n",
    "def get_class(x, w, b):\n",
    "  \"\"\" Binary classifier: given a class probability, returns 1 if the instance\n",
    "  is classifies as belonging to the positive class, and 0 otherwise.\n",
    "  \"\"\"\n",
    "  prob = get_prob(x, w, b)\n",
    "  if prob > 0.5:\n",
    "    return [1, 0] # SPAM\n",
    "  return [0, 1] # NO_SPAM\n",
    "\n",
    "# test_data\n",
    "x = [(0.0, 1.8031414310013778), (0.1, 0.7258266946966128), (0.2, 3.620890138280598), (0.30000000000000004, 7.208262407211244), (0.4, 7.9015496552923), (0.5, 0.5039830954870919), (0.6000000000000001, 4.06797329635813), (0.7000000000000001, 7.983453959845199), (0.8, 1.744624664590675), (0.9, 7.4386489533843765), (1.0, 1.3879150006932264), (1.1, 6.961529929419906), (1.2000000000000002, 3.009016171321684), (1.3, 0.21577209105583162), (1.4000000000000001, 4.314408110027064), (1.5, 2.0870358021132507), (1.6, 3.9694403318500404), (1.7000000000000002, 7.73404269939958), (1.8, 7.872276447252742), (1.9000000000000001, 3.4102830261899753), (2.0, 6.950391881664507), (2.1, 5.196834496197658), (2.2, 5.687574086560304), (2.3000000000000003, 0.18441127782233924), (2.4000000000000004, 0.7917129959244376), (2.5, 4.159116171770697), (2.6, 6.5103625862990935), (2.7, 1.4810120986723954), (2.8000000000000003, 4.099397342943209), (2.9000000000000004, 7.262115492251488), (3.0, 0.02603785021018723), (3.1, 6.371919856494345), (3.2, 4.247548334633407), (3.3000000000000003, 0.23460348973473089), (3.4000000000000004, 2.1521824243505705), (3.5, 4.609666245973957), (3.6, 7.133737762183526), (3.7, 4.011814048907767), (3.8000000000000003, 3.027436275449843), (3.9000000000000004, 6.842656283399255), (4.0, 1.8583332676912194), (4.1000000000000005, 2.7613600471652333), (4.2, 6.884602483957005), (4.3, 6.515668720680005), (4.4, 7.017996394694035), (4.5, 1.4219275884176135), (4.6000000000000005, 7.9344971212904865), (4.7, 0.9541664439429756), (4.800000000000001, 1.7302799854523538), (4.9, 2.0477059973254637), (5.0, 1.3136691193536993), (5.1000000000000005, 1.619555584912522), (5.2, 3.1699717485192718), (5.300000000000001, 2.0275549840371596), (5.4, 3.580062457275159), (5.5, 0.3961008520064677), (5.6000000000000005, 4.791811268471962), (5.7, 2.988877217484877), (5.800000000000001, 3.6916920399266697), (5.9, 2.2629700234129473), (6.0, 3.5568458181048817), (6.1000000000000005, 4.0072054410668345), (6.2, 3.397709154949051), (6.300000000000001, 4.479354923658485), (6.4, 3.779218269713576), (6.5, 4.49260035500632), (6.6000000000000005, 6.704878718129064), (6.7, 4.232842669175323), (6.800000000000001, 4.435590537868435), (6.9, 1.1194463140220652), (7.0, 7.703459356046112), (7.1000000000000005, 1.5600507580820313), (7.2, 0.5141787298535432), (7.300000000000001, 5.249958001107477), (7.4, 5.17737850872501), (7.5, 7.204197478827536), (7.6000000000000005, 5.233859061238796), (7.7, 7.504534034744136), (7.800000000000001, 7.040063987125225), (7.9, 5.853698241099453), (8.0, 4.189018630079597), (8.1, 1.8027090127187035), (8.200000000000001, 1.590556510834884), (8.3, 3.9285267451812667), (8.4, 4.265836585226139), (8.5, 2.8188896520913715), (8.6, 6.368823358544846), (8.700000000000001, 0.43720731136563984), (8.8, 0.009027105974214855), (8.9, 3.775660724199586), (9.0, 7.421673619888216), (9.1, 4.182462255005673), (9.200000000000001, 0.820464795751441), (9.3, 1.8230699838875157), (9.4, 1.8417473124953103), (9.5, 5.013064050400255), (9.600000000000001, 4.276477687095192), (9.700000000000001, 5.092134219128303), (9.8, 3.559693015164285), (9.9, 2.508221822640583), (10.0, 2.8774956261454916), (10.100000000000001, 7.031656156170564), (10.200000000000001, 6.684085289318193), (10.3, 5.338287880115494), (10.4, 4.250488260478787), (10.5, 5.748030038719616), (10.600000000000001, 5.311713013402326), (10.700000000000001, 2.0875891800145263), (10.8, 4.718231486204861), (10.9, 5.664660455878861), (11.0, 4.384353140466228), (11.100000000000001, 6.270162437068839), (11.200000000000001, 4.0677924506280885), (11.3, 6.469165566689657), (11.4, 1.6214033214768877), (11.5, 4.406764425333195), (11.600000000000001, 6.877571253785525), (11.700000000000001, 3.6222972571776255), (11.8, 3.3030920081083837), (11.9, 0.8015332684594343)]\n",
    "labels = [[1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]]\n",
    "test_data = list(zip(x, labels))\n",
    "\n",
    "# Plot the test data points\n",
    "for (x1,x2), label in test_data:\n",
    "  labelstring = get_label_string(label)\n",
    "  labelcolor = get_color_string(label)\n",
    "  plt.plot(x1, x2, marker=\"o\", markersize=8, markeredgecolor=\"black\", markerfacecolor=labelcolor)\n",
    "\n",
    "# classifier line (x-values chosen to match plot boundaries for x1)\n",
    "# The first parameter of the plot function are the x-values, the second are the y-values.\n",
    "plt.plot([0, 12], [decision_boundary(0), decision_boundary(12)], color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5HHKrHOl6cq"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Starting point:\n",
    "for x, gold_label in test_data:\n",
    "  pred_label = get_class(x, w, b)\n",
    "  #print(gold_label, pred_label)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNcOutfydYgXTFyp5n/ETfI",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
