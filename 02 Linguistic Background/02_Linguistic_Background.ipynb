{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpsf_RWFQik6"
      },
      "source": [
        "## Introduction to Natural Language Processing\n",
        "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
        "Prof. Dr. Annemarie Friedrich<br/>\n",
        "Faculty of Applied Computer Science, University of Augsburg<br/>\n",
        "Date: **SS 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOlJ0xtFRNtC"
      },
      "source": [
        "# Linguistic Background\n",
        "\n",
        "**Learning Goals**<br/>\n",
        "* Determine the domain and genre of a text.\n",
        "* Compute basic corpus statistics.\n",
        "* Be able to explain the different levels of linguistic analysis.\n",
        "* Become familiar with built-in Python methods and numpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKW_1k611gsO"
      },
      "source": [
        "## Genre and Domain\n",
        "\n",
        "**Genre** refers to a categorization or classification system that groups works or creations based on their distinct characteristics, styles, or forms. Writing or text genres can broadly be distinguished into **fiction** and **non-fiction**.\n",
        "For example, if you read a romantic novel, you expect a very different style from a news article. Both may be about the same topic (e.g., the wedding of a prince), but the language and form will be very different.\n",
        "[This Wikipedia page](https://en.wikipedia.org/wiki/List_of_writing_genres) provides a good list of textual genres. Pick a few good examples that will help you to remember the concept and enter them here.\n",
        "\n",
        "\n",
        "| Genre       | Definition, example(s) |\n",
        "| ----------- | ----------- |\n",
        "| Encyclopedic article  | Nonfiction, e.g., Wikipedia article [Example](https://en.wikipedia.org/wiki/Literary_genre)       |\n",
        "| Children's book   | Fiction (also nonfiction possible?), lots of images, easy language, ...        |\n",
        "| ... | ...|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emmeUmRD3dwN"
      },
      "source": [
        "**Domain** is a term often used to cover both the topic and the genre of a text type in the broader natural language processing community. The term _domain adapation_, which refers to adapting an existing model trained on a particular type of text to another, is often also used to cover domain- and genre-adapation.\n",
        "Yet, this usage is slightly imprecise. Many linguists use the term only in the sense of _semantic domain_, which refers roughly to a topic, and the set of words used to describe it. For example, a domain could be \"sports\" or \"biology.\" For both domains, we can image a text in the genre \"thriller\" or a text in the style of a \"news article.\".\n",
        "\n",
        "❓ Find concrete examples for:\n",
        "1. a medical thriller\n",
        "2. a medical news article\n",
        "3. a sports thriller\n",
        "4. a sports news article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39VUyGWGm4Lu"
      },
      "source": [
        "For NLP, it is a challenge that texts exhibit very different characteristics. Even today's state-of-the-art models do not work equally well for all genres and domains. They generally work well for the news domain, as most research and development has concentrated on this domain in the past. These days, the Large Language Models (LLMs) that are used to process text are mostly trained on books, Wikipedia, and web text. Yet, depending on the application or task, they still need to be adapted to the type of text we intend to process. In NLP terminology, the methods to do that are called **domain adaptation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLSTWXVbZj8U"
      },
      "source": [
        "## Linguistics\n",
        "\n",
        "**Linguistics** or **language science** is the scientific study of language and its structure. Linguists study various aspects of language, including its sounds (phonetics and phonology), word formation (morphology), sentence structure (syntax), meaning (semantics), and how language is used in communication (pragmatics).\n",
        "\n",
        "Linguistics aims to understand and describe the nature of human language. They explore how languages evolve over time, how they are acquired by individuals, and how they are used in different social and cultural contexts. Within linguistis, the study of **discourse** refers to the study of language in use, focusing on how language is organized and structured within larger units of communication beyond the level of individual sentences or utterances.\n",
        "\n",
        "The field of linguistics also examines the cognitive processes involved in language production and comprehension. It explores the relationship between language and the mind, investigating how language influences thought and how our knowledge of language is organized and processed in the brain.\n",
        "The study of linguistics has practical applications in areas such as language teaching and learning, speech and language pathology, translation and interpretation, natural language processing, and forensic linguistics.\n",
        "\n",
        "**Psycholinguistics** is the scientific study of how humans acquire, produce, comprehend, and process language. It investigates the cognitive processes and psychological mechanisms underlying language use, including language perception, production, and comprehension.\n",
        "\n",
        "Linguists employ a range of research methods, including empirical data collection, analysis of linguistic corpora, experimental studies, and theoretical modeling.\n",
        "Psycholinguists employ a variety of experimental methods, including eye-tracking, neuroimaging (such as fMRI), behavioral experiments, computational modeling, and linguistic analysis, to investigate language-related phenomena and uncover the underlying mechanisms of language processing in the human brain.\n",
        "\n",
        "\n",
        "**Corpus linguistics** is a research methodology in linguistics that involves the systematic analysis and study of large collections of authentic language data, known as corpora. It focuses on investigating linguistic patterns, structures, and usage based on empirical evidence extracted from real-world language samples. Corpus linguistics utilizes computational tools and statistical techniques to explore and uncover linguistic phenomena, providing insights into language variation, usage patterns, and the relationships between language and context.\n",
        "\n",
        "This week, we will compute such corpus statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVg6lVOKhWn3"
      },
      "source": [
        "---\n",
        "\n",
        "## Corpus Statistics\n",
        "\n",
        "❗ **Instructions**: Work through the exercises below with a partner. You will learn about some basic corpus statistics and compare the inaugural addresses by two US presidents: Barack Obama and Donald Trump. One student should use `barack_obama.txt`, the other student `donald_trump.txt`as their input. Compare and interpret your results. [Note: you may not be able to complete the exercise within the tutorial session. The rest is homework. :)]\n",
        "\n",
        "Today, we will use two different NLP toolkits, `nltk` and `spacy`. Technically, we could perform all tasks using `spacy`, but I'd like you to know about both frameworks. In addition, two things will come in handy today: `numpy` to compute some simple statistics, and `defaultdict`, which is dictionary (hashmap) that assumes a default entry of 0 if we instantiate it using `defaultdict(int)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx-GY9NEcsA9"
      },
      "outputs": [],
      "source": [
        "# Installations and imports\n",
        "!pip install --user -U nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab') # Downlad the Punkt sentence segmentation algorithm\n",
        "\n",
        "import spacy\n",
        "# This model performs segmentation, POS tagging, and dependency parsing (and several more tasks)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"Pipeline:\", nlp.pipe_names)\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import pprint as pp  # This lets us print some things more nicely"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R1Q-xr5d4u3"
      },
      "source": [
        "**Read the text file**<br/>\n",
        "The recommended way of reading a text file in Python is to use a content manager, i.e., open the file in this way:\n",
        "\n",
        "```\n",
        "file_path = \"path/to/your/file.txt\"\n",
        "with open(file_path, encoding=\"UTF-8\") as file:\n",
        "    content = file.read()\n",
        "    print(content)\n",
        "```\n",
        "\n",
        "After the block starting with `with` is completed, Python automatically closes the file object for you.\n",
        "The `file.read()` method reads the entire text string into the `content` variable. This is not recommended when working with very large text files! In that case, it's better to read the document line by line.\n",
        "However, today, our text file is not particularly big, so this will work.\n",
        "\n",
        "❓ Read in the whole text of either `barack_obama.txt` or `donald_trump.txt` into a variable called `raw_text`. Make sure the files exist in the same folder as this notebook. On Google Colaboratory, upload them on the left-hand side under \"Files.\" (You will have to do this each time you re-start the environment of the notebook, or alternatively connect your Google Drive / GitHub.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIT7SE0Zo01n"
      },
      "outputs": [],
      "source": [
        "# Read the text file into a variable called raw_text\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RArjbRexvr"
      },
      "source": [
        "**Sentence Segmentation and Tokenization**<br/>\n",
        "Next, we will break the raw text into sentences and words. Breaking a text into sentence sounds trivial, but it really isn't. Execute the following code cell and observe what happens if we just split the text at the full stop positions.\n",
        "\n",
        "Similarly, word tokenization, i.e., the process of splitting a sentence into words, is not easy. Luckily, there are many implementations of good sentence splitting algorithms available. Just make sure to pick one for your target language(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4dacgpje8MB"
      },
      "outputs": [],
      "source": [
        "sample_text = \"\"\"Have you ever watched the movie Dr. Doolitte? I have and I've quite enjoyed myself.\n",
        "  You shouldn't watch anything else tonight! Join me.\"\"\"\n",
        "\n",
        "# Dot sentence segmenter\n",
        "for sent in sample_text.split(\".\"):\n",
        "  print(\"SENTENCE:\", sent)\n",
        "\n",
        "# Whitespace tokenizer\n",
        "print()\n",
        "tokens = sample_text.split()\n",
        "pp.pprint(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUBQfwdggXil"
      },
      "outputs": [],
      "source": [
        "# Let's try this with a proper tokenizer.\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "tokenized_sentences = []\n",
        "\n",
        "for sent in sent_tokenize(sample_text):\n",
        "  tokenized_sentences.append(word_tokenize(sent))\n",
        "\n",
        "pp.pprint(tokenized_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOrmPayQTiA1"
      },
      "source": [
        "The sentence tokenizer we have used above is the [Punkt](https://aclanthology.org/J06-4003/) algorithm. It is an unsupervised algorithm trained on a large corpus in the target language. We have simply used a pre-trained model.\n",
        "\n",
        "❓ Use the `sent_tokenize` function to create a list of strings that is assigned to the variable `sentence_list`, where each string contains one sentence of our `raw_text`from above. Print the first ten sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjCGO_7yjkwZ"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w3TPjonji_K"
      },
      "source": [
        "❓Now, also perform word tokenization. Create a list of lists assigned to the variable `tokenized_sentences`, where each inner list contains the tokens that belong to one sentence. For example:\n",
        "\n",
        "```\n",
        "[['Have', 'you', 'ever', 'watched', 'the', 'movie', 'Dr.', 'Doolitte', '?'],\n",
        " ['I', 'have', 'and', 'I', \"'ve\", 'quite', 'enjoyed', 'myself', '.'],\n",
        " ['You', 'should', \"n't\", 'watch', 'anything', 'else', 'tonight', '!'],\n",
        " ['Join', 'me', '.']]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBUF872XLSph"
      },
      "outputs": [],
      "source": [
        "# Apply this tokenization to the raw text you read in above and print the first two sentences.\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBLSnooWVNEW"
      },
      "source": [
        "\n",
        "\n",
        "❓Next, create a list that contains the number of tokens of each sentence, e.g. `[9, 9, 8, 3]`. Use some numpy functions to (a) obtain the total count of tokens in the text (`np.sum()`), and (b) compute the mean, the median and the standard deviation (`std`) of the distribution of sentence lengths. (For example: `np.sum(some_list)`.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PsZazm8LSxs"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzHbhQ4GXgkt"
      },
      "source": [
        "**Type/token ratio**<br/>\n",
        "Imagine you are visiting Augsburg's zoo today. You are interested in two _kinds_ or _types_ of animals: \"lions\" and \"giraffes.\" As you are watching them, you note that there are two \"lions\" and seven \"giraffes.\" (Disclaimer: I haven't counted them recently.) This means that we have two instances of the type \"lion\" and seven instances of the type \"giraffe.\" In case you are familiar with object-oriented programming, it's the same thing (objects and classes).\n",
        "\n",
        "When analysing text, linguists apply a similar logic (the overall idea is really from philosophy). If we want to analyze the _lexical diversity_ of a text in the simplest way, we just assume that every different word is a _type_. In the easiest implementation, \"running\" and \"ran\" would be two different _types_. Of course, they both have the same word _stem_ (\"run\"), but we'll ignore that for now. Hence, the algorithm to obtain the set of _types_ of a text is quite convenient in Python:\n",
        "1. Create one list with all tokens of the text, where each token is a separate string.\n",
        "2. Apply the `set` function to create a set from this list, which means that each unique word is kept exactly once in the set. (That's the definition of a set.)\n",
        "\n",
        "Now, to compute the _type/token ratio_, we divide the number of unique tokens in a text by the number of all tokens.\n",
        "\n",
        "❓What is the type/token ratio of your text? Which president uses a more lexically diverse language in his inaugural speech?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrkdGQoFQFsd"
      },
      "outputs": [],
      "source": [
        "# Type/token ratio\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgPB7QPabnVK"
      },
      "source": [
        "❓For each word type, determine how often it is used in the original text. Hints: Use a `defaultdict` and the `sort(reverse=True)` function. Print the 10 most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mu2J3pENAJe"
      },
      "outputs": [],
      "source": [
        "# Most frequent words?\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJe3HZEOholC"
      },
      "source": [
        "If your code works correctly up to here, the output should be (for Obama/Trump, respectively):\n",
        "\n",
        "```\n",
        "[(132, ','), (120, 'the'), (109, '.'), (106, 'and'), (82, 'of'), (65, 'to'), (58, 'our'), (50, 'we'), (47, 'that'), (45, 'a')]\n",
        "\n",
        "[(95, ','), (86, '.'), (66, 'and'), (64, 'the'), (48, 'of'), (47, 'our'), (37, 'to'), (35, 'will'), (28, 'We'), (21, 'we')]\n",
        "```\n",
        "\n",
        "This is quite uninformative. The most frequent tokens are punctuation tokens and words that occur super frequently. We call those _stopwords_. Fortunately, there are lists of punctuation tokens and lists of stopwords, which we can result to identify the words that are really of interest in our texts. The following code loads a stopwords list provided by nltk, and a built-in string called `string.punctuation`, which contains the punctuation characters for English. I am making a set out of these for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiwfByvBNpYF"
      },
      "outputs": [],
      "source": [
        "# Do the same but remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords= set(stopwords.words('english'))\n",
        "print(stopwords)\n",
        "\n",
        "# ... and remove punctuation as well\n",
        "import string\n",
        "punctuations = set([p for p in string.punctuation])\n",
        "punctuations.add('’')\n",
        "punctuations.add('``')\n",
        "\n",
        "print(punctuations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lErGmJgokoO0"
      },
      "source": [
        "❓Use the stopword and punctuation sets to create a version of the file that contains all tokens except for those whose lowercase version (`string.lower()`) matches either a stopword or a punctuation character. Now, print the 30 most frequent words. Do these lists differ between the presidents' speeches?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCC53SfeOXFO"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bMX_w3ulzIk"
      },
      "source": [
        "**Word Clouds**<br/>\n",
        "Word clouds are often used to obtain a first idea of a text's content. Besides that, they really aren't that useful and linguists rather perform statistical tests on the actual count distributions if they want to draw conclusions. But for the fun of it, let's draw some word clouds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV_5Nda7k9-T"
      },
      "outputs": [],
      "source": [
        "# We can generate a nice word cloud from the cleaned text.\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MO9aJdllDzm"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cleaned_text = \"sample text\" # Replace this with a version of the text without stopwords/punctuation\n",
        "\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud().generate(cleaned_text)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTIimJ-ElwIW"
      },
      "source": [
        "**Grammatical Features**<br/>\n",
        "Finally, we will use [spacy](https://spacy.io/) to label our text with some grammatical features, and compare the count distributions of these across the two inaugural speeches.\n",
        "\n",
        "The `nlp` pipeline that we have already loaded at the beginning of this exercise automatically performs sentence segmentation and word tokenization. Each token is labeled with a **part-of-speech** (POS) tag, which refers to a feature of a word indicating which syntactic functions it can take. Examples for POS tags are NOUN, VERB, ADJ (for adjective), DET (for determiner), or ADV (for adverb).\n",
        "\n",
        "```\n",
        "doc = nlp(raw_text)\n",
        "```\n",
        "\n",
        "Spacy's `nlp(raw_text)` function returns an object of class [Doc](https://spacy.io/api/doc) that represents the annotated linguistic features. You can iterate over this object, which returns one token in each iteration step.\n",
        "The following code prints the surface text of each token (i.e., exactly as it occurs in the text), the lemma, and the part-of-speech tag of the token.\n",
        "\n",
        "```\n",
        "for token in doc[:10]:\n",
        "  print(token.text, token.lemma_, token.pos_)\n",
        "```\n",
        "\n",
        "In linguistics, a **lemma** refers to the base or dictionary form of a word, which represents its canonical or citation form. It is the form of a word that is typically listed in dictionaries or lexicons. The concept of a lemma is used to categorize words based on their shared root or core meaning, regardless of any inflections or variations that may occur in different grammatical contexts.\n",
        "\n",
        "For example, in English, the lemma of the word \"running\" would be \"run,\" as it represents the base form from which various inflected forms like \"runs,\" \"ran,\" and \"running\" are derived. Similarly, the lemma of \"cats\" would be \"cat.\"\n",
        "\n",
        "❓Which are the 20 most frequent verbs, which are 20 most frequent nouns in the inaugural speeches? Hints: spacy labels nouns as NOUN, and verbs as VERB. Use the lemmatized version of the token text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs3zJAUaP4n5"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I156eWEdo1dQ"
      },
      "source": [
        "**Syntactic Relations**<br/>\n",
        "Spacy also parses the sentence using a _dependency parser_, which returns, for each token, a _head word_ and the type of relation it has with regard to that head word. For example, in the the sentence \"John loves ice cream,\" the head of the token \"John\" is \"loves\" (indicate as the arc points from \"loves\" to \"John\"). The label of this relation is \"nsubj,\" i.e., \"nominal subject\", a subject that is a noun.\n",
        "The representation of the entire sentence is a syntactic _dependency tree_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSBJYVw7pIwO"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "d = nlp(\"John loves ice cream.\")\n",
        "displacy.render(d, style='dep', jupyter=True, options={'distance': 90})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yEb2wFRqbmm"
      },
      "source": [
        "Let's also look at sentence that contains a _relative clause_, as we will need this relation for our final task. A relative clause is a type of subordinate clause that provides additional information about a noun or pronoun in a sentence. It functions as an adjective by modifying or describing the noun it is associated with. Relative clauses typically begin with a relative pronoun (such as \"who,\" \"whom,\" \"whose,\" \"which,\" or \"that\") or a relative adverb (such as \"where,\" \"when,\" or \"why\").\n",
        "\n",
        "In the following example, the `relcl`relation between \"loves\" (the main verb of the relative clause \"who loves music\") and \"sister\" (who is described in the relative clause), connects the subordinate clause to the main clause."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJTM7PwUqc3b"
      },
      "outputs": [],
      "source": [
        "d = nlp(\"My sister, who loves music, is singing.\")\n",
        "displacy.render(d, style='dep', jupyter=True, options={'distance': 90})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk2CThsZq2yb"
      },
      "source": [
        "This means that if there is a `relcl` relation, the sentence must contain a relative clause.\n",
        "\n",
        "❓Which percentage of the sentence of the inaugural speech contains at least one relative clause?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1B-a5-Jahbg"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a70iXHK0s5BF"
      },
      "source": [
        "_Control yourselves:_ The result should be 28.2 for Obama and 10.3 for Trump.\n",
        "\n",
        "❓ What does this tell us about the style of language they are using in their inaugural speeches?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lv4ZMqefSOA"
      },
      "source": [
        "## References and Further Notes\n",
        "\n",
        "If you are interested in learning more in depth about linguistics, I highly recommend this book - it's quite fun to read.\n",
        "\n",
        "Fromkin, Rodman, and Hyams. **An Introduction to Language.** Thomson Wadsworth. 2007.\n",
        "\n",
        "If you want to learn just enough about language in order to write a good Bachelor thesis or if you're even aiming to write scientific articles, this book is a must-read:\n",
        "\n",
        "Hilary Glasman-Deal. **Science Research Writing For Non-native Speakers Of English.** Imperial College Press. 2009.\n",
        "\n",
        "In case you are interested in comparing further presidents, the speech transcripts have been taken from the [Miller Centers](https://millercenter.org/the-presidency/presidential-speeches) archive. These types of studies fall into the research area of _computational political social sciences_.\n",
        "\n",
        "Miller Center of Public Affairs, University of Virginia. \"Presidential Speeches: Downloadable Data.\" Accessed May 30, 2023. data.millercenter.org."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
