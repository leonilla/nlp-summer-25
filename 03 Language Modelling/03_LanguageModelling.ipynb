{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2BLz5rCRDKS"
      },
      "source": [
        "## Introduction to Natural Language Processing\n",
        "[**CC-BY-NC-SA**](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)<br/>\n",
        "Prof. Dr. Annemarie Friedrich / Fabio Mariani<br/>\n",
        "Faculty of Applied Computer Science, University of Augsburg<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx0Edci8Am39"
      },
      "source": [
        "#  Language Modeling\n",
        "\n",
        "**Learning Goals**\n",
        "\n",
        "* In this notebook, we will explore how to train and evaluate trigram (3-gram) language models using two sets of texts with different genres of the Brown Corpus.\n",
        "* We will assess how well each model performs on a test set using *perplexity* as a metric.\n",
        "* We will analyze the results to understand how genre, model parameters, and smoothing techniques influence language models performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvBaLB2wEbP1"
      },
      "source": [
        "## Resources\n",
        "\n",
        "Before we build our language model, we need to import several modules and download necessary datasets:\n",
        "\n",
        "- `nltk`: The Natural Language Toolkit, a popular library for working with NLP.\n",
        "- `brown`, `gutenberg`: Two corpora provided by NLTK — the Brown Corpus contains categorized texts (e.g., fiction, news), while the Gutenberg Corpus includes classic literature.\n",
        "- `math`: Used for mathematical operations, such as calculating logarithms and exponentials during perplexity evaluation.\n",
        "\n",
        "We also use `nltk.download()` to ensure that the required corpora and tokenizers are available locally. This step downloads:\n",
        "- `'gutenberg'` – Jane Austen and other literary texts.\n",
        "- `'punkt_tab'` – Tokenizers.\n",
        "- `'brown'` – A categorized corpus of American English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08HCPsEaEbP1",
        "outputId": "551d44c4-1f43-452c-df1f-f88066bb5a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install --user -U nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.lm import Laplace, StupidBackoff\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.corpus import brown, gutenberg\n",
        "from nltk.util import ngrams\n",
        "from nltk import pad_sequence\n",
        "import math\n",
        "import pprint as pp\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMiLsZO3EbP2"
      },
      "source": [
        "## Preparing the Training and Test Data\n",
        "\n",
        "\n",
        "We prepare the data used to train and test our trigram language models.\n",
        "\n",
        "We extract sentences from the Brown Corpus, grouped by two distinct categories (genres):\n",
        "- ``fiction``: used to train our first language model.\n",
        "- ``news``: used to train our second language model.\n",
        "\n",
        "&#9997; To what extent do we expect the two language models to differ?\n",
        "\n",
        "_Your Answer:_\n",
        "> Informative texts could be argued to be more predictable, since they are bound to more constraints(more realistic, no dragons or elves). In contrast, narrative texts may be more \"creatively free\", which may make completely non-sensical or highly unlikely sentences still possible in this context. The higher predictability of the former could result in a language model performing better when trained on this type of text.\n",
        "\n",
        "> First \"suspicion\": news might be the better source material for training these models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlcEdB3wEbP3",
        "outputId": "ed519853-67bd-4f80-f689-343d306078f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4249\n",
            "4623\n"
          ]
        }
      ],
      "source": [
        "# obtain sentences from Brown corpus object\n",
        "model_fiction_sentences = brown.sents(categories='fiction')\n",
        "model_news_sentences = brown.sents(categories='news')\n",
        "\n",
        "# print the number of sentences that we use to train each language model\n",
        "print(len(model_fiction_sentences))\n",
        "print(len(model_news_sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szf95aV5EbP4"
      },
      "source": [
        "We use 200 sentences from \"Emma\" by Jane Austen (Gutenberg Corpus) as our test data, i.e., we will check how well the language models estimated on the training corpora match the distribution of n-grams in this new (unseen) text. The better a language model captures the token distributions and sequences in the target domain (or genre), the better the language model (for this domain).\n",
        "\n",
        "&#9997; Do you expect the `fiction` or the `news` model to match this test text better?\n",
        "\n",
        "_Your answer:_\n",
        "> Considering Jane Austen to be a writer of fiction, and Emma to be a work of fiction, I expect the `fiction` model to match this test better.\n",
        "\n",
        "> Going back to my first \"suspicion\": even though I suspect a model trained on news might perform better in general, I would expect the model trained on fiction sentences to perform better if the unseen data we use for testing is a narrative text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OQ7Yjfr-EbP5"
      },
      "outputs": [],
      "source": [
        "# We test on these sentences.\n",
        "austen_sentences = gutenberg.sents('austen-emma.txt')[700:900]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egMDH3uYEbP5"
      },
      "source": [
        "## Training the N-gram Language Model\n",
        "\n",
        "The following function trains an N-gram language model using the **Laplace smoothing** technique.\n",
        "\n",
        "### Why Do We Need Smoothing in Language Models?\n",
        "\n",
        "When training N-gram language models, we estimate the probability of a word given its preceding context (e.g., for trigrams, `P(w3 | w1, w2)`). However, we face a major challenge:\n",
        "\n",
        "#### The Zero-Probability Problem\n",
        "\n",
        "If an N-gram from the test set **never appeared** in the training data, the model assigns it a probability of **zero**.\n",
        "\n",
        "- Even large training corpora can miss many valid n-gram combinations.\n",
        "- This leads to entire sentences getting **zero probability**, which is not desired in practice - even if we have not seen a particular combination of words, the entire sentencen may still contain some likely parts and we would like to reflect this in our score.\n",
        "- Computing perplexity would also turn complicated if our test set contains unseen sequences (which almost always happens in practice).\n",
        "\n",
        "&#9997; Check the formular in the slides - why do zero probabilities provide a problem for computing perplexity?\n",
        "\n",
        "_Your answer:_\n",
        "\n",
        "> We define Perplexity as \"the inverse probability\", i.e. (1)/(probability). Should any given N-Gram part of a sequence get a probability of 0, that would make the probability of the entire sequence 0 (as mentioned above), and its perplexity impossible to calculate (since you cannot divide by 0).\n",
        "\n",
        "**Smoothing** adjusts the probability distribution so that **no possible sequence gets a probability of zero**, even if it was unseen during training.\n",
        "\n",
        "#### Laplace Smoothing\n",
        "\n",
        "In this exercise, we use **Laplace smoothing**:\n",
        "\n",
        "- It adds `1` to every possible N-gram count.\n",
        "- This ensures **every possible word combination has a non-zero count**.\n",
        "- It’s simple and effective for our purposes.\n",
        "\n",
        "Smoothing is crucial for building robust language models that can handle new or rare word sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ifEEEpsyEbP7"
      },
      "outputs": [],
      "source": [
        "def train_ngram_model(corpus_sentences, n=3):\n",
        "    train_data, vocab = padded_everygram_pipeline(n, corpus_sentences)\n",
        "    model = Laplace(n)\n",
        "    model.fit(train_data, vocab)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQErQMikEbP7"
      },
      "source": [
        "We can then train the 2 models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pHOqeNVJEbP8"
      },
      "outputs": [],
      "source": [
        "n = 3 # trigram\n",
        "\n",
        "model_fiction = train_ngram_model(model_fiction_sentences, n)\n",
        "model_news = train_ngram_model(model_news_sentences, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d70uhcpEbP9"
      },
      "source": [
        "## Applying the Language Model on New Data\n",
        "\n",
        "Once a language model is trained, we want to see how well it performs on new, unseen data. The function below takes a trained model and a list of tokenized sentences, then calculates the probability of each word based on its context using the model:\n",
        "\n",
        "1) We add ``<s>`` padding to the beginning of the sentence so that the first n-grams can be formed correctly. For trigrams ``(n=3)``, this adds two ``<s>`` tokens: ``['<s>', '<s>', 'the', ...]``.\n",
        "\n",
        "2) We split the sentence into overlapping sequences of tokens (n-grams). For trigrams: ``('<s>', '<s>', 'the'), ('<s>', 'the', 'dog'), ('the', 'dog', 'barked'), ... ``\n",
        "\n",
        "3) Separate each n-gram into context and target word.\n",
        "    - n-gram: ``('the', 'dog', 'barked')``:\n",
        "        - context (what the model see): ``('the', 'dog')``\n",
        "        - target word (what we expect the model to predict) : ``barked``\n",
        "\n",
        "4) We calculate the model probability to predict the expected word given the context.\n",
        "\n",
        "5) The function returns a list of ``(word, probability)`` for all the words in new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KT8drmYREbP9"
      },
      "outputs": [],
      "source": [
        "def apply_model(model, tokenized_sentences, n):\n",
        "    prob_t = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_tokens = pad_sequence(sentence, n=n, pad_left=True, left_pad_symbol='<s>')\n",
        "        for ngram in ngrams(padded_tokens, n):\n",
        "            context = ngram[:-1]\n",
        "            word = ngram[-1]\n",
        "            prob = model.score(word, context)\n",
        "            prob_t.append((word, prob, context))\n",
        "    return prob_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "708ttISYEbP-",
        "outputId": "6a5389fd-63cc-4520-99ef-cbc83099992d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('\"', 7.377895824110964e-05, ('<s>', '<s>')),\n",
            " ('You', 0.00010746910263299302, ('<s>', '\"')),\n",
            " ('have', 0.00010746910263299302, ('\"', 'You')),\n",
            " ('made', 0.0002149151085321298, ('You', 'have')),\n",
            " ('her', 0.00010743446497636442, ('have', 'made')),\n",
            " ('too', 0.0001074229240519927, ('made', 'her')),\n",
            " ('tall', 0.0001074575542660649, ('her', 'too')),\n",
            " (',', 0.00010746910263299302, ('too', 'tall')),\n",
            " ('Emma', 0.00010746910263299302, ('tall', ',')),\n",
            " (',\"', 0.00010746910263299302, (',', 'Emma'))]\n"
          ]
        }
      ],
      "source": [
        "model_fiction_probs = apply_model(model_fiction, austen_sentences, n)\n",
        "model_news_probs = apply_model(model_news, austen_sentences, n)\n",
        "\n",
        "# Take a look at the data structure\n",
        "pp.pprint(model_fiction_probs[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeBCcu47EbP-"
      },
      "source": [
        "## Exercise: Calculate Perplexity\n",
        "\n",
        "Now that we’ve evaluated word probabilities using our language models, it’s time to **quantify** how surprised the models are with new data (test text) using **perplexity**.\n",
        "\n",
        "---\n",
        "\n",
        "### What is Perplexity?\n",
        "\n",
        "Perplexity measures how well a language model predicts a sequence of words. It can be interpreted as:\n",
        "\n",
        "> \"**How surprised** is the model when it sees the actual test data?\"\n",
        "\n",
        "- A **lower** perplexity score means the model is more confident and better at predicting the text.\n",
        "- A **higher** perplexity indicates more uncertainty or poor prediction.\n",
        "\n",
        "Mathematically, for a sequence of N words with probabilities \\( p_1, p_2, ..., p_N \\), perplexity is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Perplexity} = \\exp \\left( -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_i \\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( pi \\) is the predicted probability of the i-th word (following a history whose length depends on the language model),\n",
        "- \\( N \\) is the total number of predicted words.\n",
        "\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Complete the following function to compute perplexity based on the list of predicted probabilities.\n",
        "\n",
        "**Input**\n",
        "\n",
        "``words_probs``: A list of tuples like ``(word, probability, context)``\n",
        "\n",
        "Example: ``[('the', 0.003, ('<s>', '<s>')), ('dog', 0.02, ('<s>', 'the'))]``\n",
        "\n",
        "\n",
        "**Output**\n",
        "\n",
        "Returns a single float value: the computed perplexity of the model over the test data.\n",
        "\n",
        "\n",
        "\n",
        "The function may use Python's built-in ``math`` module:\n",
        "- `math.log(prob)` calculates the natural logarithm (ln) of a probability.\n",
        "- `math.exp(x)` computes the exponential \\( e^x \\).\n",
        "You can read more here: [math documentation](https://docs.python.org/3/library/math.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Ysb2jIi5EbP_"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(words_probs):\n",
        "    sum_of_lns = 0\n",
        "\n",
        "    # get the log(pi) of each item of the list and accumulate the sum in a var\n",
        "    for wp in words_probs:\n",
        "      sum_of_lns += math.log(wp[1])\n",
        "\n",
        "    # multiply the var w (-1/N)\n",
        "    exponent = sum_of_lns / len(words_probs) * -1\n",
        "\n",
        "    # calc perplexity = e ^ exponent\n",
        "    perp = math.exp(exponent)\n",
        "    print(f\"Perplexity = {perp}\")\n",
        "\n",
        "    return perp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hLNvEfPlEbP_",
        "outputId": "d65f5398-8f4b-485c-8c2d-d14d40b99e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity = 7670.091861316205\n",
            "Perplexity = 12425.717876764873\n",
            "Perplexity for model_fiction: 7670.091861316205\n",
            "Perplexity for model_news: 12425.717876764873\n"
          ]
        }
      ],
      "source": [
        "ppl_fiction = calculate_perplexity(model_fiction_probs)\n",
        "ppl_news = calculate_perplexity(model_news_probs)\n",
        "\n",
        "print(f'Perplexity for model_fiction: {ppl_fiction}')\n",
        "print(f'Perplexity for model_news: {ppl_news}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4V3ZKw_EbP_"
      },
      "source": [
        " &#9997; What do you observe? What does this mean? Do the observation match your expectations?\n",
        "\n",
        " _Your answer:_\n",
        "\n",
        " > The perplexity reported for model_fiction = 7670.0918... is lower than that reported for model_news = 12425.7178.... The observation contradicts my expectations. Again, being a model trained on Fiction sentences, I expected Fiction to be less surprised at 'Emma'.\n",
        "\n",
        "  &#9997; Experiment with at least two parameters, e.g., change the length of the history ($n$) or experiment with a different smoothing method, adding training data (check out how to use text from several categories of the Brown corpus, for example), etc. and briefly describe your findings. Submit the version of the notebook containing the answers and code for the original question, but keep your variants either below or comment them out."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}